{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef59cae-b968-4128-997b-886e9eb70ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardlai/miniconda3/envs/AIE4-midterm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding file**** /Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/src/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf\n",
      "Adding file**** /Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/src/vectorstore/pdfs/NIST.AI.600-1.pdf\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "from data import generator, documents, distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8f2372-d18f-4e24-92d4-5b90fe5105a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.                   \n",
      "Generating:   0%|          | 0/20 [00:00<?, ?it/s][ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 3, 'structure': 3, 'relevance': 3, 'score': 3.0}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI Bill of Rights', 'Existing law and policy', 'Automated system development', 'Ethical use of AI', 'Trustworthy Artificial Intelligence']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['OSTP conducted meetings', 'Private sector and civil society stakeholders', 'AI Bill of Rights', 'Positive use cases', 'Oversight possibilities']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['OSTP conducted meetings', 'Private sector and civil society stakeholders', 'AI Bill of Rights', 'Positive use cases', 'Oversight possibilities']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 3, 'structure': 3, 'relevance': 3, 'score': 3.0}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Protect the public from harm', 'Consultation', 'Testing', 'Risk identification and mitigation']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 3, 'structure': 3, 'relevance': 3, 'score': 3.0}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI Bill of Rights', 'Existing law and policy', 'Automated system development', 'Ethical use of AI', 'Trustworthy Artificial Intelligence']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['OSTP conducted meetings', 'Private sector and civil society stakeholders', 'AI Bill of Rights', 'Positive use cases', 'Oversight possibilities']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI system incidents', 'Organizational risk management authority', 'Remediation plan', 'Deactivation criteria', 'Third-party GAI resources']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Data privacy', 'Social media presence', 'Identity theft', 'Facial recognition system', 'Surveillance software']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Generative AI systems', 'Disinformation and misinformation', 'Information security risks', 'Offensive cyber capabilities', 'GAI-powered security co-pilots']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI Risk Management Framework', 'Bias in Artificial Intelligence', 'Trustworthy AI', 'Language models', 'Synthetic media transparency']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 2, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Technical companion', 'AI Bill of Rights', 'Algorithmic discrimination protections', 'Data privacy', 'Human alternatives']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Human subjects', 'Content provenance data', 'Data privacy', 'AI system performance', 'Pre-deployment testing']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['TEVV metrics', 'Measurement error models', 'AI risks', 'Feedback processes', 'Impact assessments']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Continuous monitoring', 'GAI system impacts', 'Structured feedback mechanisms', 'Harmful Bias and Homogenization', 'Information Integrity']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 3, 'structure': 3, 'relevance': 3, 'score': 3.0}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Protect the public from harm', 'Consultation', 'Testing', 'Risk identification and mitigation']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 3, 'structure': 3, 'relevance': 3, 'score': 3.0}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['National Institute of Standards and Technology', 'Artificial intelligence', 'AI Safety Institute', 'Executive Order', 'NIST Generative AI Public Working Group']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How do language models contribute to the reduction of content diversity in writing?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How do companies use surveillance software to track employee discussions about union activity?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What is discussed in the section on data privacy in the technical companion to the Blueprint for an AI Bill of Rights?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How do GAI-based systems present primary information security risks related to offensive cyber capabilities?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How should the public be involved in the consultation process for the development of automated systems?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How can automated systems be designed to protect the public from harm in a proactive and ongoing manner?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the role of the National Institute of Standards and Technology in advancing reliable and safe artificial intelligence?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How can measurement error models be used to demonstrate construct validity for pre-deployment metrics in the MEASURE function?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How can evaluations involving human subjects meet applicable requirements and be representative of the relevant population in the context of GAI applications?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What are some of the oversight possibilities discussed in the meetings conducted by OSTP with stakeholders regarding the development of the Blueprint for an AI Bill of Rights?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How can structured feedback mechanisms be used to monitor and improve outputs of the GAI system?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How does the Blueprint for an AI Bill of Rights aim to assist governments and the private sector in implementing principles for automated system development and use?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What role did stakeholders from the private sector and civil society play in providing ideas related to the development of the Blueprint for an AI Bill of Rights, including positive use cases for these technologies?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What procedures should be established and maintained for the remediation of issues triggering incident response processes for the use of a GAI system, and how should stakeholders be provided with timelines associated with the remediation plan?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What role did stakeholders from the private sector and civil society play in providing ideas related to the development of the Blueprint for an AI Bill of Rights, including positive use cases for these technologies?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How does the Blueprint for an AI Bill of Rights aim to assist governments and the private sector in implementing principles for automated system development and use?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about public involvement in the consultation process for developing automated systems. It is clear in its intent, specifying the focus on public involvement and the context of automated systems. However, it could benefit from being more specific about the type of automated systems (e.g., AI, robotics) or the nature of the consultation process (e.g., public hearings, surveys). Adding these details would enhance clarity and allow for a more targeted response.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question clearly asks about the role of the National Institute of Standards and Technology (NIST) in the context of advancing reliable and safe artificial intelligence. It is specific and independent, as it does not rely on external references or additional context to be understood. The intent is clear, seeking information about NIST's contributions or activities related to AI safety and reliability. Therefore, it meets the criteria for clarity and answerability.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What is the role of the National Institute of Standards and Technology in advancing reliable and safe artificial intelligence?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the role of language models in reducing content diversity in writing. It is specific and has a clear intent, focusing on the impact of language models on writing diversity. However, it could benefit from additional context or clarification regarding what is meant by 'content diversity' (e.g., diversity in style, topics, perspectives) and how this reduction is measured or observed. Providing examples or specifying the types of language models in question could enhance clarity and answerability.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question addresses how evaluations involving human subjects can meet applicable requirements and be representative of the relevant population specifically in the context of General Artificial Intelligence (GAI) applications. It is clear in its intent to explore the standards and representativeness of evaluations, making it understandable. However, the question could benefit from being more specific about which applicable requirements are being referred to (e.g., ethical guidelines, legal regulations) and what aspects of representativeness are of interest (e.g., demographic diversity, sample size). Providing these details would enhance clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the content of a specific section on data privacy in a document referred to as the 'technical companion to the Blueprint for an AI Bill of Rights'. While it specifies the topic of interest (data privacy) and the document, it assumes familiarity with this specific document without providing any context or details about its content. This reliance on external references makes the question less clear and answerable for those who may not have access to or knowledge of the document. To improve clarity and answerability, the question could include a brief summary of the document or specify what aspects of data privacy are of interest (e.g., principles, recommendations, implications).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What is discussed in the section on data privacy in the technical companion to the Blueprint for an AI Bill of Rights?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the use of surveillance software by companies to monitor employee discussions regarding union activity. It is specific and has a clear intent, focusing on a particular aspect of workplace surveillance. However, the question could benefit from additional context regarding the type of surveillance software or the legal and ethical implications involved. Including such details could enhance the clarity and depth of the inquiry, making it more comprehensive. Overall, the question is understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the design of automated systems aimed at protecting the public from harm, emphasizing a proactive and ongoing approach. It is relatively clear in its intent, seeking information on design principles or strategies. However, the question is somewhat broad and lacks specificity regarding the type of automated systems (e.g., surveillance, emergency response, health monitoring) or the context in which they are to be applied. To improve clarity and answerability, the question could specify the domain of application (e.g., public safety, healthcare) or the types of harm being addressed (e.g., physical, cyber threats). This would help narrow down the focus and provide a more targeted response.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: How can automated systems be designed to protect the public from harm in a proactive and ongoing manner?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question seeks to understand the contributions of private sector and civil society stakeholders in the development of the Blueprint for an AI Bill of Rights, specifically regarding their ideas and positive use cases for AI technologies. It is clear in its intent and specifies the groups involved (private sector and civil society) as well as the context (AI Bill of Rights). However, the question could be improved by clarifying what is meant by 'role'—whether it refers to direct contributions, consultations, or advocacy—and by providing a brief context about the Blueprint itself for those unfamiliar with it. This would enhance clarity and answerability.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: What role did stakeholders from the private sector and civil society play in providing ideas related to the development of the Blueprint for an AI Bill of Rights, including positive use cases for these technologies?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the primary information security risks associated with GAI-based systems in the context of offensive cyber capabilities. It is specific in its focus on GAI-based systems and the type of risks being queried, which contributes to its clarity. However, the term 'GAI-based systems' may not be universally understood without additional context, and the phrase 'offensive cyber capabilities' could benefit from clarification regarding what specific aspects or examples are being referred to. To improve clarity and answerability, the question could define 'GAI' and provide examples of the offensive cyber capabilities in question, ensuring that the audience can fully grasp the topic being discussed.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question seeks to understand the contributions of private sector and civil society stakeholders in the development of the Blueprint for an AI Bill of Rights, specifically regarding their ideas and positive use cases for AI technologies. It is clear in its intent and specifies the groups involved (private sector and civil society) as well as the context (AI Bill of Rights). However, the question could be improved by clarifying what is meant by 'role'—whether it refers to direct contributions, consultations, or advocacy—and by providing a brief context about the Blueprint itself for those unfamiliar with it. This would enhance clarity and answerability.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What role did stakeholders from the private sector and civil society play in providing ideas related to the development of the Blueprint for an AI Bill of Rights, including positive use cases for these technologies?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question clearly asks about the objectives of the Blueprint for an AI Bill of Rights in relation to assisting governments and the private sector with automated system development and use. It specifies the subject (Blueprint for an AI Bill of Rights) and the context (governments and private sector), making the intent clear. The question is self-contained and does not rely on external references, allowing for a direct response based on knowledge of the topic. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"How does the Blueprint for an AI Bill of Rights aim to assist governments and the private sector in implementing principles for automated system development and use?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the application of measurement error models in demonstrating construct validity for pre-deployment metrics within the MEASURE function. It specifies the topic (measurement error models, construct validity, pre-deployment metrics, MEASURE function) and seeks a clear explanation of their relationship. However, the question may be challenging for those unfamiliar with the specific terminology or the context of the MEASURE function. To improve clarity and answerability, it would be beneficial to provide a brief explanation of what the MEASURE function entails or the specific aspects of construct validity being referred to. This would help ensure that the question is understandable to a broader audience.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: How can measurement error models be used to demonstrate construct validity for pre-deployment metrics in the MEASURE function?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is quite complex and covers multiple aspects regarding the procedures for remediation of issues related to incident response processes in a GAI system. While it does specify the focus on procedures and stakeholder communication regarding timelines, the phrasing is somewhat convoluted, which may lead to ambiguity in understanding the specific intent. To improve clarity and answerability, the question could be broken down into simpler components, such as: 'What procedures should be established for the remediation of issues in a GAI system?' and 'How should stakeholders be informed about the timelines for the remediation plan?' This would make it easier to address each part of the inquiry directly.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What procedures should be established and maintained for the remediation of issues triggering incident response processes for the use of a GAI system, and how should stakeholders be provided with timelines associated with the remediation plan?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What contributions does NIST make to promote secure and reliable AI, considering its focus on transparency, safety, and standards, as well as its involvement in the U.S. AI Safety Institute and AI Safety Institute Consortium?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about oversight possibilities discussed in meetings conducted by the OSTP with stakeholders concerning the AI Bill of Rights. While it specifies the topic (oversight possibilities) and the context (meetings by OSTP), it assumes familiarity with the specific meetings and discussions without providing any details. This could lead to ambiguity for those not aware of the OSTP's activities or the AI Bill of Rights. To improve clarity and answerability, the question could specify what types of oversight possibilities are of interest (e.g., regulatory, ethical) or provide a brief context about the OSTP and the AI Bill of Rights to help frame the inquiry.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: What are some of the oversight possibilities discussed in the meetings conducted by OSTP with stakeholders regarding the development of the Blueprint for an AI Bill of Rights?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the use of structured feedback mechanisms to monitor and improve the outputs of a GAI (Generative AI) system. It is specific in its focus on feedback mechanisms and their application to GAI systems, making the intent clear. However, the term 'structured feedback mechanisms' could be interpreted in various ways without further context. To enhance clarity and answerability, the question could specify what types of structured feedback mechanisms are being considered (e.g., user feedback, automated evaluation metrics) and what aspects of the GAI system's outputs are intended to be monitored and improved (e.g., accuracy, relevance, creativity).\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question clearly asks about the objectives of the Blueprint for an AI Bill of Rights in relation to assisting governments and the private sector with automated system development and use. It specifies the subject (Blueprint for an AI Bill of Rights) and the context (governments and private sector), making the intent clear. The question is self-contained and does not rely on external references, allowing for a direct response based on knowledge of the topic. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: \"How does the Blueprint for an AI Bill of Rights aim to assist governments and the private sector in implementing principles for automated system development and use?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
      "Generating:   5%|▌         | 1/20 [00:03<01:15,  3.97s/it][ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: What contributions did stakeholders from the private sector and civil society make towards the development of the Blueprint for an AI Bill of Rights, including positive use cases for these technologies?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How can measurement error models be utilized to establish construct validity for pre-deployment metrics in the MEASURE function across AI system trustworthiness assessments?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How does the Blueprint for an AI Bill of Rights aim to assist in implementing principles for automated system development and use, considering the potential impact on civil rights, equal opportunities, and access to critical resources?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI system incidents', 'Organizational risk management authority', 'Remediation plan', 'Deactivation criteria', 'Third-party GAI resources']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the content of a specific section on data privacy in a document referred to as the 'technical companion to the Blueprint for an AI Bill of Rights'. While it specifies the topic of interest (data privacy) and the document, it assumes familiarity with the document itself without providing any context or details about its content. This reliance on an external reference makes the question less clear and answerable for those who may not have access to or knowledge of the document. To improve clarity and answerability, the question could include a brief summary of the document or specify what aspects of data privacy are of interest (e.g., principles, recommendations, implications).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Companies use surveillance software to track employee discussions about union activity and use the resulting data to surveil individual employees and surreptitiously intervene in discussions.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is quite complex and covers multiple aspects regarding the procedures for remediation of issues related to incident response processes in a GAI system. While it does specify the focus on procedures and stakeholder communication regarding timelines, the phrasing is somewhat convoluted, which may lead to ambiguity. To improve clarity and answerability, the question could be broken down into simpler components, such as: 'What specific procedures should be established for the remediation of issues in a GAI system?' and 'How should stakeholders be informed about the timelines for the remediation plan?' This would make it easier to understand and respond to each part of the inquiry.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "Generating:  10%|█         | 2/20 [00:05<00:42,  2.36s/it][ragas.testset.evolutions.INFO] seed question generated: What procedures should be established and maintained for the remediation of issues triggering incident response processes for the use of a GAI system, and how should stakeholders be provided with timelines associated with the remediation plan?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the design of automated systems aimed at protecting the public from harm, emphasizing a proactive and ongoing approach. It is relatively clear in its intent, seeking information on design principles or strategies. However, the question is somewhat broad and lacks specificity regarding the type of automated systems (e.g., surveillance, emergency response, health monitoring) or the context in which they are to be applied. To improve clarity and answerability, the question could specify the domain of application (e.g., public health, transportation, security) or the types of harm being addressed (e.g., physical, cyber, environmental). This would help narrow down the focus and provide a more targeted response.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and seeks to understand the contributions of NIST (National Institute of Standards and Technology) in promoting secure and reliable AI, particularly in relation to transparency, safety, and standards, as well as its involvement in specific organizations. It clearly conveys its intent and does not rely on external references, making it understandable and answerable based on the details provided. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Decommissioning AI systems', 'Data retention requirements', 'Roles and responsibilities', 'AI incident response tasks', 'National security risks']\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'GAI-based systems present primary information security risks related to offensive cyber capabilities by potentially discovering or enabling new cybersecurity risks, lowering barriers for offensive capabilities, and expanding the attack surface. Offensive cyber capabilities advanced by GAI systems may augment cybersecurity attacks such as hacking, malware, and phishing. Reports have indicated that LLMs are already able to discover vulnerabilities in systems and write code to exploit them, posing a significant threat. Sophisticated threat actors could further these risks by developing GAI-powered security co-pilots to inform attackers on evading threat detection and escalating privileges after gaining system access.', 'verdict': 1}\n",
      "Generating:  15%|█▌        | 3/20 [00:05<00:24,  1.46s/it][ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The public should be involved in the consultation process for the development of automated systems by being consulted in the design, implementation, deployment, acquisition, and maintenance phases. Emphasis should be placed on early-stage consultation before a system is introduced or a significant change is implemented. This consultation should directly engage diverse impacted communities to consider concerns and risks unique to those communities. The extent and form of outreach may vary depending on the specific system and development phase, but should include subject matter, sector-specific, and context-specific experts, as well as experts on potential impacts such as civil rights, civil liberties, and privacy. For private sector applications, consultations before product launch may need to be confidential, while government applications may require limited engagement based on system sensitivities and oversight laws. Concerns raised in the consultation should be documented, and developers should reconsider the system based on this feedback.', 'verdict': 1}\n",
      "Generating:  20%|██        | 4/20 [00:05<00:14,  1.08it/s][ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the contributions of private sector and civil society stakeholders to the development of the Blueprint for an AI Bill of Rights, specifically regarding their proposals for positive use cases of AI technologies. It is clear in its intent and specifies the groups involved and the context (AI Bill of Rights). However, the question may be challenging for those unfamiliar with the specific details of the Blueprint or the stakeholders involved. To enhance clarity and answerability, it could be beneficial to provide a brief overview of the Blueprint for an AI Bill of Rights or specify which stakeholders are being referred to, as well as examples of the positive use cases.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['OSTP conducted meetings', 'Private sector and civil society stakeholders', 'AI Bill of Rights', 'Positive use cases', 'Oversight possibilities']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI technology', 'Legal risks', 'Third-party data', 'Intellectual property infringement', 'Data privacy']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Evaluations involving human subjects in the context of GAI applications can meet applicable requirements and be representative of the relevant population by assessing and managing statistical biases related to GAI content provenance through techniques such as re-sampling, re-weighting, or adversarial training. It is also important to document how content provenance data is tracked and how it interacts with privacy and security measures, including anonymizing data, leveraging privacy filters, and removing personally identifiable information (PII) to prevent harm or misuse. Providing human subjects with options to withdraw participation or revoke consent for the use of their data in GAI applications is crucial. Additionally, using techniques like anonymization, differential privacy, or other privacy-enhancing technologies can minimize risks associated with linking AI-generated content back to individual human subjects.', 'verdict': 1}\n",
      "Generating:  25%|██▌       | 5/20 [00:06<00:10,  1.39it/s][ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about oversight possibilities discussed in meetings conducted by the OSTP with stakeholders concerning the AI Bill of Rights. While it specifies the topic (oversight possibilities) and the context (meetings by OSTP), it assumes familiarity with the specific meetings and their content without providing any details. This reliance on external knowledge makes the question less clear and answerable for those not privy to the discussions. To improve clarity and answerability, the question could include a brief description of the OSTP's role, the stakeholders involved, or the specific aspects of the AI Bill of Rights being addressed. This would help provide a clearer context for the inquiry.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What processes and procedures are in place for decommissioning and phasing out AI systems safely and in a manner that does not increase risks or decrease the organization's trustworthiness?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What does NIST do to support secure AI with transparency, safety, and standards, including its role in the U.S. AI Safety Institute and AI Safety Institute Consortium?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the aims of the Blueprint for an AI Bill of Rights in relation to existing law and policy, specifically how it seeks to translate principles into practice. It is clear in its intent and specifies the topic of interest (AI Bill of Rights) and the context (existing law and policy). However, the question could be improved by providing a brief overview of what the Blueprint entails or the specific principles it addresses, as this would help those unfamiliar with the document to better understand the context. Overall, the question is specific and independent, making it answerable with sufficient domain knowledge.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI Risk Management Framework', 'Bias in Artificial Intelligence', 'Trustworthy AI', 'Language models', 'Synthetic media transparency']\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: How did stakeholders from the private sector and civil society contribute to the development of the Blueprint for an AI Bill of Rights, including proposing positive use cases for these technologies?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the contributions of private sector and civil society stakeholders to the development of the Blueprint for an AI Bill of Rights, specifically mentioning positive use cases for these technologies. It is clear in its intent and specifies the groups involved and the topic of interest. However, the question could be improved by providing a bit more context about what the Blueprint for an AI Bill of Rights entails or what specific contributions are being sought (e.g., policy suggestions, case studies). This would help ensure that the question is fully self-contained and answerable without requiring external references. Overall, it is fairly clear but could benefit from slight elaboration.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What stakeholders were involved in providing ideas related to the development of the Blueprint for an AI Bill of Rights?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What approaches are in place for mapping AI technology and addressing legal risks, including intellectual property infringement and data privacy concerns?\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Structured feedback mechanisms can be used to monitor and improve outputs of the GAI system by implementing continuous monitoring to identify if outputs are equitable across different sub-populations. This involves seeking active and direct feedback from affected communities through mechanisms such as red-teaming. Additionally, evaluating the quality and integrity of data used in training, assessing the provenance of AI-generated content, and employing techniques like chaos engineering can help in monitoring and improving outputs. Defining use cases, contexts of use, capabilities, and negative impacts where structured human feedback exercises, such as GAI red-teaming, would be most beneficial can also contribute to GAI risk measurement and management based on the context of use.', 'verdict': 1}\n",
      "Generating:  30%|███       | 6/20 [00:06<00:08,  1.65it/s][ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI content', 'Provenance data', 'Content provenance', 'Incident disclosure', 'AI incidents']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Healthcare navigators', 'Automated customer service', 'Ballot curing laws', 'Fallback system', 'Voter signature matching algorithm']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['National Science Foundation', 'Artificial Intelligence Research Institutes', 'Cyber-Physical Systems', 'Facial Recognition Match', 'Educational Redlining']\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"How does the Blueprint for an AI Bill of Rights aim to assist in moving principles into practice, considering the relationship to existing law and policy?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question seeks to understand the objectives of the Blueprint for an AI Bill of Rights in relation to automated system development and its implications for civil rights, equal opportunities, and access to resources. It is specific in its focus on the Blueprint and the principles it aims to implement, making the intent clear. However, the question is somewhat complex and may benefit from simplification or breaking down into smaller parts for clarity. For example, it could separately ask about the principles outlined in the Blueprint and then inquire about their potential impacts. This would enhance answerability by allowing for more focused responses.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How do language models contribute to the reduction of content diversity?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is specific in asking about the procedures for remediation of issues related to incident response processes in the context of a GAI system. It also clearly seeks information on how stakeholders should be informed about timelines for the remediation plan. However, the question could be improved by clarifying what is meant by 'remediation of issues' and 'incident response processes', as these terms may vary in interpretation. Additionally, specifying the types of stakeholders involved or the context of the GAI system could enhance clarity. To improve answerability, the question could be reframed to include examples of issues or types of stakeholders.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What procedures should be established and maintained for the remediation of issues triggering incident response processes for the use of a GAI system, and how should stakeholders be provided with timelines associated with the remediation plan?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How can organizations enhance content provenance through structured public feedback in relation to GAI content?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is specific and seeks to understand the application of measurement error models in establishing construct validity for pre-deployment metrics within the context of AI system trustworthiness assessments. It clearly identifies the topic (measurement error models, construct validity, pre-deployment metrics, AI system trustworthiness) and the intent is to explore their utilization. However, the complexity of the terminology may pose a challenge for those not familiar with the specific concepts involved. To enhance clarity and answerability, the question could briefly define key terms such as 'construct validity' and 'measurement error models' or provide context on what the MEASURE function entails. This would make the question more accessible to a broader audience while retaining its specificity.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How do ballot curing laws provide a fallback system for voters to correct their ballots in case of issues?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the involvement of stakeholders in the development of the AI Bill of Rights, focusing on their contributions and positive use cases. They share the same depth and breadth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] evolution_filter failed, retrying with 1\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What were the private sector and civil society's contributions to the AI Bill of Rights Blueprint, including positive use cases?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What are some funding opportunities provided by the National Science Foundation in the field of artificial intelligence and cyber-physical systems?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How does the AI Bill of Rights help with principles for automated systems, civil rights, equal opportunities, and resource access?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on the role of NIST in advancing reliable and safe AI, while the second question delves into specific actions and collaborations of NIST regarding secure AI, including its involvement with the U.S. AI Safety Institute. This indicates a difference in depth and breadth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What measures should be implemented for addressing incidents in GAI systems that trigger incident response processes, and how can stakeholders be informed about the timelines for the remediation plan?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking about the processes and procedures for safely decommissioning and phasing out AI systems while maintaining organizational trustworthiness. It does not rely on external references and can be understood independently. The intent is clear, seeking information on risk management and trust issues related to AI system decommissioning. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What processes and procedures are in place for decommissioning and phasing out AI systems safely and in a manner that does not increase risks or decrease the organization's trustworthiness?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How can measurement error models help establish construct validity for pre-deployment metrics in the MEASURE function for AI trust assessments?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on the Blueprint for an AI Bill of Rights and its role in assisting both governments and the private sector, while the second question emphasizes its help in implementing principles specifically with existing laws and policies. This difference in focus leads to a variation in depth and breadth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 3, 'score': 1.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Federal prisoners', 'Racial bias', 'Sentiment analyzer', 'Unintended bias', 'Automated test proctoring software']\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the contributions of private sector and civil society stakeholders to the AI Bill of Rights Blueprint, focusing on the same aspects and requiring similar depth and breadth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the stakeholders involved in providing ideas for the development of the Blueprint for an AI Bill of Rights. It is specific and clear in its intent, as it seeks information about the parties or groups that contributed to this initiative. However, it may benefit from additional context regarding what the Blueprint for an AI Bill of Rights entails or the timeframe of its development, as this could help clarify the scope of the stakeholders being referred to. Overall, the question is understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How is the Justice Department working to curb racial bias in deciding who's released from prison?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks how organizations can improve content provenance by utilizing structured public feedback specifically in the context of GAI (Generative AI) content. It is clear in its intent, specifying both the subject (organizations) and the focus area (content provenance through structured public feedback related to GAI content). However, the term 'content provenance' may not be universally understood, and the question could benefit from a brief definition or context regarding what is meant by 'structured public feedback' and how it relates to GAI content. This would enhance clarity and ensure that a wider audience can understand and answer the question effectively.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for information about funding opportunities from the National Science Foundation (NSF) specifically in the fields of artificial intelligence and cyber-physical systems. It is clear in its intent and specifies the organization (NSF) and the relevant fields of interest. The question is self-contained and does not rely on external references, making it understandable and answerable based on general knowledge of NSF funding. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What established protocols and considerations should be taken into account when decommissioning and phasing out AI systems to ensure safety, risk mitigation, and organizational trustworthiness?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about approaches for mapping AI technology and addressing legal risks, specifically mentioning intellectual property infringement and data privacy concerns. It is clear in its intent and specifies the areas of interest, making it understandable. However, the question could be improved by clarifying what is meant by 'mapping AI technology' and whether it refers to regulatory frameworks, compliance strategies, or something else. Additionally, providing context on the specific legal risks or jurisdictions of interest could enhance clarity and answerability. Overall, while the question is relatively clear, it could benefit from more specificity regarding the approaches and the context in which they are applied.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.0}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Unacceptable use', 'Harmful bias and homogenization', 'GAI risks', 'Information integrity', 'Transparent policies']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the role of language models in reducing content diversity, which is a specific and clear inquiry. It does not rely on external references or context, making it independent and understandable. However, the term 'content diversity' could be interpreted in various ways (e.g., diversity in topics, styles, perspectives), which may introduce some ambiguity. To enhance clarity, the question could specify what aspect of content diversity it is referring to, such as 'in online media', 'in generated text', or 'in user-generated content'.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"How do language models contribute to the reduction of content diversity?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses specifically on the Blueprint for an AI Bill of Rights and its role in assisting governments and the private sector, while the second question addresses broader themes of civil rights and resource access without mentioning the Blueprint or the specific audience. This leads to differences in constraints and depth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about how ballot curing laws serve as a fallback system for voters to correct their ballots when issues arise. It is specific in its focus on ballot curing laws and their function, making the intent clear. The question is independent and can be understood without needing additional context or external references. However, it could be improved by briefly defining 'ballot curing laws' for clarity, especially for those who may not be familiar with the term. Overall, the question is clear and answerable based on the details provided.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"How do ballot curing laws provide a fallback system for voters to correct their ballots in case of issues?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the application of measurement error models in establishing construct validity for pre-deployment metrics within the MEASURE function, with the second question adding a specific context of AI trust assessments. This slight difference in context does not significantly alter the depth or breadth of the inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How do LM's impact content diversity reduction?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How can organizations address harmful bias and homogenization in the development and deployment of GAI systems?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How do ballot curing laws ensure that voters have a mechanism to rectify ballot issues, especially when automated systems fail to accurately validate signatures?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 2, 'score': 1.25}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'NIST develops measurements, technology, tools, and standards to advance reliable, safe, transparent, explainable, privacy-enhanced, and fair artificial intelligence (AI) to realize its full commercial and societal benefits without harm to people or the planet. NIST established the U.S. AI Safety Institute and the companion AI Safety Institute Consortium to continue the efforts set in motion by the Executive Order on Safe, Secure, and Trustworthy AI, building the necessary science for safe, secure, and trustworthy development and use of AI.', 'verdict': 1}\n",
      "Generating:  35%|███▌      | 7/20 [00:09<00:18,  1.43s/it][ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
      "Generating:  40%|████      | 8/20 [00:09<00:12,  1.01s/it][ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI systems', 'Digital content transparency', 'Structured feedback', 'Adversarial testing', 'Interpretability and explainability methods']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the efforts of the Justice Department to address racial bias in the context of prison release decisions. It is specific and has a clear intent, focusing on a particular agency (the Justice Department) and a defined issue (racial bias in release decisions). The question is understandable and answerable based on general knowledge of the Justice Department's initiatives and policies. However, it could be improved by specifying a time frame or particular programs or policies being referenced, which would provide a more focused context for the answer.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: How is the Justice Department working to curb racial bias in deciding who's released from prison?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and seeks information on measures for addressing incidents in GAI systems and how stakeholders can be informed about remediation timelines. It is clear in its intent, asking for both measures and communication strategies. However, the question could be improved by clarifying what types of incidents are being referred to (e.g., security breaches, operational failures) and what specific stakeholders are involved (e.g., developers, users, regulatory bodies). Providing this context would enhance understanding and answerability for those unfamiliar with the specific incidents in GAI systems.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The Blueprint for an AI Bill of Rights is meant to assist governments and the private sector in moving principles into practice. The expectations given in the Technical Companion are meant to serve as a blueprint for the development of additional technical standards and practices that should be tailored for particular sectors and contexts. While existing laws informed the development of the Blueprint for an AI Bill of Rights, this framework does not detail those laws beyond providing them as examples, where appropriate, of existing protective measures. This framework instead shares a broad, forward-leaning vision of recommended principles for automated system development and use to inform private and public involvement with these systems where they have the potential to meaningfully impact rights, opportunities, or access.', 'verdict': 1}\n",
      "Generating:  45%|████▌     | 9/20 [00:10<00:08,  1.25it/s][ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and clear in its intent, asking for established protocols and considerations related to the decommissioning and phasing out of AI systems. It outlines the key areas of focus: safety, risk mitigation, and organizational trustworthiness. This makes it understandable and answerable based on the details provided. However, to enhance clarity, it could specify whether it is looking for general best practices, legal considerations, or specific case studies. Overall, the question meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and seeks to understand the role of ballot curing laws in providing voters with a mechanism to address ballot issues, particularly in the context of automated systems failing to validate signatures. It is clear in its intent and does not rely on external references, making it understandable and answerable based on the details provided. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What protocols are needed for decommissioning AI systems safely and maintaining trust?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the purpose of conducting adversarial testing in the context of GAI systems?\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Organizations can enhance content provenance through structured public feedback by integrating pre- and post-deployment external feedback into the monitoring process for GAI models and corresponding applications. This helps enhance awareness of performance changes and mitigate potential risks and harms from outputs. By capturing input from external sources, such as through AI red-teaming, organizations can gain valuable insights about authentication efficacy, vulnerabilities, impacts of adversarial threats, and unintended consequences resulting from the utilization of content provenance approaches on users and communities. Furthermore, tracking and documenting the provenance of datasets can help identify instances where AI-generated data may be a root cause of performance issues with the GAI system.', 'verdict': 1}\n",
      "Generating:  50%|█████     | 10/20 [00:10<00:07,  1.34it/s][ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks how organizations can tackle harmful bias and homogenization in the development and deployment of Generative AI (GAI) systems. It is specific in its focus on organizations and the issues of bias and homogenization, making the intent clear. However, the question could benefit from more specificity regarding the types of organizations (e.g., tech companies, governmental bodies) or the context in which these systems are being developed and deployed (e.g., specific industries or applications). Adding such details would enhance clarity and answerability, allowing for more targeted responses.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: How can organizations address harmful bias and homogenization in the development and deployment of GAI systems?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How to handle incidents in GAI systems and inform stakeholders about remediation timelines?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': \"The AI Bill of Rights helps by providing a framework that outlines protections for automated systems that have the potential to impact individuals' exercise of civil rights, equal opportunities, and access to critical resources or services. It ensures that these rights are enjoyed equally and fully protected, regardless of the changing role of automated systems in our lives.\", 'verdict': 1}\n",
      "Generating:  55%|█████▌    | 11/20 [00:10<00:04,  1.81it/s][ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the impact of language models (LMs) on content diversity reduction. While it specifies the subject (language models) and the focus (content diversity reduction), it is somewhat vague. The term 'content diversity reduction' could be interpreted in various ways, and the question does not clarify what specific aspects of content diversity are being referred to (e.g., in terms of topics, perspectives, or styles). To improve clarity and answerability, the question could specify the context in which LMs are being evaluated (e.g., social media, news articles) and what is meant by 'content diversity reduction' (e.g., a decrease in the variety of topics covered, homogenization of viewpoints).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"How do LM's impact content diversity reduction?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How do ballot curing laws help voters fix ballot issues when automated systems fail to validate signatures?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How can organizations mitigate harmful bias and homogenization in GAI systems while ensuring transparent risk management and safe decommissioning?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Approaches for mapping AI technology and addressing legal risks, including intellectual property infringement and data privacy concerns, involve conducting periodic monitoring of AI-generated content for privacy risks, implementing processes for responding to potential intellectual property infringement claims or other rights, connecting new GAI policies to existing governance and legal activities, documenting training data curation policies, establishing policies for data collection and retention, implementing policies for third-party intellectual property and training data usage, re-evaluating models and risks when adapting to new domains, and leveraging approaches to detect PII or sensitive data in generated output.', 'verdict': 1}\n",
      "Generating:  60%|██████    | 12/20 [00:11<00:04,  1.78it/s][ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question is broader, asking about processes and procedures, while the second question focuses specifically on protocols. This difference in scope and depth leads to different requirements for the answers.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the purpose of conducting adversarial testing specifically in the context of Generative AI (GAI) systems. It is clear and specific, indicating the topic of interest (adversarial testing) and its application (GAI systems). The intent is unambiguous, allowing for a direct response regarding the reasons and benefits of such testing. Therefore, the question meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What is the purpose of conducting adversarial testing in the context of GAI systems?\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question is more detailed and specific about the procedures and maintenance for remediation, while the second question is more general and lacks the depth regarding the establishment of procedures. Therefore, they do not have the same depth and breadth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the role of ballot curing laws in assisting voters with ballot issues, focusing on the corrective measures available. They share similar constraints and requirements, as well as depth and breadth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the impact of language models (LMs) on content diversity reduction. While it specifies the subject (language models) and the focus (content diversity reduction), it is somewhat vague as it does not clarify what is meant by 'impact' or 'content diversity reduction'. Additionally, the abbreviation 'LM' may not be universally understood without context. To improve clarity and answerability, the question could specify the type of impact being considered (e.g., quantitative, qualitative) and provide a definition or context for 'content diversity reduction'.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"Why is it essential to regularly conduct adversarial testing to assess and mitigate risks in GAI systems, considering the need for structured feedback and monitoring of system impacts across various sub-populations?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 3, 'structure': 3, 'relevance': 3, 'score': 3.0}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Notice of use', 'Explanations for decisions', 'Plain language documentation', 'Accountability']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question seeks to understand how the Justice Department is addressing racial bias in prison release decisions, referencing reports from NPR and the National Institute of Justice, as well as mentioning flaws in the risk assessment tool and the First Step Act. While it specifies the topic of interest (racial bias in prison release decisions) and the sources (NPR and the National Institute of Justice), it assumes familiarity with these reports and the specific flaws in the risk assessment tool and the First Step Act without providing any context. This reliance on external references makes the question less clear and answerable for those who may not have access to or knowledge of these sources. To improve clarity and answerability, the question could briefly summarize the key findings or issues raised in the NPR and National Institute of Justice reports, or clarify what specific aspects of the Justice Department's response are being inquired about.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: How is the Justice Department addressing racial bias in prison release decisions, as reported by NPR and the National Institute of Justice, in light of flaws in the risk assessment tool and the First Step Act?\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': \"Protocols are needed for decommissioning AI systems safely and maintaining trust, including ensuring GAI systems can be deactivated when necessary and considering factors such as data retention requirements, data security, dependencies between systems, use of open-source data or models, and users' emotional entanglement with GAI functions.\", 'verdict': 1}\n",
      "Generating:  65%|██████▌   | 13/20 [00:13<00:06,  1.01it/s][ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI systems', 'Digital content transparency', 'Structured feedback', 'Adversarial testing', 'Interpretability and explainability methods']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What is the importance of providing plain language documentation in automated systems according to the given context?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Establish and maintain procedures for escalating GAI system incidents to the organizational risk management authority when specific criteria for deactivation or disengagement is met for a particular context of use or for the GAI system as a whole. Establish and maintain procedures for the remediation of issues which trigger incident response processes for the use of a GAI system, and provide stakeholders timelines associated with the remediation plan.', 'verdict': 1}\n",
      "Generating:  70%|███████   | 14/20 [00:13<00:04,  1.26it/s][ragas.testset.evolutions.DEBUG] answer generated: {'answer': \"Stakeholders involved in providing ideas related to the development of the Blueprint for an AI Bill of Rights included Adobe, American Civil Liberties Union (ACLU), The Aspen Commission on Information Disorder, The Awood Center, The Australian Human Rights Commission, Biometrics Institute, The Brookings Institute, BSA | The Software Alliance, Cantellus Group, Center for American Progress, Center for Democracy and Technology, Center on Privacy and Technology at Georgetown Law, Christiana Care, Color of Change, Coworker, Data Robot, Data Trust Alliance, Data and Society Research Institute, Deepmind, EdSAFE AI Alliance, Electronic Privacy Information Center (EPIC), Encode Justice, Equal AI, Google, Hitachi's AI Policy Committee, The Innocence Project, Institute of Electrical and Electronics Engineers (IEEE), Intuit, Lawyers Committee for Civil Rights Under Law, Legal Aid Society, The Leadership Conference on Civil and Human Rights, Meta, Microsoft, The MIT AI Policy Forum, Movement Alliance Project, The National Association of Criminal Defense Lawyers, O'Neil Risk Consulting & Algorithmic Auditing, The Partnership on AI, Pinterest, The Plaintext Group, pymetrics, SAP, The Security Industry Association, Software and Information Industry Association (SIIA), Special Competitive Studies Project, Thorn, United for Respect, University of California at Berkeley Citris Policy Lab, University of California at Berkeley Labor Center, Unfinished/Project Liberty, Upturn, US Chamber of Commerce, US Chamber of Commerce Technology Engagement Center, A.I. Working Group, Vibrent Health, Warehouse Worker Resource Center, Waymap\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How can structured feedback about content provenance be integrated into the design, implementation, and deployment decisions of AI systems?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is complex and multifaceted, asking about the importance of regular adversarial testing in GAI systems while also considering structured feedback and monitoring impacts across sub-populations. While it is specific in its focus on adversarial testing and GAI systems, the phrasing is somewhat convoluted, which may lead to ambiguity in intent. To improve clarity and answerability, the question could be broken down into simpler components or rephrased to focus on one aspect at a time, such as first asking about the importance of adversarial testing and then separately addressing the role of structured feedback and monitoring. This would make it easier for respondents to provide clear and focused answers.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"Why is it essential to regularly conduct adversarial testing to assess and mitigate risks in GAI systems, considering the need for structured feedback and monitoring of system impacts across various sub-populations?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question addresses the complex issue of mitigating harmful bias and homogenization in Generative AI (GAI) systems, while also focusing on transparent risk management and safe decommissioning. It is specific in its intent and covers multiple aspects of GAI systems, making it clear what information is being sought. However, the question may be challenging for some audiences due to the technical nature of the terms used (e.g., 'harmful bias', 'homogenization', 'transparent risk management', 'safe decommissioning') without providing definitions or context. To improve clarity and answerability, the question could benefit from breaking down these concepts or providing examples of what is meant by each term. Additionally, specifying the type of organizations (e.g., tech companies, regulatory bodies) could help tailor the response more effectively.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the importance of plain language documentation in automated systems, referencing 'the given context' without providing that context within the question itself. This reliance on unspecified external information makes the question unclear and potentially unanswerable for those who do not have access to the context. To improve clarity and answerability, the question could either include a brief summary of the relevant context or be rephrased to focus on the general importance of plain language documentation in automated systems without needing specific external references.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What is the importance of providing plain language documentation in automated systems according to the given context?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How can organizations address bias and homogenization in GAI systems while ensuring transparency and safety?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the integration of structured feedback regarding content provenance into various stages of AI system development (design, implementation, and deployment). It is specific in its focus on structured feedback and content provenance, making the intent clear. However, the question may be challenging for those unfamiliar with the concepts of content provenance or structured feedback in the context of AI systems. To improve clarity and answerability, it could be beneficial to briefly define 'structured feedback' and 'content provenance' or provide examples of how these concepts might influence AI system decisions. This would help ensure that a wider audience can understand and engage with the question.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"How can structured feedback about content provenance be integrated into the design, implementation, and deployment decisions of AI systems?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question seeks to understand how the Justice Department is addressing racial bias in prison release decisions, referencing reports from NPR and the National Institute of Justice, as well as mentioning flaws in the risk assessment tool and the First Step Act. While it specifies the topic of interest (racial bias in prison release decisions) and the sources (NPR and the National Institute of Justice), it assumes familiarity with these reports and the specific flaws in the risk assessment tool without providing any context. This reliance on external references makes the question less independent and potentially unclear for those not familiar with the mentioned sources or the First Step Act. To improve clarity and answerability, the question could briefly summarize the key findings or issues raised by NPR and the National Institute of Justice regarding the risk assessment tool, or clarify what specific aspects of the Justice Department's response are being inquired about.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions focus on addressing bias and homogenization in GAI systems, but the second question introduces additional requirements regarding transparency and safety, which alters the depth and breadth of the inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How can feedback on content origin be effectively incorporated into AI system design, implementation, and deployment decisions to ensure transparency and mitigate risks?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 2, 'score': 1.25}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is specific and seeks to understand the importance of regular adversarial testing in GAI systems, particularly in relation to structured feedback and monitoring impacts on different sub-populations. It is clear in its intent and does not rely on external references, making it understandable. However, the complexity of the terms used (e.g., 'adversarial testing', 'GAI systems', 'structured feedback') may pose a challenge for those unfamiliar with the domain. To enhance clarity, the question could be simplified or broken down into more specific components, such as asking about the benefits of adversarial testing in general before linking it to structured feedback and sub-populations.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the importance of plain language documentation in automated systems, referencing 'the given context' without providing that context within the question itself. This reliance on unspecified external information makes the question unclear and potentially unanswerable for those who do not have access to the context. To improve clarity and answerability, the question could either include a brief summary of the relevant context or be rephrased to focus on general principles regarding the importance of plain language documentation in automated systems, independent of specific contexts.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"Why is regular adversarial testing important for GAI systems to manage risks and monitor impacts on different sub-populations?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated system', 'Plain language documentation', 'Outcome impact', 'Explanation of outcomes', 'Notice and explanation']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Decommissioning AI systems', 'Data retention requirements', 'Roles and responsibilities', 'AI incident response tasks', 'National security risks']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How should designers, developers, and deployers of automated systems provide notice and explanation to users impacted by the system?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Organizations can address bias and homogenization in GAI systems by obtaining input from stakeholder communities to identify unacceptable use, maintaining an updated hierarchy of identified and expected GAI risks, reevaluating organizational risk tolerances, and devising a plan to halt development or deployment of a GAI system that poses unacceptable negative risk. Transparency and safety can be ensured through establishing policies and mechanisms to prevent GAI systems from generating harmful content, establishing transparent acceptable use policies, and implementing controls based on organizational risk priorities.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What processes should be established to verify the AI Actors conducting GAI incident response tasks demonstrate and maintain the appropriate skills and training?\n",
      "Generating:  80%|████████  | 16/20 [00:18<00:06,  1.59s/it][ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks about the general purpose of adversarial testing in GAI systems, while the second question specifically addresses the importance of this testing for managing risks and monitoring impacts on sub-populations. This indicates a difference in depth and breadth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks how feedback on content origin can be integrated into AI system design, implementation, and deployment to enhance transparency and reduce risks. It is specific in its focus on feedback regarding content origin and its implications for AI systems. However, the question is somewhat complex and may benefit from simplification or breaking down into more manageable parts. For improved clarity, it could specify what types of feedback are being referred to (e.g., user feedback, algorithmic feedback) and what specific risks or transparency issues are of concern. Additionally, providing examples of AI systems or contexts could help ground the question further.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How can feedback improve AI system design and reduce risks?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Regular adversarial testing is important for GAI systems to manage risks and monitor impacts on different sub-populations because it helps in mapping and measuring GAI risks, including attempts to deceive or manipulate provenance techniques or other misuses. By conducting adversarial testing, vulnerabilities can be identified, potential misuse scenarios understood, and unintended outputs revealed, contributing to the overall trustworthiness and reliability of the system.', 'verdict': 1}\n",
      "Generating:  85%|████████▌ | 17/20 [00:20<00:05,  1.68s/it][ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question specifically addresses structured feedback about content provenance and its integration into AI systems, while the second question is more general about feedback improving AI system design and reducing risks. This leads to differences in constraints and depth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks how various stakeholders (designers, developers, and deployers) of automated systems should inform and explain to users who are affected by these systems. It is clear in its intent, specifying the roles involved and the focus on user communication. However, the question could benefit from being more specific about the type of automated systems in question (e.g., AI systems, software applications) or the context in which this notice and explanation should be provided (e.g., legal compliance, ethical considerations). Adding such details would enhance clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: \"How should designers, developers, and deployers of automated systems provide notice and explanation to users impacted by the system?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the processes needed to verify that AI Actors involved in GAI incident response tasks possess and maintain the necessary skills and training. It is specific in its focus on verification processes and the context of AI Actors in GAI incident response. However, the term 'GAI' may not be universally understood, and the question could benefit from a brief definition or context for clarity. Additionally, specifying the types of skills and training relevant to the incident response tasks could enhance the question's clarity and answerability. Overall, while the intent is clear, providing more context would improve understanding.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What processes should be established to verify the AI Actors conducting GAI incident response tasks demonstrate and maintain the appropriate skills and training?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: What measures should be implemented to ensure that AI Actors involved in GAI incident response tasks possess and uphold the necessary competencies and training standards?\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': \"Feedback can improve AI system design and reduce risks by providing structured input from operators, users, and potentially impacted communities. This feedback can help identify issues related to content provenance, harmful bias, and homogenization, allowing for adjustments to be made to enhance the system's trustworthiness and performance. By actively seeking feedback on content quality and potential biases, AI developers can address vulnerabilities, understand misuse scenarios, and ensure that the system aligns with its intended purpose.\", 'verdict': 1}\n",
      "Generating:  90%|█████████ | 18/20 [00:22<00:03,  1.84s/it][ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking about the measures needed to ensure that AI Actors involved in General Artificial Intelligence (GAI) incident response tasks have the required competencies and training standards. It does not rely on external references and can be understood independently. However, it could be improved by specifying what types of competencies and training standards are being referred to, as this would help narrow down the response and provide more targeted information. For example, mentioning specific skills, knowledge areas, or training methodologies could enhance clarity.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking how various stakeholders (designers, developers, and deployers) of automated systems can ensure that users are informed about key aspects such as system functioning, responsible parties, and outcome explanations. It does not rely on external references and conveys a clear intent, making it understandable and answerable. However, to enhance clarity, the question could specify the type of automated systems being referred to (e.g., AI systems, software applications) or the context in which this information should be provided (e.g., user interfaces, documentation). This would help narrow down the focus and provide more targeted responses.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What steps are needed for AI Actors in GAI incident response to have the required competencies and training?\"\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"How should designers, developers, and deployers of automated systems ensure users are informed about system functioning, responsible parties, and outcome explanations?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the necessary processes and steps for AI Actors in GAI incident response to ensure they possess the required skills and training, indicating they have the same constraints and requirements.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on the notice and explanation provided to users impacted by automated systems, while the second question emphasizes informing users about functioning, responsible parties, and outcomes. The scope and depth of inquiry differ significantly, leading to different requirements.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Algorithmic discrimination', 'Independent evaluation', 'Algorithmic impact assessment', 'Reporting']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What expectations should be met by automated systems according to the provided context?\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Designers, developers, and deployers of automated systems should provide generally accessible plain language documentation that includes clear descriptions of the overall system functioning, the role of automation, notice of system usage, identification of responsible parties, and explanations of outcomes in a clear, timely, and accessible manner. Users impacted by the system should be notified of significant changes in use cases or key functionalities.', 'verdict': 1}\n",
      "Generating:  95%|█████████▌| 19/20 [00:27<00:02,  2.72s/it][ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the expectations for automated systems based on 'the provided context', but it does not include or describe this context within the query. This reliance on unspecified external information makes the question unclear and unanswerable for those who do not have access to that context. To improve clarity and answerability, the question should either include the relevant context directly or specify the type of expectations being referred to (e.g., performance, reliability, ethical considerations).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: What expectations should be met by automated systems according to the provided context?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the expectations for automated systems based on 'the provided context', but it does not include or describe this context within the question itself. This reliance on unspecified external information makes the question unclear and unanswerable for those who do not have access to that context. To improve clarity and answerability, the question should either include the relevant context directly or specify the type of expectations being referred to (e.g., performance, reliability, ethical considerations).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 3 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI Bill of Rights', 'White House Office of Science and Technology Policy', 'Automated Systems', 'Civil Rights', 'Democratic Values']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What is the purpose of the Blueprint for an AI Bill of Rights in protecting civil rights and promoting democratic values in the building, deployment, and governance of automated systems?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question clearly asks about the purpose of the Blueprint for an AI Bill of Rights in relation to civil rights and democratic values, specifically in the context of automated systems. It is specific and independent, as it does not rely on external references or additional context to be understood. The intent is clear, seeking an explanation of the Blueprint's role in protecting rights and promoting values. Therefore, it meets the criteria for clarity and answerability.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What is the purpose of the Blueprint for an AI Bill of Rights in protecting civil rights and promoting democratic values in the building, deployment, and governance of automated systems?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What principles and practices outlined in the Blueprint for an AI Bill of Rights aim to safeguard civil rights and democratic values in the development, implementation, and management of automated systems, and how do they relate to existing laws and policies?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question seeks to understand the principles and practices in the Blueprint for an AI Bill of Rights that are intended to protect civil rights and democratic values in relation to automated systems. It also asks how these principles relate to existing laws and policies. While the question is specific and has a clear intent, it assumes familiarity with the Blueprint for an AI Bill of Rights and existing laws and policies without providing any context or definitions. To improve clarity and answerability, the question could briefly summarize what the Blueprint entails or specify which existing laws and policies are being referenced. This would help ensure that the question is understandable to a broader audience.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What principles and practices outlined in the Blueprint for an AI Bill of Rights aim to safeguard civil rights and democratic values in the development, implementation, and management of automated systems, and how do they relate to existing laws and policies?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question seeks to understand the principles and practices in the Blueprint for an AI Bill of Rights that are intended to protect civil rights and democratic values in relation to automated systems. It also asks how these principles relate to existing laws and policies. While the question is specific and has a clear intent, it assumes familiarity with the 'Blueprint for an AI Bill of Rights' and existing laws and policies without providing any context or definitions. To improve clarity and answerability, the question could briefly summarize what the Blueprint entails or specify which existing laws and policies are being referenced. This would help ensure that the question is understandable to a broader audience.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 3 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.0}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Unacceptable use', 'Harmful bias and homogenization', 'GAI risks', 'Information integrity', 'Transparent policies']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How is information integrity addressed in the risk management process for GAI systems?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about how information integrity is addressed within the risk management process specifically for GAI (General Artificial Intelligence) systems. It is clear in its intent, focusing on a specific aspect of risk management related to information integrity. However, the term 'information integrity' could be interpreted in various ways, and the question does not provide any specific context or framework for what is meant by 'risk management process' in this scenario. To improve clarity and answerability, the question could specify what aspects of information integrity are of interest (e.g., data accuracy, security measures) and provide a brief description of the risk management process being referred to, such as whether it is a theoretical framework or a practical application in a specific industry.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: How is information integrity addressed in the risk management process for GAI systems?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about how information integrity is addressed within the risk management process specifically for GAI (General Artificial Intelligence) systems. It is clear in its intent, focusing on a specific aspect of risk management related to information integrity. However, the term 'information integrity' could be interpreted in various ways, and the question does not provide any specific context or examples of what aspects of information integrity are of interest (e.g., data accuracy, consistency, security). To improve clarity and answerability, the question could specify which elements of information integrity are being referred to or provide a brief context about the risk management process in GAI systems.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: How is information integrity addressed in the risk management process for GAI systems?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How is the risk management process for GAI systems designed to ensure information integrity and safe decommissioning?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the design of the risk management process for General Artificial Intelligence (GAI) systems, specifically focusing on ensuring information integrity and safe decommissioning. It is clear in its intent and specifies the topic of interest, making it understandable. However, the term 'risk management process' could be interpreted in various ways, and the question does not provide any specific context or criteria for what aspects of the process are of interest. To improve clarity and answerability, the question could specify which elements of the risk management process are being referred to (e.g., assessment, mitigation strategies, monitoring) or provide examples of what is meant by 'information integrity' and 'safe decommissioning'.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How does the risk management process for GAI systems ensure information integrity and safe decommissioning?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions focus on the risk management process for GAI systems and address the topic of information integrity. However, the second question introduces the additional aspect of safe decommissioning, which expands the breadth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
      "Generating: 100%|██████████| 20/20 [00:48<00:00,  2.43s/it]\n"
     ]
    }
   ],
   "source": [
    "testset = generator.generate_with_langchain_docs(documents, 20, distributions, with_debugging_logs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21f39721-322c-4557-86e8-54086fcd8e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "190c6752-59bf-440c-815e-5ba839880b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What approaches are in place for mapping AI te...</td>\n",
       "      <td>[ \\n26 \\nMAP 4.1: Approaches for mapping AI te...</td>\n",
       "      <td>Approaches for mapping AI technology and addre...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are some funding opportunities provided b...</td>\n",
       "      <td>[ \\nENDNOTES\\n23. National Science Foundation....</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How should the public be involved in the consu...</td>\n",
       "      <td>[ \\n \\n \\n \\n \\n \\n \\nSAFE AND EFFECTIVE \\nSYS...</td>\n",
       "      <td>The public should be involved in the consultat...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can structured feedback mechanisms be used...</td>\n",
       "      <td>[ \\n29 \\nMS-1.1-006 \\nImplement continuous mon...</td>\n",
       "      <td>Structured feedback mechanisms can be used to ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do language models contribute to the reduc...</td>\n",
       "      <td>[ \\n57 \\nNational Institute of Standards and T...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How can organizations enhance content provenan...</td>\n",
       "      <td>[ \\n52 \\n• \\nMonitoring system capabilities an...</td>\n",
       "      <td>Organizations can enhance content provenance t...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How do GAI-based systems present primary infor...</td>\n",
       "      <td>[ \\n10 \\nGAI systems can ease the unintentiona...</td>\n",
       "      <td>GAI-based systems present primary information ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can evaluations involving human subjects m...</td>\n",
       "      <td>[ \\n30 \\nMEASURE 2.2: Evaluations involving hu...</td>\n",
       "      <td>Evaluations involving human subjects in the co...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What stakeholders were involved in providing i...</td>\n",
       "      <td>[APPENDIX\\n• OSTP conducted meetings with a va...</td>\n",
       "      <td>Stakeholders involved in providing ideas relat...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do companies use surveillance software to ...</td>\n",
       "      <td>[ \\n \\n  \\n \\nDATA PRIVACY \\nWHY THIS PRINCIPL...</td>\n",
       "      <td>Companies use surveillance software to track e...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How can feedback improve AI system design and ...</td>\n",
       "      <td>[ \\n39 \\nMS-3.3-004 \\nProvide input for traini...</td>\n",
       "      <td>Feedback can improve AI system design and redu...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What does NIST do to support secure AI with tr...</td>\n",
       "      <td>[ \\n \\n \\nAbout AI at NIST: The National Insti...</td>\n",
       "      <td>NIST develops measurements, technology, tools,...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How to handle incidents in GAI systems and inf...</td>\n",
       "      <td>[ \\n42 \\nMG-2.4-002 \\nEstablish and maintain p...</td>\n",
       "      <td>Establish and maintain procedures for escalati...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How does the risk management process for GAI s...</td>\n",
       "      <td>[ \\n15 \\nGV-1.3-004 Obtain input from stakehol...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Why is regular adversarial testing important f...</td>\n",
       "      <td>[ \\n39 \\nMS-3.3-004 \\nProvide input for traini...</td>\n",
       "      <td>Regular adversarial testing is important for G...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How can organizations address bias and homogen...</td>\n",
       "      <td>[ \\n15 \\nGV-1.3-004 Obtain input from stakehol...</td>\n",
       "      <td>Organizations can address bias and homogenizat...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How does the AI Bill of Rights help with princ...</td>\n",
       "      <td>[ \\n \\n \\nSECTION TITLE\\nApplying The Blueprin...</td>\n",
       "      <td>The AI Bill of Rights helps by providing a fra...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What protocols are needed for decommissioning ...</td>\n",
       "      <td>[ \\n17 \\nGOVERN 1.7: Processes and procedures ...</td>\n",
       "      <td>Protocols are needed for decommissioning AI sy...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How does the AI Bill of Rights Blueprint help ...</td>\n",
       "      <td>[SECTION TITLE\\n \\n \\n \\n \\n \\n \\nApplying The...</td>\n",
       "      <td>The Blueprint for an AI Bill of Rights is mean...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can system creators inform users about fun...</td>\n",
       "      <td>[ \\nYou should know that an automated system i...</td>\n",
       "      <td>Designers, developers, and deployers of automa...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': '/Users/richardlai/Documents/MyPro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   What approaches are in place for mapping AI te...   \n",
       "1   What are some funding opportunities provided b...   \n",
       "2   How should the public be involved in the consu...   \n",
       "3   How can structured feedback mechanisms be used...   \n",
       "4   How do language models contribute to the reduc...   \n",
       "5   How can organizations enhance content provenan...   \n",
       "6   How do GAI-based systems present primary infor...   \n",
       "7   How can evaluations involving human subjects m...   \n",
       "8   What stakeholders were involved in providing i...   \n",
       "9   How do companies use surveillance software to ...   \n",
       "10  How can feedback improve AI system design and ...   \n",
       "11  What does NIST do to support secure AI with tr...   \n",
       "12  How to handle incidents in GAI systems and inf...   \n",
       "13  How does the risk management process for GAI s...   \n",
       "14  Why is regular adversarial testing important f...   \n",
       "15  How can organizations address bias and homogen...   \n",
       "16  How does the AI Bill of Rights help with princ...   \n",
       "17  What protocols are needed for decommissioning ...   \n",
       "18  How does the AI Bill of Rights Blueprint help ...   \n",
       "19  How can system creators inform users about fun...   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   [ \\n26 \\nMAP 4.1: Approaches for mapping AI te...   \n",
       "1   [ \\nENDNOTES\\n23. National Science Foundation....   \n",
       "2   [ \\n \\n \\n \\n \\n \\n \\nSAFE AND EFFECTIVE \\nSYS...   \n",
       "3   [ \\n29 \\nMS-1.1-006 \\nImplement continuous mon...   \n",
       "4   [ \\n57 \\nNational Institute of Standards and T...   \n",
       "5   [ \\n52 \\n• \\nMonitoring system capabilities an...   \n",
       "6   [ \\n10 \\nGAI systems can ease the unintentiona...   \n",
       "7   [ \\n30 \\nMEASURE 2.2: Evaluations involving hu...   \n",
       "8   [APPENDIX\\n• OSTP conducted meetings with a va...   \n",
       "9   [ \\n \\n  \\n \\nDATA PRIVACY \\nWHY THIS PRINCIPL...   \n",
       "10  [ \\n39 \\nMS-3.3-004 \\nProvide input for traini...   \n",
       "11  [ \\n \\n \\nAbout AI at NIST: The National Insti...   \n",
       "12  [ \\n42 \\nMG-2.4-002 \\nEstablish and maintain p...   \n",
       "13  [ \\n15 \\nGV-1.3-004 Obtain input from stakehol...   \n",
       "14  [ \\n39 \\nMS-3.3-004 \\nProvide input for traini...   \n",
       "15  [ \\n15 \\nGV-1.3-004 Obtain input from stakehol...   \n",
       "16  [ \\n \\n \\nSECTION TITLE\\nApplying The Blueprin...   \n",
       "17  [ \\n17 \\nGOVERN 1.7: Processes and procedures ...   \n",
       "18  [SECTION TITLE\\n \\n \\n \\n \\n \\n \\nApplying The...   \n",
       "19  [ \\nYou should know that an automated system i...   \n",
       "\n",
       "                                         ground_truth evolution_type  \\\n",
       "0   Approaches for mapping AI technology and addre...         simple   \n",
       "1   The answer to given question is not present in...         simple   \n",
       "2   The public should be involved in the consultat...         simple   \n",
       "3   Structured feedback mechanisms can be used to ...         simple   \n",
       "4   The answer to given question is not present in...         simple   \n",
       "5   Organizations can enhance content provenance t...         simple   \n",
       "6   GAI-based systems present primary information ...         simple   \n",
       "7   Evaluations involving human subjects in the co...         simple   \n",
       "8   Stakeholders involved in providing ideas relat...         simple   \n",
       "9   Companies use surveillance software to track e...         simple   \n",
       "10  Feedback can improve AI system design and redu...  multi_context   \n",
       "11  NIST develops measurements, technology, tools,...  multi_context   \n",
       "12  Establish and maintain procedures for escalati...  multi_context   \n",
       "13  The answer to given question is not present in...  multi_context   \n",
       "14  Regular adversarial testing is important for G...  multi_context   \n",
       "15  Organizations can address bias and homogenizat...  multi_context   \n",
       "16  The AI Bill of Rights helps by providing a fra...  multi_context   \n",
       "17  Protocols are needed for decommissioning AI sy...  multi_context   \n",
       "18  The Blueprint for an AI Bill of Rights is mean...      reasoning   \n",
       "19  Designers, developers, and deployers of automa...      reasoning   \n",
       "\n",
       "                                             metadata  episode_done  \n",
       "0   [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "1   [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "2   [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "3   [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "4   [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "5   [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "6   [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "7   [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "8   [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "9   [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "10  [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "11  [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "12  [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "13  [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "14  [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "15  [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "16  [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "17  [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "18  [{'source': '/Users/richardlai/Documents/MyPro...          True  \n",
       "19  [{'source': '/Users/richardlai/Documents/MyPro...          True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c58afb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = test_df[\"question\"].values.tolist()\n",
    "test_groundtruths = test_df[\"ground_truth\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6f46e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='What approaches are in place for mapping AI technology and addressing legal risks, including intellectual property infringement and data privacy concerns?' id='803d0bdc-f5b9-4235-b529-410254b4a7a5'\n",
      "****Adding new context: [Document(metadata={'page': 29, 'modDate': \"D:20240805143048-04'00'\", 'author': 'National Institute of Standards and Technology', 'producer': 'Adobe PDF Library 24.2.159', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'trapped': '', 'keywords': '', 'total_pages': 64, 'subject': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'creationDate': \"D:20240805141702-04'00'\", 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'format': 'PDF 1.6', '_id': '218e6d62-0aaa-4104-a81e-0be56a17ae16', '_collection_name': 'ai_policy'}, page_content='26 \\nMAP 4.1: Approaches for mapping AI technology and legal risks of its components – including the use of third-party data or \\nsoftware – are in place, followed, and documented, as are risks of infringement of a third-party’s intellectual property or other \\nrights. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMP-4.1-001 Conduct periodic monitoring of AI-generated content for privacy risks; address any \\npossible instances of PII or sensitive data exposure. \\nData Privacy \\nMP-4.1-002 Implement processes for responding to potential intellectual property infringement \\nclaims or other rights. \\nIntellectual Property \\nMP-4.1-003 \\nConnect new GAI policies, procedures, and processes to existing model, data, \\nsoftware development, and IT governance and to legal, compliance, and risk \\nmanagement activities. \\nInformation Security; Data Privacy \\nMP-4.1-004 Document training data curation policies, to the extent possible and according to \\napplicable laws and policies. \\nIntellectual Property; Data Privacy; \\nObscene, Degrading, and/or \\nAbusive Content \\nMP-4.1-005 \\nEstablish policies for collection, retention, and minimum quality of data, in \\nconsideration of the following risks: Disclosure of inappropriate CBRN information; \\nUse of Illegal or dangerous content; Oﬀensive cyber capabilities; Training data \\nimbalances that could give rise to harmful biases; Leak of personally identiﬁable \\ninformation, including facial likenesses of individuals. \\nCBRN Information or Capabilities; \\nIntellectual Property; Information \\nSecurity; Harmful Bias and \\nHomogenization; Dangerous, \\nViolent, or Hateful Content; Data \\nPrivacy \\nMP-4.1-006 Implement policies and practices deﬁning how third-party intellectual property and \\ntraining data will be used, stored, and protected. \\nIntellectual Property; Value Chain \\nand Component Integration \\nMP-4.1-007 Re-evaluate models that were ﬁne-tuned or enhanced on top of third-party \\nmodels. \\nValue Chain and Component \\nIntegration \\nMP-4.1-008 \\nRe-evaluate risks when adapting GAI models to new domains. Additionally, \\nestablish warning systems to determine if a GAI system is being used in a new \\ndomain where previous assumptions (relating to context of use or mapped risks \\nsuch as security, and safety) may no longer hold.  \\nCBRN Information or Capabilities; \\nIntellectual Property; Harmful Bias \\nand Homogenization; Dangerous, \\nViolent, or Hateful Content; Data \\nPrivacy \\nMP-4.1-009 Leverage approaches to detect the presence of PII or sensitive data in generated \\noutput text, image, video, or audio. \\nData Privacy'), Document(metadata={'total_pages': 64, 'format': 'PDF 1.6', 'creator': 'Acrobat PDFMaker 24 for Word', 'subject': '', 'producer': 'Adobe PDF Library 24.2.159', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'page': 30, 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'creationDate': \"D:20240805141702-04'00'\", 'keywords': '', 'author': 'National Institute of Standards and Technology', 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', '_id': '69a678c9-76ac-455c-9b0f-be4ec4ab8914', '_collection_name': 'ai_policy'}, page_content=\"27 \\nMP-4.1-010 \\nConduct appropriate diligence on training data use to assess intellectual property, \\nand privacy, risks, including to examine whether use of proprietary or sensitive \\ntraining data is consistent with applicable laws.  \\nIntellectual Property; Data Privacy \\nAI Actor Tasks: Governance and Oversight, Operation and Monitoring, Procurement, Third-party entities \\n \\nMAP 5.1: Likelihood and magnitude of each identiﬁed impact (both potentially beneﬁcial and harmful) based on expected use, past \\nuses of AI systems in similar contexts, public incident reports, feedback from those external to the team that developed or deployed \\nthe AI system, or other data are identiﬁed and documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMP-5.1-001 Apply TEVV practices for content provenance (e.g., probing a system's synthetic \\ndata generation capabilities for potential misuse or vulnerabilities. \\nInformation Integrity; Information \\nSecurity \\nMP-5.1-002 \\nIdentify potential content provenance harms of GAI, such as misinformation or \\ndisinformation, deepfakes, including NCII, or tampered content. Enumerate and \\nrank risks based on their likelihood and potential impact, and determine how well \\nprovenance solutions address speciﬁc risks and/or harms. \\nInformation Integrity; Dangerous, \\nViolent, or Hateful Content; \\nObscene, Degrading, and/or \\nAbusive Content \\nMP-5.1-003 \\nConsider disclosing use of GAI to end users in relevant contexts, while considering \\nthe objective of disclosure, the context of use, the likelihood and magnitude of the \\nrisk posed, the audience of the disclosure, as well as the frequency of the \\ndisclosures. \\nHuman-AI Conﬁguration \\nMP-5.1-004 Prioritize GAI structured public feedback processes based on risk assessment \\nestimates. \\nInformation Integrity; CBRN \\nInformation or Capabilities; \\nDangerous, Violent, or Hateful \\nContent; Harmful Bias and \\nHomogenization \\nMP-5.1-005 Conduct adversarial role-playing exercises, GAI red-teaming, or chaos testing to \\nidentify anomalous or unforeseen failure modes. \\nInformation Security \\nMP-5.1-006 \\nProﬁle threats and negative impacts arising from GAI systems interacting with, \\nmanipulating, or generating content, and outlining known and potential \\nvulnerabilities and the likelihood of their occurrence. \\nInformation Security \\nAI Actor Tasks: AI Deployment, AI Design, AI Development, AI Impact Assessment, Aﬀected Individuals and Communities, End-\\nUsers, Operation and Monitoring\"), Document(metadata={'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'subject': '', 'total_pages': 64, 'producer': 'Adobe PDF Library 24.2.159', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'page': 25, 'author': 'National Institute of Standards and Technology', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'creationDate': \"D:20240805141702-04'00'\", 'format': 'PDF 1.6', 'modDate': \"D:20240805143048-04'00'\", 'keywords': '', 'trapped': '', 'creator': 'Acrobat PDFMaker 24 for Word', '_id': '0e89f51b-6256-4895-85ac-dfce1f1bd712', '_collection_name': 'ai_policy'}, page_content='22 \\nGV-6.2-003 \\nEstablish incident response plans for third-party GAI technologies: Align incident \\nresponse plans with impacts enumerated in MAP 5.1; Communicate third-party \\nGAI incident response plans to all relevant AI Actors; Deﬁne ownership of GAI \\nincident response functions; Rehearse third-party GAI incident response plans at \\na regular cadence; Improve incident response plans based on retrospective \\nlearning; Review incident response plans for alignment with relevant breach \\nreporting, data protection, data privacy, or other laws. \\nData Privacy; Human-AI \\nConﬁguration; Information \\nSecurity; Value Chain and \\nComponent Integration; Harmful \\nBias and Homogenization \\nGV-6.2-004 \\nEstablish policies and procedures for continuous monitoring of third-party GAI \\nsystems in deployment. \\nValue Chain and Component \\nIntegration \\nGV-6.2-005 \\nEstablish policies and procedures that address GAI data redundancy, including \\nmodel weights and other system artifacts. \\nHarmful Bias and Homogenization \\nGV-6.2-006 \\nEstablish policies and procedures to test and manage risks related to rollover and \\nfallback technologies for GAI systems, acknowledging that rollover and fallback \\nmay include manual processing. \\nInformation Integrity \\nGV-6.2-007 \\nReview vendor contracts and avoid arbitrary or capricious termination of critical \\nGAI technologies or vendor services and non-standard terms that may amplify or \\ndefer liability in unexpected ways and/or contribute to unauthorized data \\ncollection by vendors or third-parties (e.g., secondary data use). Consider: Clear \\nassignment of liability and responsibility for incidents, GAI system changes over \\ntime (e.g., ﬁne-tuning, drift, decay); Request: Notiﬁcation and disclosure for \\nserious incidents arising from third-party data and systems; Service Level \\nAgreements (SLAs) in vendor contracts that address incident response, response \\ntimes, and availability of critical support. \\nHuman-AI Conﬁguration; \\nInformation Security; Value Chain \\nand Component Integration \\nAI Actor Tasks: AI Deployment, Operation and Monitoring, TEVV, Third-party entities \\n \\nMAP 1.1: Intended purposes, potentially beneﬁcial uses, context speciﬁc laws, norms and expectations, and prospective settings in \\nwhich the AI system will be deployed are understood and documented. Considerations include: the speciﬁc set or types of users \\nalong with their expectations; potential positive and negative impacts of system uses to individuals, communities, organizations, \\nsociety, and the planet; assumptions and related limitations about AI system purposes, uses, and risks across the development or \\nproduct AI lifecycle; and related TEVV and system metrics. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMP-1.1-001 \\nWhen identifying intended purposes, consider factors such as internal vs. \\nexternal use, narrow vs. broad application scope, ﬁne-tuning, and varieties of \\ndata sources (e.g., grounding, retrieval-augmented generation). \\nData Privacy; Intellectual \\nProperty'), Document(metadata={'format': 'PDF 1.6', 'keywords': '', 'trapped': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'author': 'National Institute of Standards and Technology', 'subject': '', 'page': 38, 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'total_pages': 64, 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", '_id': '7b670864-d247-4009-973c-59d92d95fb50', '_collection_name': 'ai_policy'}, page_content='35 \\nMEASURE 2.9: The AI model is explained, validated, and documented, and AI system output is interpreted within its context – as \\nidentiﬁed in the MAP function – to inform responsible use and governance. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-2.9-001 \\nApply and document ML explanation results such as: Analysis of embeddings, \\nCounterfactual prompts, Gradient-based attributions, Model \\ncompression/surrogate models, Occlusion/term reduction. \\nConfabulation \\nMS-2.9-002 \\nDocument GAI model details including: Proposed use and organizational value; \\nAssumptions and limitations, Data collection methodologies; Data provenance; \\nData quality; Model architecture (e.g., convolutional neural network, \\ntransformers, etc.); Optimization objectives; Training algorithms; RLHF \\napproaches; Fine-tuning or retrieval-augmented generation approaches; \\nEvaluation data; Ethical considerations; Legal and regulatory requirements. \\nInformation Integrity; Harmful Bias \\nand Homogenization \\nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, End-Users, Operation and Monitoring, TEVV \\n \\nMEASURE 2.10: Privacy risk of the AI system – as identiﬁed in the MAP function – is examined and documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-2.10-001 \\nConduct AI red-teaming to assess issues such as: Outputting of training data \\nsamples, and subsequent reverse engineering, model extraction, and \\nmembership inference risks; Revealing biometric, conﬁdential, copyrighted, \\nlicensed, patented, personal, proprietary, sensitive, or trade-marked information; \\nTracking or revealing location information of users or members of training \\ndatasets. \\nHuman-AI Conﬁguration; \\nInformation Integrity; Intellectual \\nProperty \\nMS-2.10-002 \\nEngage directly with end-users and other stakeholders to understand their \\nexpectations and concerns regarding content provenance. Use this feedback to \\nguide the design of provenance data-tracking techniques. \\nHuman-AI Conﬁguration; \\nInformation Integrity \\nMS-2.10-003 Verify deduplication of GAI training data samples, particularly regarding synthetic \\ndata. \\nHarmful Bias and Homogenization \\nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, End-Users, Operation and Monitoring, TEVV')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='What are some funding opportunities provided by the National Science Foundation in the field of artificial intelligence and cyber-physical systems?' id='92068167-1a77-4715-8596-fb17c976d028'\n",
      "****Adding new context: [Document(metadata={'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'modDate': \"D:20240805143048-04'00'\", 'total_pages': 64, 'creator': 'Acrobat PDFMaker 24 for Word', 'format': 'PDF 1.6', 'producer': 'Adobe PDF Library 24.2.159', 'keywords': '', 'trapped': '', 'subject': '', 'creationDate': \"D:20240805141702-04'00'\", 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'author': 'National Institute of Standards and Technology', 'page': 2, '_id': '91bbe1b1-8123-4737-be3a-53e3571ba4d9', '_collection_name': 'ai_policy'}, page_content='About AI at NIST: The National Institute of Standards and Technology (NIST) develops measurements, \\ntechnology, tools, and standards to advance reliable, safe, transparent, explainable, privacy-enhanced, \\nand fair artiﬁcial intelligence (AI) so that its full commercial and societal beneﬁts can be realized without \\nharm to people or the planet. NIST, which has conducted both fundamental and applied work on AI for \\nmore than a decade, is also helping to fulﬁll the 2023 Executive Order on Safe, Secure, and Trustworthy \\nAI. NIST established the U.S. AI Safety Institute and the companion AI Safety Institute Consortium to \\ncontinue the eﬀorts set in motion by the E.O. to build the science necessary for safe, secure, and \\ntrustworthy development and use of AI. \\nAcknowledgments: This report was accomplished with the many helpful comments and contributions \\nfrom the community, including the NIST Generative AI Public Working Group, and NIST staﬀ and guest \\nresearchers: Chloe Autio, Jesse Dunietz, Patrick Hall, Shomik Jain, Kamie Roberts, Reva Schwartz, Martin \\nStanley, and Elham Tabassi. \\nNIST Technical Series Policies \\nCopyright, Use, and Licensing Statements \\nNIST Technical Series Publication Identifier Syntax \\nPublication History \\nApproved by the NIST Editorial Review Board on 07-25-2024 \\nContact Information \\nai-inquiries@nist.gov \\nNational Institute of Standards and Technology \\nAttn: NIST AI Innovation Lab, Information Technology Laboratory \\n100 Bureau Drive (Mail Stop 8900) Gaithersburg, MD 20899-8900 \\nAdditional Information \\nAdditional information about this publication and other NIST AI publications are available at \\nhttps://airc.nist.gov/Home. \\n \\nDisclaimer: Certain commercial entities, equipment, or materials may be identiﬁed in this document in \\norder to adequately describe an experimental procedure or concept. Such identiﬁcation is not intended to \\nimply recommendation or endorsement by the National Institute of Standards and Technology, nor is it \\nintended to imply that the entities, materials, or equipment are necessarily the best available for the \\npurpose. Any mention of commercial, non-proﬁt, academic partners, or their products, or references is \\nfor information only; it is not intended to imply endorsement or recommendation by any U.S. \\nGovernment agency.'), Document(metadata={'creationDate': \"D:20220920133035-04'00'\", 'total_pages': 73, 'modDate': \"D:20221003104118-04'00'\", 'keywords': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'format': 'PDF 1.6', 'producer': 'iLovePDF', 'trapped': '', 'subject': '', 'page': 21, '_id': 'd6f25e7d-9157-4928-9ac5-2a6e307e5a83', '_collection_name': 'ai_policy'}, page_content='SAFE AND EFFECTIVE \\nSYSTEMS \\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\\nReal-life examples of how these principles can become reality, through laws, policies, and practical \\ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \\xad\\nSome U.S government agencies have developed specific frameworks for ethical use of AI \\nsystems. The Department of Energy (DOE) has activated the AI Advancement Council that oversees coordina-\\ntion and advises on implementation of the DOE AI Strategy and addresses issues and/or escalations on the \\nethical use and development of AI systems.20 The Department of Defense has adopted Artificial Intelligence \\nEthical Principles, and tenets for Responsible Artificial Intelligence specifically tailored to its national \\nsecurity and defense activities.21 Similarly, the U.S. Intelligence Community (IC) has developed the Principles \\nof Artificial Intelligence Ethics for the Intelligence Community to guide personnel on whether and how to \\ndevelop and use AI in furtherance of the IC\\'s mission, as well as an AI Ethics Framework to help implement \\nthese principles.22\\nThe National Science Foundation (NSF) funds extensive research to help foster the \\ndevelopment of automated systems that adhere to and advance their safety, security and \\neffectiveness. Multiple NSF programs support research that directly addresses many of these principles: \\nthe National AI Research Institutes23 support research on all aspects of safe, trustworthy, fair, and explainable \\nAI algorithms and systems; the Cyber Physical Systems24 program supports research on developing safe \\nautonomous and cyber physical systems with AI components; the Secure and Trustworthy Cyberspace25 \\nprogram supports research on cybersecurity and privacy enhancing technologies in automated systems; the \\nFormal Methods in the Field26 program supports research on rigorous formal verification and analysis of \\nautomated systems and machine learning, and the Designing Accountable Software Systems27 program supports \\nresearch on rigorous and reproducible methodologies for developing software systems with legal and regulatory \\ncompliance in mind. \\nSome state legislatures have placed strong transparency and validity requirements on \\nthe use of pretrial risk assessments. The use of algorithmic pretrial risk assessments has been a \\ncause of concern for civil rights groups.28 Idaho Code Section 19-1910, enacted in 2019,29 requires that any \\npretrial risk assessment, before use in the state, first be \"shown to be free of bias against any class of \\nindividuals protected from discrimination by state or federal law\", that any locality using a pretrial risk \\nassessment must first formally validate the claim of its being free of bias, that \"all documents, records, and \\ninformation used to build or validate the risk assessment shall be open to public inspection,\" and that assertions \\nof trade secrets cannot be used \"to quash discovery in a criminal matter by a party to a criminal case.\" \\n22'), Document(metadata={'title': 'Blueprint for an AI Bill of Rights', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'total_pages': 73, 'subject': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'trapped': '', 'format': 'PDF 1.6', 'producer': 'iLovePDF', 'page': 63, 'creationDate': \"D:20220920133035-04'00'\", 'keywords': '', 'author': '', 'modDate': \"D:20221003104118-04'00'\", 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', '_id': '18095a53-8bcd-4633-b40d-1796145a3492', '_collection_name': 'ai_policy'}, page_content='ENDNOTES\\n12. Expectations about reporting are intended for the entity developing or using the automated system. The\\nresulting reports can be provided to the public, regulators, auditors, industry standards groups, or others\\nengaged in independent review, and should be made public as much as possible consistent with law,\\nregulation, and policy, and noting that intellectual property or law enforcement considerations may prevent\\npublic release. These reporting expectations are important for transparency, so the American people can\\nhave confidence that their rights, opportunities, and access as well as their expectations around\\ntechnologies are respected.\\n13. National Artificial Intelligence Initiative Office. Agency Inventories of AI Use Cases. Accessed Sept. 8,\\n2022. https://www.ai.gov/ai-use-case-inventories/\\n14. National Highway Traffic Safety Administration. https://www.nhtsa.gov/\\n15. See, e.g., Charles Pruitt. People Doing What They Do Best: The Professional Engineers and NHTSA. Public\\nAdministration Review. Vol. 39, No. 4. Jul.-Aug., 1979. https://www.jstor.org/stable/976213?seq=1\\n16. The US Department of Transportation has publicly described the health and other benefits of these\\n“traffic calming” measures. See, e.g.: U.S. Department of Transportation. Traffic Calming to Slow Vehicle\\nSpeeds. Accessed Apr. 17, 2022. https://www.transportation.gov/mission/health/Traffic-Calming-to-Slow\\xad\\nVehicle-Speeds\\n17. Karen Hao. Worried about your firm’s AI ethics? These startups are here to help.\\nA growing ecosystem of “responsible AI” ventures promise to help organizations monitor and fix their AI\\nmodels. MIT Technology Review. Jan 15., 2021.\\nhttps://www.technologyreview.com/2021/01/15/1016183/ai-ethics-startups/; Disha Sinha. Top Progressive\\nCompanies Building Ethical AI to Look Out for in 2021. Analytics Insight. June 30, 2021. https://\\nwww.analyticsinsight.net/top-progressive-companies-building-ethical-ai-to-look-out-for\\xad\\nin-2021/ https://www.technologyreview.com/2021/01/15/1016183/ai-ethics-startups/; Disha Sinha. Top\\nProgressive Companies Building Ethical AI to Look Out for in 2021. Analytics Insight. June 30, 2021.\\n18. Office of Management and Budget. Study to Identify Methods to Assess Equity: Report to the President.\\nAug. 2021. https://www.whitehouse.gov/wp-content/uploads/2021/08/OMB-Report-on-E013985\\xad\\nImplementation_508-Compliant-Secure-v1.1.pdf\\n19. National Institute of Standards and Technology. AI Risk Management Framework. Accessed May 23,\\n2022. https://www.nist.gov/itl/ai-risk-management-framework\\n20. U.S. Department of Energy. U.S. Department of Energy Establishes Artificial Intelligence Advancement\\nCouncil. U.S. Department of Energy Artificial Intelligence and Technology Office. April 18, 2022. https://\\nwww.energy.gov/ai/articles/us-department-energy-establishes-artificial-intelligence-advancement-council\\n21. Department of Defense. U.S Department of Defense Responsible Artificial Intelligence Strategy and\\nImplementation Pathway. Jun. 2022. https://media.defense.gov/2022/Jun/22/2003022604/-1/-1/0/\\nDepartment-of-Defense-Responsible-Artificial-Intelligence-Strategy-and-Implementation\\xad\\nPathway.PDF\\n22. Director of National Intelligence. Principles of Artificial Intelligence Ethics for the Intelligence\\nCommunity. https://www.dni.gov/index.php/features/2763-principles-of-artificial-intelligence-ethics-for\\xad\\nthe-intelligence-community\\n64'), Document(metadata={'modDate': \"D:20221003104118-04'00'\", 'page': 20, 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'creationDate': \"D:20220920133035-04'00'\", 'format': 'PDF 1.6', 'trapped': '', 'total_pages': 73, 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'keywords': '', 'subject': '', 'author': '', 'producer': 'iLovePDF', 'title': 'Blueprint for an AI Bill of Rights', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', '_id': 'b7f5bf9a-6ee2-4daf-9b7d-88dc45adeb9c', '_collection_name': 'ai_policy'}, page_content='SAFE AND EFFECTIVE \\nSYSTEMS \\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\\nReal-life examples of how these principles can become reality, through laws, policies, and practical \\ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \\xad\\xad\\nExecutive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the \\nFederal Government requires that certain federal agencies adhere to nine principles when \\ndesigning, developing, acquiring, or using AI for purposes other than national security or \\ndefense. These principles—while taking into account the sensitive law enforcement and other contexts in which \\nthe federal government may use AI, as opposed to private sector use of AI—require that AI is: (a) lawful and \\nrespectful of our Nation’s values; (b) purposeful and performance-driven; (c) accurate, reliable, and effective; (d) \\nsafe, secure, and resilient; (e) understandable; (f ) responsible and traceable; (g) regularly monitored; (h) transpar-\\nent; and, (i) accountable. The Blueprint for an AI Bill of Rights is consistent with the Executive Order. \\nAffected agencies across the federal government have released AI use case inventories13 and are implementing \\nplans to bring those AI systems into compliance with the Executive Order or retire them. \\nThe law and policy landscape for motor vehicles shows that strong safety regulations—and \\nmeasures to address harms when they occur—can enhance innovation in the context of com-\\nplex technologies. Cars, like automated digital systems, comprise a complex collection of components. \\nThe National Highway Traffic Safety Administration,14 through its rigorous standards and independent \\nevaluation, helps make sure vehicles on our roads are safe without limiting manufacturers’ ability to \\ninnovate.15 At the same time, rules of the road are implemented locally to impose contextually appropriate \\nrequirements on drivers, such as slowing down near schools or playgrounds.16\\nFrom large companies to start-ups, industry is providing innovative solutions that allow \\norganizations to mitigate risks to the safety and efficacy of AI systems, both before \\ndeployment and through monitoring over time.17 These innovative solutions include risk \\nassessments, auditing mechanisms, assessment of organizational procedures, dashboards to allow for ongoing \\nmonitoring, documentation procedures specific to model assessments, and many other strategies that aim to \\nmitigate risks posed by the use of AI to companies’ reputation, legal responsibilities, and other product safety \\nand effectiveness concerns. \\nThe Office of Management and Budget (OMB) has called for an expansion of opportunities \\nfor meaningful stakeholder engagement in the design of programs and services. OMB also \\npoints to numerous examples of effective and proactive stakeholder engagement, including the Community-\\nBased Participatory Research Program developed by the National Institutes of Health and the participatory \\ntechnology assessments developed by the National Oceanic and Atmospheric Administration.18\\nThe National Institute of Standards and Technology (NIST) is developing a risk \\nmanagement framework to better manage risks posed to individuals, organizations, and \\nsociety by AI.19 The NIST AI Risk Management Framework, as mandated by Congress, is intended for \\nvoluntary use to help incorporate trustworthiness considerations into the design, development, use, and \\nevaluation of AI products, services, and systems. The NIST framework is being developed through a consensus-\\ndriven, open, transparent, and collaborative process that includes workshops and other opportunities to provide \\ninput. The NIST framework aims to foster the development of innovative approaches to address \\ncharacteristics of trustworthiness including accuracy, explainability and interpretability, reliability, privacy, \\nrobustness, safety, security (resilience), and mitigation of unintended and/or harmful bias, as well as of \\nharmful \\nuses. \\nThe \\nNIST \\nframework \\nwill \\nconsider \\nand \\nencompass \\nprinciples \\nsuch \\nas \\ntransparency, accountability, and fairness during pre-design, design and development, deployment, use, \\nand testing and evaluation of AI technologies and systems. It is expected to be released in the winter of 2022-23. \\n21')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='How should the public be involved in the consultation process for the development of automated systems?' id='178f354f-0c6d-44f6-8a8f-f7c3da7f398e'\n",
      "****Adding new context: [Document(metadata={'format': 'PDF 1.6', 'producer': 'iLovePDF', 'author': '', 'trapped': '', 'keywords': '', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'subject': '', 'creationDate': \"D:20220920133035-04'00'\", 'title': 'Blueprint for an AI Bill of Rights', 'modDate': \"D:20221003104118-04'00'\", 'page': 17, 'total_pages': 73, 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', '_id': 'a49d8973-2625-4111-9d42-ea999a90c1a5', '_collection_name': 'ai_policy'}, page_content='SAFE AND EFFECTIVE \\nSYSTEMS \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nIn order to ensure that an automated system is safe and effective, it should include safeguards to protect the \\npublic from harm in a proactive and ongoing manner; avoid use of data inappropriate for or irrelevant to the task \\nat hand, including reuse that could cause compounded harm; and demonstrate the safety and effectiveness of \\nthe system. These expectations are explained below. \\nProtect the public from harm in a proactive and ongoing manner \\nConsultation. The public should be consulted in the design, implementation, deployment, acquisition, and \\nmaintenance phases of automated system development, with emphasis on early-stage consultation before a \\nsystem is introduced or a large change implemented. This consultation should directly engage diverse impact\\xad\\ned communities to consider concerns and risks that may be unique to those communities, or disproportionate\\xad\\nly prevalent or severe for them. The extent of this engagement and the form of outreach to relevant stakehold\\xad\\ners may differ depending on the specific automated system and development phase, but should include \\nsubject matter, sector-specific, and context-specific experts as well as experts on potential impacts such as \\ncivil rights, civil liberties, and privacy experts. For private sector applications, consultations before product \\nlaunch may need to be confidential. Government applications, particularly law enforcement applications or \\napplications that raise national security considerations, may require confidential or limited engagement based \\non system sensitivities and preexisting oversight laws and structures. Concerns raised in this consultation \\nshould be documented, and the automated system developers were proposing to create, use, or deploy should \\nbe reconsidered based on this feedback. \\nTesting. Systems should undergo extensive testing before deployment. This testing should follow \\ndomain-specific best practices, when available, for ensuring the technology will work in its real-world \\ncontext. Such testing should take into account both the specific technology used and the roles of any human \\noperators or reviewers who impact system outcomes or effectiveness; testing should include both automated \\nsystems testing and human-led (manual) testing. Testing conditions should mirror as closely as possible the \\nconditions in which the system will be deployed, and new testing may be required for each deployment to \\naccount for material differences in conditions from one deployment to another. Following testing, system \\nperformance should be compared with the in-place, potentially human-driven, status quo procedures, with \\nexisting human performance considered as a performance baseline for the algorithm to meet pre-deployment, \\nand as a lifecycle minimum performance standard. Decision possibilities resulting from performance testing \\nshould include the possibility of not deploying the system. \\nRisk identification and mitigation. Before deployment, and in a proactive and ongoing manner, poten\\xad\\ntial risks of the automated system should be identified and mitigated. Identified risks should focus on the \\npotential for meaningful impact on people’s rights, opportunities, or access and include those to impacted \\ncommunities that may not be direct users of the automated system, risks resulting from purposeful misuse of \\nthe system, and other concerns identified via the consultation process. Assessment and, where possible, mea\\xad\\nsurement of the impact of risks should be included and balanced such that high impact risks receive attention \\nand mitigation proportionate with those impacts. Automated systems with the intended purpose of violating \\nthe safety of others should not be developed or used; systems with such safety violations as identified unin\\xad\\ntended consequences should not be used until the risk can be mitigated. Ongoing risk mitigation may necessi\\xad\\ntate rollback or significant modification to a launched automated system. \\n18'), Document(metadata={'keywords': '', 'modDate': \"D:20221003104118-04'00'\", 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'title': 'Blueprint for an AI Bill of Rights', 'format': 'PDF 1.6', 'trapped': '', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'page': 39, 'total_pages': 73, 'author': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'subject': '', '_id': 'f1c12e75-8edc-40e3-bb7c-002ec096a918', '_collection_name': 'ai_policy'}, page_content='You should know that an automated system is being used, \\nand understand how and why it contributes to outcomes \\nthat impact you. Designers, developers, and deployers of automat\\xad\\ned systems should provide generally accessible plain language docu\\xad\\nmentation including clear descriptions of the overall system func\\xad\\ntioning and the role automation plays, notice that such systems are in \\nuse, the individual or organization responsible for the system, and ex\\xad\\nplanations of outcomes that are clear, timely, and accessible. Such \\nnotice should be kept up-to-date and people impacted by the system \\nshould be notified of significant use case or key functionality chang\\xad\\nes. You should know how and why an outcome impacting you was de\\xad\\ntermined by an automated system, including when the automated \\nsystem is not the sole input determining the outcome. Automated \\nsystems should provide explanations that are technically valid, \\nmeaningful and useful to you and to any operators or others who \\nneed to understand the system, and calibrated to the level of risk \\nbased on the context. Reporting that includes summary information \\nabout these automated systems in plain language and assessments of \\nthe clarity and quality of the notice and explanations should be made \\npublic whenever possible.   \\nNOTICE AND EXPLANATION\\n40'), Document(metadata={'total_pages': 73, 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'subject': '', 'title': 'Blueprint for an AI Bill of Rights', 'creationDate': \"D:20220920133035-04'00'\", 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'author': '', 'page': 45, 'format': 'PDF 1.6', 'modDate': \"D:20221003104118-04'00'\", 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'producer': 'iLovePDF', 'trapped': '', 'keywords': '', '_id': '213f9d52-bb3b-49ee-be8b-4822f54dc279', '_collection_name': 'ai_policy'}, page_content='You should be able to opt out, where appropriate, and \\nhave access to a person who can quickly consider and \\nremedy problems you encounter. You should be able to opt \\nout from automated systems in favor of a human alternative, where \\nappropriate. Appropriateness should be determined based on rea\\xad\\nsonable expectations in a given context and with a focus on ensuring \\nbroad accessibility and protecting the public from especially harm\\xad\\nful impacts. In some cases, a human or other alternative may be re\\xad\\nquired by law. You should have access to timely human consider\\xad\\nation and remedy by a fallback and escalation process if an automat\\xad\\ned system fails, it produces an error, or you would like to appeal or \\ncontest its impacts on you. Human consideration and fallback \\nshould be accessible, equitable, effective, maintained, accompanied \\nby appropriate operator training, and should not impose an unrea\\xad\\nsonable burden on the public. Automated systems with an intended \\nuse within sensitive domains, including, but not limited to, criminal \\njustice, employment, education, and health, should additionally be \\ntailored to the purpose, provide meaningful access for oversight, \\ninclude training for any people interacting with the system, and in\\xad\\ncorporate human consideration for adverse or high-risk decisions. \\nReporting that includes a description of these human governance \\nprocesses and assessment of their timeliness, accessibility, out\\xad\\ncomes, and effectiveness should be made public whenever possible. \\nHUMAN ALTERNATIVES, CONSIDERATION\\nALLBACK\\nF\\nAND\\n, \\n46'), Document(metadata={'author': '', 'producer': 'iLovePDF', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'trapped': '', 'creationDate': \"D:20220920133035-04'00'\", 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'modDate': \"D:20221003104118-04'00'\", 'total_pages': 73, 'page': 19, 'title': 'Blueprint for an AI Bill of Rights', 'keywords': '', 'subject': '', 'format': 'PDF 1.6', '_id': '9c462b21-619b-4a6a-a6dc-c6c0342fc133', '_collection_name': 'ai_policy'}, page_content='SAFE AND EFFECTIVE \\nSYSTEMS \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nDerived data sources tracked and reviewed carefully. Data that is derived from other data through \\nthe use of algorithms, such as data derived or inferred from prior model outputs, should be identified and \\ntracked, e.g., via a specialized type in a data schema. Derived data should be viewed as potentially high-risk \\ninputs that may lead to feedback loops, compounded harm, or inaccurate results. Such sources should be care\\xad\\nfully validated against the risk of collateral consequences. \\nData reuse limits in sensitive domains. Data reuse, and especially data reuse in a new context, can result \\nin the spreading and scaling of harms. Data from some domains, including criminal justice data and data indi\\xad\\ncating adverse outcomes in domains such as finance, employment, and housing, is especially sensitive, and in \\nsome cases its reuse is limited by law. Accordingly, such data should be subject to extra oversight to ensure \\nsafety and efficacy. Data reuse of sensitive domain data in other contexts (e.g., criminal data reuse for civil legal \\nmatters or private sector use) should only occur where use of such data is legally authorized and, after examina\\xad\\ntion, has benefits for those impacted by the system that outweigh identified risks and, as appropriate, reason\\xad\\nable measures have been implemented to mitigate the identified risks. Such data should be clearly labeled to \\nidentify contexts for limited reuse based on sensitivity. Where possible, aggregated datasets may be useful for \\nreplacing individual-level sensitive data. \\nDemonstrate the safety and effectiveness of the system \\nIndependent evaluation. Automated systems should be designed to allow for independent evaluation (e.g., \\nvia application programming interfaces). Independent evaluators, such as researchers, journalists, ethics \\nreview boards, inspectors general, and third-party auditors, should be given access to the system and samples \\nof associated data, in a manner consistent with privacy, security, law, or regulation (including, e.g., intellectual \\nproperty law), in order to perform such evaluations. Mechanisms should be included to ensure that system \\naccess for evaluation is: provided in a timely manner to the deployment-ready version of the system; trusted to \\nprovide genuine, unfiltered access to the full system; and truly independent such that evaluator access cannot \\nbe revoked without reasonable and verified justification. \\nReporting.12 Entities responsible for the development or use of automated systems should provide \\nregularly-updated reports that include: an overview of the system, including how it is embedded in the \\norganization’s business processes or other activities, system goals, any human-run procedures that form a \\npart of the system, and specific performance expectations; a description of any data used to train machine \\nlearning models or for other purposes, including how data sources were processed and interpreted, a \\nsummary of what data might be missing, incomplete, or erroneous, and data relevancy justifications; the \\nresults of public consultation such as concerns raised and any decisions made due to these concerns; risk \\nidentification and management assessments and any steps taken to mitigate potential harms; the results of \\nperformance testing including, but not limited to, accuracy, differential demographic impact, resulting \\nerror rates (overall and per demographic group), and comparisons to previously deployed systems; \\nongoing monitoring procedures and regular performance testing reports, including monitoring frequency, \\nresults, and actions taken; and the procedures for and results from independent evaluations. Reporting \\nshould be provided in a plain language and machine-readable manner. \\n20')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='How can structured feedback mechanisms be used to monitor and improve outputs of the GAI system?' id='1304d158-7d05-498d-b4e5-ff013ec5c851'\n",
      "****Adding new context: [Document(metadata={'subject': '', 'author': 'National Institute of Standards and Technology', 'producer': 'Adobe PDF Library 24.2.159', 'page': 52, 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'format': 'PDF 1.6', 'modDate': \"D:20240805143048-04'00'\", 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'trapped': '', 'keywords': '', 'total_pages': 64, 'creationDate': \"D:20240805141702-04'00'\", 'creator': 'Acrobat PDFMaker 24 for Word', '_id': 'f923229c-1cd8-4b6c-8d03-5152882532e6', '_collection_name': 'ai_policy'}, page_content='49 \\nearly lifecycle TEVV approaches are developed and matured for GAI, organizations may use \\nrecommended “pre-deployment testing” practices to measure performance, capabilities, limits, risks, \\nand impacts. This section describes risk measurement and estimation as part of pre-deployment TEVV, \\nand examines the state of play for pre-deployment testing methodologies.  \\nLimitations of Current Pre-deployment Test Approaches \\nCurrently available pre-deployment TEVV processes used for GAI applications may be inadequate, non-\\nsystematically applied, or fail to reﬂect or mismatched to deployment contexts. For example, the \\nanecdotal testing of GAI system capabilities through video games or standardized tests designed for \\nhumans (e.g., intelligence tests, professional licensing exams) does not guarantee GAI system validity or \\nreliability in those domains. Similarly, jailbreaking or prompt engineering tests may not systematically \\nassess validity or reliability risks.  \\nMeasurement gaps can arise from mismatches between laboratory and real-world settings. Current \\ntesting approaches often remain focused on laboratory conditions or restricted to benchmark test \\ndatasets and in silico techniques that may not extrapolate well to—or directly assess GAI impacts in real-\\nworld conditions. For example, current measurement gaps for GAI make it diﬃcult to precisely estimate \\nits potential ecosystem-level or longitudinal risks and related political, social, and economic impacts. \\nGaps between benchmarks and real-world use of GAI systems may likely be exacerbated due to prompt \\nsensitivity and broad heterogeneity of contexts of use. \\nA.1.5. Structured Public Feedback \\nStructured public feedback can be used to evaluate whether GAI systems are performing as intended \\nand to calibrate and verify traditional measurement methods. Examples of structured feedback include, \\nbut are not limited to: \\n• \\nParticipatory Engagement Methods: Methods used to solicit feedback from civil society groups, \\naﬀected communities, and users, including focus groups, small user studies, and surveys. \\n• \\nField Testing: Methods used to determine how people interact with, consume, use, and make \\nsense of AI-generated information, and subsequent actions and eﬀects, including UX, usability, \\nand other structured, randomized experiments.  \\n• \\nAI Red-teaming: A structured testing exercise used to probe an AI system to ﬁnd ﬂaws and \\nvulnerabilities such as inaccurate, harmful, or discriminatory outputs, often in a controlled \\nenvironment and in collaboration with system developers. \\nInformation gathered from structured public feedback can inform design, implementation, deployment \\napproval, maintenance, or decommissioning decisions. Results and insights gleaned from these exercises \\ncan serve multiple purposes, including improving data quality and preprocessing, bolstering governance \\ndecision making, and enhancing system documentation and debugging practices. When implementing \\nfeedback activities, organizations should follow human subjects research requirements and best \\npractices such as informed consent and subject compensation.'), Document(metadata={'producer': 'Adobe PDF Library 24.2.159', 'author': 'National Institute of Standards and Technology', 'page': 42, 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'format': 'PDF 1.6', 'trapped': '', 'total_pages': 64, 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'creator': 'Acrobat PDFMaker 24 for Word', 'subject': '', 'keywords': '', '_id': 'e4eca81e-f4db-4f61-b60c-a4ef1cb91810', '_collection_name': 'ai_policy'}, page_content=\"39 \\nMS-3.3-004 \\nProvide input for training materials about the capabilities and limitations of GAI \\nsystems related to digital content transparency for AI Actors, other \\nprofessionals, and the public about the societal impacts of AI and the role of \\ndiverse and inclusive content generation. \\nHuman-AI Conﬁguration; \\nInformation Integrity; Harmful Bias \\nand Homogenization \\nMS-3.3-005 \\nRecord and integrate structured feedback about content provenance from \\noperators, users, and potentially impacted communities through the use of \\nmethods such as user research studies, focus groups, or community forums. \\nActively seek feedback on generated content quality and potential biases. \\nAssess the general awareness among end users and impacted communities \\nabout the availability of these feedback channels. \\nHuman-AI Conﬁguration; \\nInformation Integrity; Harmful Bias \\nand Homogenization \\nAI Actor Tasks: AI Deployment, Aﬀected Individuals and Communities, End-Users, Operation and Monitoring, TEVV \\n \\nMEASURE 4.2: Measurement results regarding AI system trustworthiness in deployment context(s) and across the AI lifecycle are \\ninformed by input from domain experts and relevant AI Actors to validate whether the system is performing consistently as \\nintended. Results are documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-4.2-001 \\nConduct adversarial testing at a regular cadence to map and measure GAI risks, \\nincluding tests to address attempts to deceive or manipulate the application of \\nprovenance techniques or other misuses. Identify vulnerabilities and \\nunderstand potential misuse scenarios and unintended outputs. \\nInformation Integrity; Information \\nSecurity \\nMS-4.2-002 \\nEvaluate GAI system performance in real-world scenarios to observe its \\nbehavior in practical environments and reveal issues that might not surface in \\ncontrolled and optimized testing environments. \\nHuman-AI Conﬁguration; \\nConfabulation; Information \\nSecurity \\nMS-4.2-003 \\nImplement interpretability and explainability methods to evaluate GAI system \\ndecisions and verify alignment with intended purpose. \\nInformation Integrity; Harmful Bias \\nand Homogenization \\nMS-4.2-004 \\nMonitor and document instances where human operators or other systems \\noverride the GAI's decisions. Evaluate these cases to understand if the overrides \\nare linked to issues related to content provenance. \\nInformation Integrity \\nMS-4.2-005 \\nVerify and document the incorporation of results of structured public feedback \\nexercises into design, implementation, deployment approval (“go”/“no-go” \\ndecisions), monitoring, and decommission decisions. \\nHuman-AI Conﬁguration; \\nInformation Security \\nAI Actor Tasks: AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV\"), Document(metadata={'page': 32, 'modDate': \"D:20240805143048-04'00'\", 'creator': 'Acrobat PDFMaker 24 for Word', 'keywords': '', 'format': 'PDF 1.6', 'total_pages': 64, 'author': 'National Institute of Standards and Technology', 'producer': 'Adobe PDF Library 24.2.159', 'subject': '', 'creationDate': \"D:20240805141702-04'00'\", 'trapped': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', '_id': '8db2703c-4f91-4524-8702-80a7309bd6bb', '_collection_name': 'ai_policy'}, page_content='29 \\nMS-1.1-006 \\nImplement continuous monitoring of GAI system impacts to identify whether GAI \\noutputs are equitable across various sub-populations. Seek active and direct \\nfeedback from aﬀected communities via structured feedback mechanisms or red-\\nteaming to monitor and improve outputs.  \\nHarmful Bias and Homogenization \\nMS-1.1-007 \\nEvaluate the quality and integrity of data used in training and the provenance of \\nAI-generated content, for example by employing techniques like chaos \\nengineering and seeking stakeholder feedback. \\nInformation Integrity \\nMS-1.1-008 \\nDeﬁne use cases, contexts of use, capabilities, and negative impacts where \\nstructured human feedback exercises, e.g., GAI red-teaming, would be most \\nbeneﬁcial for GAI risk measurement and management based on the context of \\nuse. \\nHarmful Bias and \\nHomogenization; CBRN \\nInformation or Capabilities \\nMS-1.1-009 \\nTrack and document risks or opportunities related to all GAI risks that cannot be \\nmeasured quantitatively, including explanations as to why some risks cannot be \\nmeasured (e.g., due to technological limitations, resource constraints, or \\ntrustworthy considerations). Include unmeasured risks in marginal risks. \\nInformation Integrity \\nAI Actor Tasks: AI Development, Domain Experts, TEVV \\n \\nMEASURE 1.3: Internal experts who did not serve as front-line developers for the system and/or independent assessors are \\ninvolved in regular assessments and updates. Domain experts, users, AI Actors external to the team that developed or deployed the \\nAI system, and aﬀected communities are consulted in support of assessments as necessary per organizational risk tolerance. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-1.3-001 \\nDeﬁne relevant groups of interest (e.g., demographic groups, subject matter \\nexperts, experience with GAI technology) within the context of use as part of \\nplans for gathering structured public feedback. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization; CBRN \\nInformation or Capabilities \\nMS-1.3-002 \\nEngage in internal and external evaluations, GAI red-teaming, impact \\nassessments, or other structured human feedback exercises in consultation \\nwith representative AI Actors with expertise and familiarity in the context of \\nuse, and/or who are representative of the populations associated with the \\ncontext of use. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization; CBRN \\nInformation or Capabilities \\nMS-1.3-003 \\nVerify those conducting structured human feedback exercises are not directly \\ninvolved in system development tasks for the same GAI model. \\nHuman-AI Conﬁguration; Data \\nPrivacy \\nAI Actor Tasks: AI Deployment, AI Development, AI Impact Assessment, Aﬀected Individuals and Communities, Domain Experts, \\nEnd-Users, Operation and Monitoring, TEVV'), Document(metadata={'trapped': '', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'keywords': '', 'author': 'National Institute of Standards and Technology', 'producer': 'Adobe PDF Library 24.2.159', 'format': 'PDF 1.6', 'total_pages': 64, 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'creationDate': \"D:20240805141702-04'00'\", 'page': 23, 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'creator': 'Acrobat PDFMaker 24 for Word', 'subject': '', 'modDate': \"D:20240805143048-04'00'\", '_id': '8ef72c65-f681-4a9a-812e-fe998d271e60', '_collection_name': 'ai_policy'}, page_content='20 \\nGV-4.3-003 \\nVerify information sharing and feedback mechanisms among individuals and \\norganizations regarding any negative impact from GAI systems. \\nInformation Integrity; Data \\nPrivacy \\nAI Actor Tasks: AI Impact Assessment, Aﬀected Individuals and Communities, Governance and Oversight \\n \\nGOVERN 5.1: Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those \\nexternal to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI \\nrisks. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-5.1-001 \\nAllocate time and resources for outreach, feedback, and recourse processes in GAI \\nsystem development. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization \\nGV-5.1-002 \\nDocument interactions with GAI systems to users prior to interactive activities, \\nparticularly in contexts involving more signiﬁcant risks.  \\nHuman-AI Conﬁguration; \\nConfabulation \\nAI Actor Tasks: AI Design, AI Impact Assessment, Aﬀected Individuals and Communities, Governance and Oversight \\n \\nGOVERN 6.1: Policies and procedures are in place that address AI risks associated with third-party entities, including risks of \\ninfringement of a third-party’s intellectual property or other rights. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-6.1-001 Categorize diﬀerent types of GAI content with associated third-party rights (e.g., \\ncopyright, intellectual property, data privacy). \\nData Privacy; Intellectual \\nProperty; Value Chain and \\nComponent Integration \\nGV-6.1-002 Conduct joint educational activities and events in collaboration with third parties \\nto promote best practices for managing GAI risks.  \\nValue Chain and Component \\nIntegration \\nGV-6.1-003 \\nDevelop and validate approaches for measuring the success of content \\nprovenance management eﬀorts with third parties (e.g., incidents detected and \\nresponse times). \\nInformation Integrity; Value Chain \\nand Component Integration \\nGV-6.1-004 \\nDraft and maintain well-deﬁned contracts and service level agreements (SLAs) \\nthat specify content ownership, usage rights, quality standards, security \\nrequirements, and content provenance expectations for GAI systems. \\nInformation Integrity; Information \\nSecurity; Intellectual Property')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='How do language models contribute to the reduction of content diversity in writing?' id='81817c80-8b86-4c55-a716-2347e14a94ff'\n",
      "****Adding new context: [Document(metadata={'author': 'National Institute of Standards and Technology', 'page': 62, 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'format': 'PDF 1.6', 'total_pages': 64, 'creator': 'Acrobat PDFMaker 24 for Word', 'subject': '', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'trapped': '', 'modDate': \"D:20240805143048-04'00'\", 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'keywords': '', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', '_id': 'ea5dc15b-d02c-4d82-9d15-445aacbcc6a2', '_collection_name': 'ai_policy'}, page_content='59 \\nTirrell, L. (2017) Toxic Speech: Toward an Epidemiology of Discursive Harm. Philosophical Topics, 45(2), \\n139-162. https://www.jstor.org/stable/26529441  \\nTufekci, Z. (2015) Algorithmic Harms Beyond Facebook and Google: Emergent Challenges of \\nComputational Agency. Colorado Technology Law Journal. https://ctlj.colorado.edu/wp-\\ncontent/uploads/2015/08/Tufekci-ﬁnal.pdf \\nTurri, V. et al. (2023) Why We Need to Know More: Exploring the State of AI Incident Documentation \\nPractices. AAAI/ACM Conference on AI, Ethics, and Society. \\nhttps://dl.acm.org/doi/fullHtml/10.1145/3600211.3604700 \\nUrbina, F. et al. (2022) Dual use of artiﬁcial-intelligence-powered drug discovery. Nature Machine \\nIntelligence. https://www.nature.com/articles/s42256-022-00465-9 \\nWang, X. et al. (2023) Energy and Carbon Considerations of Fine-Tuning BERT. ACL Anthology. \\nhttps://aclanthology.org/2023.ﬁndings-emnlp.607.pdf \\nWang, Y. et al. (2023) Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. arXiv. \\nhttps://arxiv.org/pdf/2308.13387 \\nWardle, C. et al. (2017) Information Disorder: Toward an interdisciplinary framework for research and \\npolicy making. Council of Europe. https://rm.coe.int/information-disorder-toward-an-interdisciplinary-\\nframework-for-researc/168076277c \\nWeatherbed, J. (2024) Trolls have ﬂooded X with graphic Taylor Swift AI fakes. The Verge. \\nhttps://www.theverge.com/2024/1/25/24050334/x-twitter-taylor-swift-ai-fake-images-trending \\nWei, J. et al. (2024) Long Form Factuality in Large Language Models. arXiv. \\nhttps://arxiv.org/pdf/2403.18802 \\nWeidinger, L. et al. (2021) Ethical and social risks of harm from Language Models. arXiv. \\nhttps://arxiv.org/pdf/2112.04359 \\nWeidinger, L. et al. (2023) Sociotechnical Safety Evaluation of Generative AI Systems. arXiv. \\nhttps://arxiv.org/pdf/2310.11986 \\nWeidinger, L. et al. (2022) Taxonomy of Risks posed by Language Models. FAccT ’22. \\nhttps://dl.acm.org/doi/pdf/10.1145/3531146.3533088 \\nWest, D. (2023) AI poses disproportionate risks to women. Brookings. \\nhttps://www.brookings.edu/articles/ai-poses-disproportionate-risks-to-women/ \\nWu, K. et al. (2024) How well do LLMs cite relevant medical references? An evaluation framework and \\nanalyses. arXiv. https://arxiv.org/pdf/2402.02008 \\nYin, L. et al. (2024) OpenAI’s GPT Is A Recruiter’s Dream Tool. Tests Show There’s Racial Bias. Bloomberg. \\nhttps://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/ \\nYu, Z. et al. (March 2024) Don’t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large \\nLanguage Models. arXiv. https://arxiv.org/html/2403.17336v1 \\nZaugg, I. et al. (2022) Digitally-disadvantaged languages. Policy Review. \\nhttps://policyreview.info/pdf/policyreview-2022-2-1654.pdf'), Document(metadata={'page': 11, 'trapped': '', 'keywords': '', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'format': 'PDF 1.6', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'total_pages': 64, 'author': 'National Institute of Standards and Technology', 'producer': 'Adobe PDF Library 24.2.159', 'modDate': \"D:20240805143048-04'00'\", 'creator': 'Acrobat PDFMaker 24 for Word', 'subject': '', 'creationDate': \"D:20240805141702-04'00'\", '_id': '39629e60-af43-4d74-ab0d-935e14d0b62c', '_collection_name': 'ai_policy'}, page_content='8 \\nTrustworthy AI Characteristics: Accountable and Transparent, Privacy Enhanced, Safe, Secure and \\nResilient \\n2.5. Environmental Impacts \\nTraining, maintaining, and operating (running inference on) GAI systems are resource-intensive activities, \\nwith potentially large energy and environmental footprints. Energy and carbon emissions vary based on \\nwhat is being done with the GAI model (i.e., pre-training, ﬁne-tuning, inference), the modality of the \\ncontent, hardware used, and type of task or application. \\nCurrent estimates suggest that training a single transformer LLM can emit as much carbon as 300 round-\\ntrip ﬂights between San Francisco and New York. In a study comparing energy consumption and carbon \\nemissions for LLM inference, generative tasks (e.g., text summarization) were found to be more energy- \\nand carbon-intensive than discriminative or non-generative tasks (e.g., text classiﬁcation).  \\nMethods for creating smaller versions of trained models, such as model distillation or compression, \\ncould reduce environmental impacts at inference time, but training and tuning such models may still \\ncontribute to their environmental impacts. Currently there is no agreed upon method to estimate \\nenvironmental impacts from GAI.  \\nTrustworthy AI Characteristics: Accountable and Transparent, Safe \\n2.6. Harmful Bias and Homogenization \\nBias exists in many forms and can become ingrained in automated systems. AI systems, including GAI \\nsystems, can increase the speed and scale at which harmful biases manifest and are acted upon, \\npotentially perpetuating and amplifying harms to individuals, groups, communities, organizations, and \\nsociety. For example, when prompted to generate images of CEOs, doctors, lawyers, and judges, current \\ntext-to-image models underrepresent women and/or racial minorities, and people with disabilities. \\nImage generator models have also produced biased or stereotyped output for various demographic \\ngroups and have diﬃculty producing non-stereotyped content even when the prompt speciﬁcally \\nrequests image features that are inconsistent with the stereotypes. Harmful bias in GAI models, which \\nmay stem from their training data, can also cause representational harms or perpetuate or exacerbate \\nbias based on race, gender, disability, or other protected classes.  \\nHarmful bias in GAI systems can also lead to harms via disparities between how a model performs for \\ndiﬀerent subgroups or languages (e.g., an LLM may perform less well for non-English languages or \\ncertain dialects). Such disparities can contribute to discriminatory decision-making or ampliﬁcation of \\nexisting societal biases. In addition, GAI systems may be inappropriately trusted to perform similarly \\nacross all subgroups, which could leave the groups facing underperformance with worse outcomes than \\nif no GAI system were used. Disparate or reduced performance for lower-resource languages also \\npresents challenges to model adoption, inclusion, and accessibility, and may make preservation of \\nendangered languages more diﬃcult if GAI systems become embedded in everyday processes that would \\notherwise have been opportunities to use these languages.  \\nBias is mutually reinforcing with the problem of undesired homogenization, in which GAI systems \\nproduce skewed distributions of outputs that are overly uniform (for example, repetitive aesthetic styles'), Document(metadata={'page': 57, 'total_pages': 64, 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'author': 'National Institute of Standards and Technology', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'trapped': '', 'subject': '', 'modDate': \"D:20240805143048-04'00'\", 'format': 'PDF 1.6', '_id': 'd264a16d-bc63-454c-8cd7-6d03419dd7ad', '_collection_name': 'ai_policy'}, page_content='54 \\nAppendix B. References \\nAcemoglu, D. (2024) The Simple Macroeconomics of AI https://www.nber.org/papers/w32487 \\nAI Incident Database. https://incidentdatabase.ai/ \\nAtherton, D. (2024) Deepfakes and Child Safety: A Survey and Analysis of 2023 Incidents and Responses. \\nAI Incident Database. https://incidentdatabase.ai/blog/deepfakes-and-child-safety/ \\nBadyal, N. et al. (2023) Intentional Biases in LLM Responses. arXiv. https://arxiv.org/pdf/2311.07611 \\nBing Chat: Data Exﬁltration Exploit Explained. Embrace The Red. \\nhttps://embracethered.com/blog/posts/2023/bing-chat-data-exﬁltration-poc-and-ﬁx/ \\nBommasani, R. et al. (2022) Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome \\nHomogenization? arXiv. https://arxiv.org/pdf/2211.13972 \\nBoyarskaya, M. et al. (2020) Overcoming Failures of Imagination in AI Infused System Development and \\nDeployment. arXiv. https://arxiv.org/pdf/2011.13416 \\nBrowne, D. et al. (2023) Securing the AI Pipeline. Mandiant. \\nhttps://www.mandiant.com/resources/blog/securing-ai-pipeline \\nBurgess, M. (2024) Generative AI’s Biggest Security Flaw Is Not Easy to Fix. WIRED. \\nhttps://www.wired.com/story/generative-ai-prompt-injection-hacking/ \\nBurtell, M. et al. (2024) The Surprising Power of Next Word Prediction: Large Language Models \\nExplained, Part 1. Georgetown Center for Security and Emerging Technology. \\nhttps://cset.georgetown.edu/article/the-surprising-power-of-next-word-prediction-large-language-\\nmodels-explained-part-1/ \\nCanadian Centre for Cyber Security (2023) Generative artiﬁcial intelligence (AI) - ITSAP.00.041. \\nhttps://www.cyber.gc.ca/en/guidance/generative-artiﬁcial-intelligence-ai-itsap00041 \\nCarlini, N., et al. (2021) Extracting Training Data from Large Language Models. Usenix. \\nhttps://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting \\nCarlini, N. et al. (2023) Quantifying Memorization Across Neural Language Models. ICLR 2023. \\nhttps://arxiv.org/pdf/2202.07646 \\nCarlini, N. et al. (2024) Stealing Part of a Production Language Model. arXiv. \\nhttps://arxiv.org/abs/2403.06634 \\nChandra, B. et al. (2023) Dismantling the Disinformation Business of Chinese Inﬂuence Operations. \\nRAND. https://www.rand.org/pubs/commentary/2023/10/dismantling-the-disinformation-business-of-\\nchinese.html \\nCiriello, R. et al. (2024) Ethical Tensions in Human-AI Companionship: A Dialectical Inquiry into Replika. \\nResearchGate. https://www.researchgate.net/publication/374505266_Ethical_Tensions_in_Human-\\nAI_Companionship_A_Dialectical_Inquiry_into_Replika \\nDahl, M. et al. (2024) Large Legal Fictions: Proﬁling Legal Hallucinations in Large Language Models. arXiv. \\nhttps://arxiv.org/abs/2401.01301'), Document(metadata={'creator': 'Acrobat PDFMaker 24 for Word', 'page': 63, 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'total_pages': 64, 'keywords': '', 'subject': '', 'author': 'National Institute of Standards and Technology', 'producer': 'Adobe PDF Library 24.2.159', 'format': 'PDF 1.6', 'trapped': '', 'creationDate': \"D:20240805141702-04'00'\", 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'modDate': \"D:20240805143048-04'00'\", '_id': 'e6de3125-07d3-4823-91c7-050fe69d6105', '_collection_name': 'ai_policy'}, page_content='60 \\nZhang, Y. et al. (2023) Human favoritism, not AI aversion: People’s perceptions (and bias) toward \\ngenerative AI, human experts, and human–GAI collaboration in persuasive content generation. Judgment \\nand Decision Making. https://www.cambridge.org/core/journals/judgment-and-decision-\\nmaking/article/human-favoritism-not-ai-aversion-peoples-perceptions-and-bias-toward-generative-ai-\\nhuman-experts-and-humangai-collaboration-in-persuasive-content-\\ngeneration/419C4BD9CE82673EAF1D8F6C350C4FA8 \\nZhang, Y. et al. (2023) Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models. \\narXiv. https://arxiv.org/pdf/2309.01219 \\nZhao, X. et al. (2023) Provable Robust Watermarking for AI-Generated Text. Semantic Scholar. \\nhttps://www.semanticscholar.org/paper/Provable-Robust-Watermarking-for-AI-Generated-Text-Zhao-\\nAnanth/75b68d0903af9d9f6e47ce3cf7e1a7d27ec811dc')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='How can organizations enhance content provenance through structured public feedback in relation to GAI content?' id='9e8619b1-66c5-41cc-9f85-330af9ba3519'\n",
      "****Adding new context: [Document(metadata={'author': 'National Institute of Standards and Technology', 'format': 'PDF 1.6', 'trapped': '', 'subject': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'total_pages': 64, 'creationDate': \"D:20240805141702-04'00'\", 'producer': 'Adobe PDF Library 24.2.159', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'keywords': '', 'modDate': \"D:20240805143048-04'00'\", 'page': 54, 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', '_id': '6b2ebef7-3125-4dc9-b5a6-fe1029780d8a', '_collection_name': 'ai_policy'}, page_content='51 \\ngeneral public participants. For example, expert AI red-teamers could modify or verify the \\nprompts written by general public AI red-teamers. These approaches may also expand coverage \\nof the AI risk attack surface.  \\n• \\nHuman / AI: Performed by GAI in combination with specialist or non-specialist human teams. \\nGAI-led red-teaming can be more cost eﬀective than human red-teamers alone. Human or GAI-\\nled AI red-teaming may be better suited for eliciting diﬀerent types of harms. \\n \\nA.1.6. Content Provenance \\nOverview \\nGAI technologies can be leveraged for many applications such as content generation and synthetic data. \\nSome aspects of GAI outputs, such as the production of deepfake content, can challenge our ability to \\ndistinguish human-generated content from AI-generated synthetic content. To help manage and mitigate \\nthese risks, digital transparency mechanisms like provenance data tracking can trace the origin and \\nhistory of content. Provenance data tracking and synthetic content detection can help facilitate greater \\ninformation access about both authentic and synthetic content to users, enabling better knowledge of \\ntrustworthiness in AI systems. When combined with other organizational accountability mechanisms, \\ndigital content transparency approaches can enable processes to trace negative outcomes back to their \\nsource, improve information integrity, and uphold public trust. Provenance data tracking and synthetic \\ncontent detection mechanisms provide information about the origin and history of content to assist in \\nGAI risk management eﬀorts. \\nProvenance metadata can include information about GAI model developers or creators of GAI content, \\ndate/time of creation, location, modiﬁcations, and sources. Metadata can be tracked for text, images, \\nvideos, audio, and underlying datasets. The implementation of provenance data tracking techniques can \\nhelp assess the authenticity, integrity, intellectual property rights, and potential manipulations in digital \\ncontent. Some well-known techniques for provenance data tracking include digital watermarking, \\nmetadata recording, digital ﬁngerprinting, and human authentication, among others. \\nProvenance Data Tracking Approaches \\nProvenance data tracking techniques for GAI systems can be used to track the history and origin of data \\ninputs, metadata, and synthetic content. Provenance data tracking records the origin and history for \\ndigital content, allowing its authenticity to be determined. It consists of techniques to record metadata \\nas well as overt and covert digital watermarks on content. Data provenance refers to tracking the origin \\nand history of input data through metadata and digital watermarking techniques. Provenance data \\ntracking processes can include and assist AI Actors across the lifecycle who may not have full visibility or \\ncontrol over the various trade-oﬀs and cascading impacts of early-stage model decisions on downstream \\nperformance and synthetic outputs. For example, by selecting a watermarking model to prioritize \\nrobustness (the durability of a watermark), an AI actor may inadvertently diminish computational \\ncomplexity (the resources required to implement watermarking). Organizational risk management \\neﬀorts for enhancing content provenance include:  \\n• \\nTracking provenance of training data and metadata for GAI systems; \\n• \\nDocumenting provenance data limitations within GAI systems;'), Document(metadata={'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'producer': 'Adobe PDF Library 24.2.159', 'format': 'PDF 1.6', 'modDate': \"D:20240805143048-04'00'\", 'keywords': '', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'trapped': '', 'total_pages': 64, 'creator': 'Acrobat PDFMaker 24 for Word', 'subject': '', 'page': 55, 'creationDate': \"D:20240805141702-04'00'\", 'author': 'National Institute of Standards and Technology', '_id': 'd37bb147-1a0e-43d1-8c2b-fb74014c5420', '_collection_name': 'ai_policy'}, page_content='52 \\n• \\nMonitoring system capabilities and limitations in deployment through rigorous TEVV processes; \\n• \\nEvaluating how humans engage, interact with, or adapt to GAI content (especially in decision \\nmaking tasks informed by GAI content), and how they react to applied provenance techniques \\nsuch as overt disclosures. \\nOrganizations can document and delineate GAI system objectives and limitations to identify gaps where \\nprovenance data may be most useful. For instance, GAI systems used for content creation may require \\nrobust watermarking techniques and corresponding detectors to identify the source of content or \\nmetadata recording techniques and metadata management tools and repositories to trace content \\norigins and modiﬁcations. Further narrowing of GAI task deﬁnitions to include provenance data can \\nenable organizations to maximize the utility of provenance data and risk management eﬀorts. \\nA.1.7. Enhancing Content Provenance through Structured Public Feedback \\nWhile indirect feedback methods such as automated error collection systems are useful, they often lack \\nthe context and depth that direct input from end users can provide. Organizations can leverage feedback \\napproaches described in the Pre-Deployment Testing section to capture input from external sources such \\nas through AI red-teaming.  \\nIntegrating pre- and post-deployment external feedback into the monitoring process for GAI models and \\ncorresponding applications can help enhance awareness of performance changes and mitigate potential \\nrisks and harms from outputs. There are many ways to capture and make use of user feedback – before \\nand after GAI systems and digital content transparency approaches are deployed – to gain insights about \\nauthentication eﬃcacy and vulnerabilities, impacts of adversarial threats on techniques, and unintended \\nconsequences resulting from the utilization of content provenance approaches on users and \\ncommunities. Furthermore, organizations can track and document the provenance of datasets to identify \\ninstances in which AI-generated data is a potential root cause of performance issues with the GAI \\nsystem. \\nA.1.8. Incident Disclosure \\nOverview \\nAI incidents can be deﬁned as an “event, circumstance, or series of events where the development, use, \\nor malfunction of one or more AI systems directly or indirectly contributes to one of the following harms: \\ninjury or harm to the health of a person or groups of people (including psychological harms and harms to \\nmental health); disruption of the management and operation of critical infrastructure; violations of \\nhuman rights or a breach of obligations under applicable law intended to protect fundamental, labor, \\nand intellectual property rights; or harm to property, communities, or the environment.” AI incidents can \\noccur in the aggregate (i.e., for systemic discrimination) or acutely (i.e., for one individual). \\nState of AI Incident Tracking and Disclosure \\nFormal channels do not currently exist to report and document AI incidents. However, a number of \\npublicly available databases have been created to document their occurrence. These reporting channels \\nmake decisions on an ad hoc basis about what kinds of incidents to track. Some, for example, track by \\namount of media coverage.'), Document(metadata={'trapped': '', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'keywords': '', 'author': 'National Institute of Standards and Technology', 'producer': 'Adobe PDF Library 24.2.159', 'format': 'PDF 1.6', 'total_pages': 64, 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'creationDate': \"D:20240805141702-04'00'\", 'page': 23, 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'creator': 'Acrobat PDFMaker 24 for Word', 'subject': '', 'modDate': \"D:20240805143048-04'00'\", '_id': '8ef72c65-f681-4a9a-812e-fe998d271e60', '_collection_name': 'ai_policy'}, page_content='20 \\nGV-4.3-003 \\nVerify information sharing and feedback mechanisms among individuals and \\norganizations regarding any negative impact from GAI systems. \\nInformation Integrity; Data \\nPrivacy \\nAI Actor Tasks: AI Impact Assessment, Aﬀected Individuals and Communities, Governance and Oversight \\n \\nGOVERN 5.1: Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those \\nexternal to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI \\nrisks. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-5.1-001 \\nAllocate time and resources for outreach, feedback, and recourse processes in GAI \\nsystem development. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization \\nGV-5.1-002 \\nDocument interactions with GAI systems to users prior to interactive activities, \\nparticularly in contexts involving more signiﬁcant risks.  \\nHuman-AI Conﬁguration; \\nConfabulation \\nAI Actor Tasks: AI Design, AI Impact Assessment, Aﬀected Individuals and Communities, Governance and Oversight \\n \\nGOVERN 6.1: Policies and procedures are in place that address AI risks associated with third-party entities, including risks of \\ninfringement of a third-party’s intellectual property or other rights. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-6.1-001 Categorize diﬀerent types of GAI content with associated third-party rights (e.g., \\ncopyright, intellectual property, data privacy). \\nData Privacy; Intellectual \\nProperty; Value Chain and \\nComponent Integration \\nGV-6.1-002 Conduct joint educational activities and events in collaboration with third parties \\nto promote best practices for managing GAI risks.  \\nValue Chain and Component \\nIntegration \\nGV-6.1-003 \\nDevelop and validate approaches for measuring the success of content \\nprovenance management eﬀorts with third parties (e.g., incidents detected and \\nresponse times). \\nInformation Integrity; Value Chain \\nand Component Integration \\nGV-6.1-004 \\nDraft and maintain well-deﬁned contracts and service level agreements (SLAs) \\nthat specify content ownership, usage rights, quality standards, security \\nrequirements, and content provenance expectations for GAI systems. \\nInformation Integrity; Information \\nSecurity; Intellectual Property'), Document(metadata={'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'producer': 'Adobe PDF Library 24.2.159', 'page': 50, 'creationDate': \"D:20240805141702-04'00'\", 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'trapped': '', 'author': 'National Institute of Standards and Technology', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'format': 'PDF 1.6', 'keywords': '', 'total_pages': 64, 'subject': '', 'modDate': \"D:20240805143048-04'00'\", 'creator': 'Acrobat PDFMaker 24 for Word', '_id': '21e8c6b5-98cf-4396-9030-866c59e85f9e', '_collection_name': 'ai_policy'}, page_content='47 \\nAppendix A. Primary GAI Considerations \\nThe following primary considerations were derived as overarching themes from the GAI PWG \\nconsultation process. These considerations (Governance, Pre-Deployment Testing, Content Provenance, \\nand Incident Disclosure) are relevant for voluntary use by any organization designing, developing, and \\nusing GAI and also inform the Actions to Manage GAI risks. Information included about the primary \\nconsiderations is not exhaustive, but highlights the most relevant topics derived from the GAI PWG.  \\nAcknowledgments: These considerations could not have been surfaced without the helpful analysis and \\ncontributions from the community and NIST staﬀ GAI PWG leads: George Awad, Luca Belli, Harold Booth, \\nMat Heyman, Yooyoung Lee, Mark Pryzbocki, Reva Schwartz, Martin Stanley, and Kyra Yee. \\nA.1. Governance \\nA.1.1. Overview \\nLike any other technology system, governance principles and techniques can be used to manage risks \\nrelated to generative AI models, capabilities, and applications. Organizations may choose to apply their \\nexisting risk tiering to GAI systems, or they may opt to revise or update AI system risk levels to address \\nthese unique GAI risks. This section describes how organizational governance regimes may be re-\\nevaluated and adjusted for GAI contexts. It also addresses third-party considerations for governing across \\nthe AI value chain.  \\nA.1.2. Organizational Governance \\nGAI opportunities, risks and long-term performance characteristics are typically less well-understood \\nthan non-generative AI tools and may be perceived and acted upon by humans in ways that vary greatly. \\nAccordingly, GAI may call for diﬀerent levels of oversight from AI Actors or diﬀerent human-AI \\nconﬁgurations in order to manage their risks eﬀectively. Organizations’ use of GAI systems may also \\nwarrant additional human review, tracking and documentation, and greater management oversight.  \\nAI technology can produce varied outputs in multiple modalities and present many classes of user \\ninterfaces. This leads to a broader set of AI Actors interacting with GAI systems for widely diﬀering \\napplications and contexts of use. These can include data labeling and preparation, development of GAI \\nmodels, content moderation, code generation and review, text generation and editing, image and video \\ngeneration, summarization, search, and chat. These activities can take place within organizational \\nsettings or in the public domain. \\nOrganizations can restrict AI applications that cause harm, exceed stated risk tolerances, or that conﬂict \\nwith their tolerances or values. Governance tools and protocols that are applied to other types of AI \\nsystems can be applied to GAI systems. These plans and actions include: \\n• Accessibility and reasonable \\naccommodations \\n• AI actor credentials and qualiﬁcations  \\n• Alignment to organizational values \\n• Auditing and assessment \\n• Change-management controls \\n• Commercial use \\n• Data provenance')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='How do GAI-based systems present primary information security risks related to offensive cyber capabilities?' id='3042880d-b107-4fb9-b9d0-3788deda97b7'\n",
      "****Adding new context: [Document(metadata={'producer': 'Adobe PDF Library 24.2.159', 'author': 'National Institute of Standards and Technology', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'format': 'PDF 1.6', 'creationDate': \"D:20240805141702-04'00'\", 'page': 13, 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'trapped': '', 'keywords': '', 'modDate': \"D:20240805143048-04'00'\", 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'creator': 'Acrobat PDFMaker 24 for Word', 'subject': '', 'total_pages': 64, '_id': '189f47fb-a361-4bda-81d4-42d56f994066', '_collection_name': 'ai_policy'}, page_content='10 \\nGAI systems can ease the unintentional production or dissemination of false, inaccurate, or misleading \\ncontent (misinformation) at scale, particularly if the content stems from confabulations.  \\nGAI systems can also ease the deliberate production or dissemination of false or misleading information \\n(disinformation) at scale, where an actor has the explicit intent to deceive or cause harm to others. Even \\nvery subtle changes to text or images can manipulate human and machine perception. \\nSimilarly, GAI systems could enable a higher degree of sophistication for malicious actors to produce \\ndisinformation that is targeted towards speciﬁc demographics. Current and emerging multimodal models \\nmake it possible to generate both text-based disinformation and highly realistic “deepfakes” – that is, \\nsynthetic audiovisual content and photorealistic images.12 Additional disinformation threats could be \\nenabled by future GAI models trained on new data modalities. \\nDisinformation and misinformation – both of which may be facilitated by GAI – may erode public trust in \\ntrue or valid evidence and information, with downstream eﬀects. For example, a synthetic image of a \\nPentagon blast went viral and brieﬂy caused a drop in the stock market. Generative AI models can also \\nassist malicious actors in creating compelling imagery and propaganda to support disinformation \\ncampaigns, which may not be photorealistic, but could enable these campaigns to gain more reach and \\nengagement on social media platforms. Additionally, generative AI models can assist malicious actors in \\ncreating fraudulent content intended to impersonate others. \\nTrustworthy AI Characteristics: Accountable and Transparent, Safe, Valid and Reliable, Interpretable and \\nExplainable \\n2.9. Information Security \\nInformation security for computer systems and data is a mature ﬁeld with widely accepted and \\nstandardized practices for oﬀensive and defensive cyber capabilities. GAI-based systems present two \\nprimary information security risks: GAI could potentially discover or enable new cybersecurity risks by \\nlowering the barriers for or easing automated exercise of oﬀensive capabilities; simultaneously, it \\nexpands the available attack surface, as GAI itself is vulnerable to attacks like prompt injection or data \\npoisoning.  \\nOﬀensive cyber capabilities advanced by GAI systems may augment cybersecurity attacks such as \\nhacking, malware, and phishing. Reports have indicated that LLMs are already able to discover some \\nvulnerabilities in systems (hardware, software, data) and write code to exploit them. Sophisticated threat \\nactors might further these risks by developing GAI-powered security co-pilots for use in several parts of \\nthe attack chain, including informing attackers on how to proactively evade threat detection and escalate \\nprivileges after gaining system access. \\nInformation security for GAI models and systems also includes maintaining availability of the GAI system \\nand the integrity and (when applicable) the conﬁdentiality of the GAI code, training data, and model \\nweights. To identify and secure potential attack points in AI systems or speciﬁc components of the AI \\n \\n \\n12 See also https://doi.org/10.6028/NIST.AI.100-4, to be published.'), Document(metadata={'page': 10, 'modDate': \"D:20240805143048-04'00'\", 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'author': 'National Institute of Standards and Technology', 'creator': 'Acrobat PDFMaker 24 for Word', 'format': 'PDF 1.6', 'trapped': '', 'keywords': '', 'total_pages': 64, 'producer': 'Adobe PDF Library 24.2.159', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'creationDate': \"D:20240805141702-04'00'\", 'subject': '', '_id': '65cad62e-986d-4f70-8bff-bae2da9da694', '_collection_name': 'ai_policy'}, page_content='7 \\nunethical behavior. Text-to-image models also make it easy to create images that could be used to \\npromote dangerous or violent messages. Similar concerns are present for other GAI media, including \\nvideo and audio. GAI may also produce content that recommends self-harm or criminal/illegal activities.  \\nMany current systems restrict model outputs to limit certain content or in response to certain prompts, \\nbut this approach may still produce harmful recommendations in response to other less-explicit, novel \\nprompts (also relevant to CBRN Information or Capabilities, Data Privacy, Information Security, and \\nObscene, Degrading and/or Abusive Content). Crafting such prompts deliberately is known as \\n“jailbreaking,” or, manipulating prompts to circumvent output controls. Limitations of GAI systems can be \\nharmful or dangerous in certain contexts. Studies have observed that users may disclose mental health \\nissues in conversations with chatbots – and that users exhibit negative reactions to unhelpful responses \\nfrom these chatbots during situations of distress. \\nThis risk encompasses diﬃculty controlling creation of and public exposure to oﬀensive or hateful \\nlanguage, and denigrating or stereotypical content generated by AI. This kind of speech may contribute \\nto downstream harm such as fueling dangerous or violent behaviors. The spread of denigrating or \\nstereotypical content can also further exacerbate representational harms (see Harmful Bias and \\nHomogenization below).  \\nTrustworthy AI Characteristics: Safe, Secure and Resilient \\n2.4. Data Privacy \\nGAI systems raise several risks to privacy. GAI system training requires large volumes of data, which in \\nsome cases may include personal data. The use of personal data for GAI training raises risks to widely \\naccepted privacy principles, including to transparency, individual participation (including consent), and \\npurpose speciﬁcation. For example, most model developers do not disclose speciﬁc data sources on \\nwhich models were trained, limiting user awareness of whether personally identiﬁably information (PII) \\nwas trained on and, if so, how it was collected.  \\nModels may leak, generate, or correctly infer sensitive information about individuals. For example, \\nduring adversarial attacks, LLMs have revealed sensitive information (from the public domain) that was \\nincluded in their training data. This problem has been referred to as data memorization, and may pose \\nexacerbated privacy risks even for data present only in a small number of training samples.  \\nIn addition to revealing sensitive information in GAI training data, GAI models may be able to correctly \\ninfer PII or sensitive data that was not in their training data nor disclosed by the user by stitching \\ntogether information from disparate sources. These inferences can have negative impact on an individual \\neven if the inferences are not accurate (e.g., confabulations), and especially if they reveal information \\nthat the individual considers sensitive or that is used to disadvantage or harm them. \\nBeyond harms from information exposure (such as extortion or dignitary harm), wrong or inappropriate \\ninferences of PII can contribute to downstream or secondary harmful impacts. For example, predictive \\ninferences made by GAI models based on PII or protected attributes can contribute to adverse decisions, \\nleading to representational or allocative harms to individuals or groups (see Harmful Bias and \\nHomogenization below).'), Document(metadata={'subject': '', 'creationDate': \"D:20240805141702-04'00'\", 'author': 'National Institute of Standards and Technology', 'format': 'PDF 1.6', 'total_pages': 64, 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'trapped': '', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'keywords': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'page': 14, 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'modDate': \"D:20240805143048-04'00'\", '_id': '8bdf790b-d9d1-4f90-a508-1623a41d810b', '_collection_name': 'ai_policy'}, page_content='11 \\nvalue chain (e.g., data inputs, processing, GAI training, or deployment environments), conventional \\ncybersecurity practices may need to adapt or evolve. \\nFor instance, prompt injection involves modifying what input is provided to a GAI system so that it \\nbehaves in unintended ways. In direct prompt injections, attackers might craft malicious prompts and \\ninput them directly to a GAI system, with a variety of downstream negative consequences to \\ninterconnected systems. Indirect prompt injection attacks occur when adversaries remotely (i.e., without \\na direct interface) exploit LLM-integrated applications by injecting prompts into data likely to be \\nretrieved. Security researchers have already demonstrated how indirect prompt injections can exploit \\nvulnerabilities by stealing proprietary data or running malicious code remotely on a machine. Merely \\nquerying a closed production model can elicit previously undisclosed information about that model. \\nAnother cybersecurity risk to GAI is data poisoning, in which an adversary compromises a training \\ndataset used by a model to manipulate its outputs or operation. Malicious tampering with data or parts \\nof the model could exacerbate risks associated with GAI system outputs. \\nTrustworthy AI Characteristics: Privacy Enhanced, Safe, Secure and Resilient, Valid and Reliable \\n2.10. \\nIntellectual Property \\nIntellectual property risks from GAI systems may arise where the use of copyrighted works is not a fair \\nuse under the fair use doctrine. If a GAI system’s training data included copyrighted material, GAI \\noutputs displaying instances of training data memorization (see Data Privacy above) could infringe on \\ncopyright. \\nHow GAI relates to copyright, including the status of generated content that is similar to but does not \\nstrictly copy work protected by copyright, is currently being debated in legal fora. Similar discussions are \\ntaking place regarding the use or emulation of personal identity, likeness, or voice without permission.  \\nTrustworthy AI Characteristics: Accountable and Transparent, Fair with Harmful Bias Managed, Privacy \\nEnhanced  \\n2.11. \\nObscene, Degrading, and/or Abusive Content \\nGAI can ease the production of and access to illegal non-consensual intimate imagery (NCII) of adults, \\nand/or child sexual abuse material (CSAM). GAI-generated obscene, abusive or degrading content can \\ncreate privacy, psychological and emotional, and even physical harms, and in some cases may be illegal.  \\nGenerated explicit or obscene AI content may include highly realistic “deepfakes” of real individuals, \\nincluding children. The spread of this kind of material can have downstream negative consequences: in \\nthe context of CSAM, even if the generated images do not resemble speciﬁc individuals, the prevalence \\nof such images can divert time and resources from eﬀorts to ﬁnd real-world victims. Outside of CSAM, \\nthe creation and spread of NCII disproportionately impacts women and sexual minorities, and can have \\nsubsequent negative consequences including decline in overall mental health, substance abuse, and \\neven suicidal thoughts.  \\nData used for training GAI models may unintentionally include CSAM and NCII. A recent report noted \\nthat several commonly used GAI training datasets were found to contain hundreds of known images of'), Document(metadata={'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'page': 8, 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'total_pages': 64, 'creator': 'Acrobat PDFMaker 24 for Word', 'subject': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'creationDate': \"D:20240805141702-04'00'\", 'trapped': '', 'producer': 'Adobe PDF Library 24.2.159', 'modDate': \"D:20240805143048-04'00'\", 'keywords': '', 'format': 'PDF 1.6', '_id': 'f1c71f5e-8d47-4259-a0f3-dab182a610ef', '_collection_name': 'ai_policy'}, page_content='5 \\noperations, or other cyberattacks; increased attack surface for targeted cyberattacks, which may \\ncompromise a system’s availability or the conﬁdentiality or integrity of training data, code, or \\nmodel weights.  \\n10. Intellectual Property: Eased production or replication of alleged copyrighted, trademarked, or \\nlicensed content without authorization (possibly in situations which do not fall under fair use); \\neased exposure of trade secrets; or plagiarism or illegal replication.  \\n11. Obscene, Degrading, and/or Abusive Content: Eased production of and access to obscene, \\ndegrading, and/or abusive imagery which can cause harm, including synthetic child sexual abuse \\nmaterial (CSAM), and nonconsensual intimate images (NCII) of adults. \\n12. Value Chain and Component Integration: Non-transparent or untraceable integration of \\nupstream third-party components, including data that has been improperly obtained or not \\nprocessed and cleaned due to increased automation from GAI; improper supplier vetting across \\nthe AI lifecycle; or other issues that diminish transparency or accountability for downstream \\nusers. \\n2.1. CBRN Information or Capabilities \\nIn the future, GAI may enable malicious actors to more easily access CBRN weapons and/or relevant \\nknowledge, information, materials, tools, or technologies that could be misused to assist in the design, \\ndevelopment, production, or use of CBRN weapons or other dangerous materials or agents. While \\nrelevant biological and chemical threat knowledge and information is often publicly accessible, LLMs \\ncould facilitate its analysis or synthesis, particularly by individuals without formal scientiﬁc training or \\nexpertise.  \\nRecent research on this topic found that LLM outputs regarding biological threat creation and attack \\nplanning provided minimal assistance beyond traditional search engine queries, suggesting that state-of-\\nthe-art LLMs at the time these studies were conducted do not substantially increase the operational \\nlikelihood of such an attack. The physical synthesis development, production, and use of chemical or \\nbiological agents will continue to require both applicable expertise and supporting materials and \\ninfrastructure. The impact of GAI on chemical or biological agent misuse will depend on what the key \\nbarriers for malicious actors are (e.g., whether information access is one such barrier), and how well GAI \\ncan help actors address those barriers.  \\nFurthermore, chemical and biological design tools (BDTs) – highly specialized AI systems trained on \\nscientiﬁc data that aid in chemical and biological design – may augment design capabilities in chemistry \\nand biology beyond what text-based LLMs are able to provide. As these models become more \\neﬃcacious, including for beneﬁcial uses, it will be important to assess their potential to be used for \\nharm, such as the ideation and design of novel harmful chemical or biological agents.  \\nWhile some of these described capabilities lie beyond the reach of existing GAI tools, ongoing \\nassessments of this risk would be enhanced by monitoring both the ability of AI tools to facilitate CBRN \\nweapons planning and GAI systems’ connection or access to relevant data and tools. \\nTrustworthy AI Characteristic: Safe, Explainable and Interpretable')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='How can evaluations involving human subjects meet applicable requirements and be representative of the relevant population in the context of GAI applications?' id='9083e49c-9fc2-42c0-92b5-be54824e91ff'\n",
      "****Adding new context: [Document(metadata={'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'trapped': '', 'total_pages': 64, 'modDate': \"D:20240805143048-04'00'\", 'producer': 'Adobe PDF Library 24.2.159', 'author': 'National Institute of Standards and Technology', 'page': 33, 'keywords': '', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'format': 'PDF 1.6', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'creator': 'Acrobat PDFMaker 24 for Word', 'creationDate': \"D:20240805141702-04'00'\", 'subject': '', '_id': '4e1f1011-857e-463c-8453-c81883444e2f', '_collection_name': 'ai_policy'}, page_content='30 \\nMEASURE 2.2: Evaluations involving human subjects meet applicable requirements (including human subject protection) and are \\nrepresentative of the relevant population. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-2.2-001 Assess and manage statistical biases related to GAI content provenance through \\ntechniques such as re-sampling, re-weighting, or adversarial training. \\nInformation Integrity; Information \\nSecurity; Harmful Bias and \\nHomogenization \\nMS-2.2-002 \\nDocument how content provenance data is tracked and how that data interacts \\nwith privacy and security. Consider: Anonymizing data to protect the privacy of \\nhuman subjects; Leveraging privacy output ﬁlters; Removing any personally \\nidentiﬁable information (PII) to prevent potential harm or misuse. \\nData Privacy; Human AI \\nConﬁguration; Information \\nIntegrity; Information Security; \\nDangerous, Violent, or Hateful \\nContent \\nMS-2.2-003 Provide human subjects with options to withdraw participation or revoke their \\nconsent for present or future use of their data in GAI applications.  \\nData Privacy; Human-AI \\nConﬁguration; Information \\nIntegrity \\nMS-2.2-004 \\nUse techniques such as anonymization, diﬀerential privacy or other privacy-\\nenhancing technologies to minimize the risks associated with linking AI-generated \\ncontent back to individual human subjects. \\nData Privacy; Human-AI \\nConﬁguration \\nAI Actor Tasks: AI Development, Human Factors, TEVV \\n \\nMEASURE 2.3: AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for \\nconditions similar to deployment setting(s). Measures are documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-2.3-001 Consider baseline model performance on suites of benchmarks when selecting a \\nmodel for ﬁne tuning or enhancement with retrieval-augmented generation. \\nInformation Security; \\nConfabulation \\nMS-2.3-002 Evaluate claims of model capabilities using empirically validated methods. \\nConfabulation; Information \\nSecurity \\nMS-2.3-003 Share results of pre-deployment testing with relevant GAI Actors, such as those \\nwith system release approval authority. \\nHuman-AI Conﬁguration'), Document(metadata={'subject': '', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'producer': 'Adobe PDF Library 24.2.159', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'keywords': '', 'total_pages': 64, 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'author': 'National Institute of Standards and Technology', 'creator': 'Acrobat PDFMaker 24 for Word', 'page': 51, 'format': 'PDF 1.6', 'trapped': '', '_id': 'c1fe7d74-5109-4641-b0c6-8cbafb2f1f68', '_collection_name': 'ai_policy'}, page_content='48 \\n• Data protection \\n• Data retention  \\n• Consistency in use of deﬁning key terms \\n• Decommissioning \\n• Discouraging anonymous use \\n• Education  \\n• Impact assessments  \\n• Incident response \\n• Monitoring \\n• Opt-outs  \\n• Risk-based controls \\n• Risk mapping and measurement \\n• Science-backed TEVV practices \\n• Secure software development practices \\n• Stakeholder engagement \\n• Synthetic content detection and \\nlabeling tools and techniques \\n• Whistleblower protections \\n• Workforce diversity and \\ninterdisciplinary teams\\nEstablishing acceptable use policies and guidance for the use of GAI in formal human-AI teaming settings \\nas well as diﬀerent levels of human-AI conﬁgurations can help to decrease risks arising from misuse, \\nabuse, inappropriate repurpose, and misalignment between systems and users. These practices are just \\none example of adapting existing governance protocols for GAI contexts.  \\nA.1.3. Third-Party Considerations \\nOrganizations may seek to acquire, embed, incorporate, or use open-source or proprietary third-party \\nGAI models, systems, or generated data for various applications across an enterprise. Use of these GAI \\ntools and inputs has implications for all functions of the organization – including but not limited to \\nacquisition, human resources, legal, compliance, and IT services – regardless of whether they are carried \\nout by employees or third parties. Many of the actions cited above are relevant and options for \\naddressing third-party considerations. \\nThird party GAI integrations may give rise to increased intellectual property, data privacy, or information \\nsecurity risks, pointing to the need for clear guidelines for transparency and risk management regarding \\nthe collection and use of third-party data for model inputs. Organizations may consider varying risk \\ncontrols for foundation models, ﬁne-tuned models, and embedded tools, enhanced processes for \\ninteracting with external GAI technologies or service providers. Organizations can apply standard or \\nexisting risk controls and processes to proprietary or open-source GAI technologies, data, and third-party \\nservice providers, including acquisition and procurement due diligence, requests for software bills of \\nmaterials (SBOMs), application of service level agreements (SLAs), and statement on standards for \\nattestation engagement (SSAE) reports to help with third-party transparency and risk management for \\nGAI systems. \\nA.1.4. Pre-Deployment Testing \\nOverview \\nThe diverse ways and contexts in which GAI systems may be developed, used, and repurposed \\ncomplicates risk mapping and pre-deployment measurement eﬀorts. Robust test, evaluation, validation, \\nand veriﬁcation (TEVV) processes can be iteratively applied – and documented – in early stages of the AI \\nlifecycle and informed by representative AI Actors (see Figure 3 of the AI RMF). Until new and rigorous'), Document(metadata={'trapped': '', 'keywords': '', 'producer': 'Adobe PDF Library 24.2.159', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'total_pages': 64, 'subject': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'page': 34, 'author': 'National Institute of Standards and Technology', 'creationDate': \"D:20240805141702-04'00'\", 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'format': 'PDF 1.6', 'modDate': \"D:20240805143048-04'00'\", '_id': 'e013cb43-4e0b-4958-9e0c-d22d810521de', '_collection_name': 'ai_policy'}, page_content='31 \\nMS-2.3-004 \\nUtilize a purpose-built testing environment such as NIST Dioptra to empirically \\nevaluate GAI trustworthy characteristics. \\nCBRN Information or Capabilities; \\nData Privacy; Confabulation; \\nInformation Integrity; Information \\nSecurity; Dangerous, Violent, or \\nHateful Content; Harmful Bias and \\nHomogenization \\nAI Actor Tasks: AI Deployment, TEVV \\n \\nMEASURE 2.5: The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability beyond the \\nconditions under which the technology was developed are documented. \\nAction ID \\nSuggested Action \\nRisks \\nMS-2.5-001 Avoid extrapolating GAI system performance or capabilities from narrow, non-\\nsystematic, and anecdotal assessments. \\nHuman-AI Conﬁguration; \\nConfabulation \\nMS-2.5-002 \\nDocument the extent to which human domain knowledge is employed to \\nimprove GAI system performance, via, e.g., RLHF, ﬁne-tuning, retrieval-\\naugmented generation, content moderation, business rules. \\nHuman-AI Conﬁguration \\nMS-2.5-003 Review and verify sources and citations in GAI system outputs during pre-\\ndeployment risk measurement and ongoing monitoring activities. \\nConfabulation \\nMS-2.5-004 Track and document instances of anthropomorphization (e.g., human images, \\nmentions of human feelings, cyborg imagery or motifs) in GAI system interfaces. Human-AI Conﬁguration \\nMS-2.5-005 Verify GAI system training data and TEVV data provenance, and that ﬁne-tuning \\nor retrieval-augmented generation data is grounded. \\nInformation Integrity \\nMS-2.5-006 \\nRegularly review security and safety guardrails, especially if the GAI system is \\nbeing operated in novel circumstances. This includes reviewing reasons why the \\nGAI system was initially assessed as being safe to deploy.  \\nInformation Security; Dangerous, \\nViolent, or Hateful Content \\nAI Actor Tasks: Domain Experts, TEVV'), Document(metadata={'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'producer': 'Adobe PDF Library 24.2.159', 'page': 50, 'creationDate': \"D:20240805141702-04'00'\", 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'trapped': '', 'author': 'National Institute of Standards and Technology', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'format': 'PDF 1.6', 'keywords': '', 'total_pages': 64, 'subject': '', 'modDate': \"D:20240805143048-04'00'\", 'creator': 'Acrobat PDFMaker 24 for Word', '_id': '21e8c6b5-98cf-4396-9030-866c59e85f9e', '_collection_name': 'ai_policy'}, page_content='47 \\nAppendix A. Primary GAI Considerations \\nThe following primary considerations were derived as overarching themes from the GAI PWG \\nconsultation process. These considerations (Governance, Pre-Deployment Testing, Content Provenance, \\nand Incident Disclosure) are relevant for voluntary use by any organization designing, developing, and \\nusing GAI and also inform the Actions to Manage GAI risks. Information included about the primary \\nconsiderations is not exhaustive, but highlights the most relevant topics derived from the GAI PWG.  \\nAcknowledgments: These considerations could not have been surfaced without the helpful analysis and \\ncontributions from the community and NIST staﬀ GAI PWG leads: George Awad, Luca Belli, Harold Booth, \\nMat Heyman, Yooyoung Lee, Mark Pryzbocki, Reva Schwartz, Martin Stanley, and Kyra Yee. \\nA.1. Governance \\nA.1.1. Overview \\nLike any other technology system, governance principles and techniques can be used to manage risks \\nrelated to generative AI models, capabilities, and applications. Organizations may choose to apply their \\nexisting risk tiering to GAI systems, or they may opt to revise or update AI system risk levels to address \\nthese unique GAI risks. This section describes how organizational governance regimes may be re-\\nevaluated and adjusted for GAI contexts. It also addresses third-party considerations for governing across \\nthe AI value chain.  \\nA.1.2. Organizational Governance \\nGAI opportunities, risks and long-term performance characteristics are typically less well-understood \\nthan non-generative AI tools and may be perceived and acted upon by humans in ways that vary greatly. \\nAccordingly, GAI may call for diﬀerent levels of oversight from AI Actors or diﬀerent human-AI \\nconﬁgurations in order to manage their risks eﬀectively. Organizations’ use of GAI systems may also \\nwarrant additional human review, tracking and documentation, and greater management oversight.  \\nAI technology can produce varied outputs in multiple modalities and present many classes of user \\ninterfaces. This leads to a broader set of AI Actors interacting with GAI systems for widely diﬀering \\napplications and contexts of use. These can include data labeling and preparation, development of GAI \\nmodels, content moderation, code generation and review, text generation and editing, image and video \\ngeneration, summarization, search, and chat. These activities can take place within organizational \\nsettings or in the public domain. \\nOrganizations can restrict AI applications that cause harm, exceed stated risk tolerances, or that conﬂict \\nwith their tolerances or values. Governance tools and protocols that are applied to other types of AI \\nsystems can be applied to GAI systems. These plans and actions include: \\n• Accessibility and reasonable \\naccommodations \\n• AI actor credentials and qualiﬁcations  \\n• Alignment to organizational values \\n• Auditing and assessment \\n• Change-management controls \\n• Commercial use \\n• Data provenance')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='What stakeholders were involved in providing ideas related to the development of the Blueprint for an AI Bill of Rights?' id='c17ec770-a5c7-43a1-83b9-7d621efabf11'\n",
      "****Adding new context: [Document(metadata={'trapped': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'total_pages': 73, 'keywords': '', 'creationDate': \"D:20220920133035-04'00'\", 'subject': '', 'modDate': \"D:20221003104118-04'00'\", 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'page': 61, 'format': 'PDF 1.6', '_id': '7d55d565-5ea2-4cc9-a813-9bf14793c862', '_collection_name': 'ai_policy'}, page_content=\"APPENDIX\\n• OSTP conducted meetings with a variety of stakeholders in the private sector and civil society. Some of these\\nmeetings were specifically focused on providing ideas related to the development of the Blueprint for an AI\\nBill of Rights while others provided useful general context on the positive use cases, potential harms, and/or\\noversight possibilities for these technologies. Participants in these conversations from the private sector and\\ncivil society included:\\nAdobe \\nAmerican Civil Liberties Union \\n(ACLU) \\nThe Aspen Commission on \\nInformation Disorder \\nThe Awood Center \\nThe Australian Human Rights \\nCommission \\nBiometrics Institute \\nThe Brookings Institute \\nBSA | The Software Alliance \\nCantellus Group \\nCenter for American Progress \\nCenter for Democracy and \\nTechnology \\nCenter on Privacy and Technology \\nat Georgetown Law \\nChristiana Care \\nColor of Change \\nCoworker \\nData Robot \\nData Trust Alliance \\nData and Society Research Institute \\nDeepmind \\nEdSAFE AI Alliance \\nElectronic Privacy Information \\nCenter (EPIC) \\nEncode Justice \\nEqual AI \\nGoogle \\nHitachi's AI Policy Committee \\nThe Innocence Project \\nInstitute of Electrical and \\nElectronics Engineers (IEEE) \\nIntuit \\nLawyers Committee for Civil Rights \\nUnder Law \\nLegal Aid Society \\nThe Leadership Conference on \\nCivil and Human Rights \\nMeta \\nMicrosoft \\nThe MIT AI Policy Forum \\nMovement Alliance Project \\nThe National Association of \\nCriminal Defense Lawyers \\nO’Neil Risk Consulting & \\nAlgorithmic Auditing \\nThe Partnership on AI \\nPinterest \\nThe Plaintext Group \\npymetrics \\nSAP \\nThe Security Industry Association \\nSoftware and Information Industry \\nAssociation (SIIA) \\nSpecial Competitive Studies Project \\nThorn \\nUnited for Respect \\nUniversity of California at Berkeley \\nCitris Policy Lab \\nUniversity of California at Berkeley \\nLabor Center \\nUnfinished/Project Liberty \\nUpturn \\nUS Chamber of Commerce \\nUS Chamber of Commerce \\nTechnology Engagement Center \\nA.I. Working Group\\nVibrent Health\\nWarehouse Worker Resource\\nCenter\\nWaymap\\n62\"), Document(metadata={'subject': '', 'keywords': '', 'title': 'Blueprint for an AI Bill of Rights', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'producer': 'iLovePDF', 'format': 'PDF 1.6', 'page': 3, 'creationDate': \"D:20220920133035-04'00'\", 'author': '', 'trapped': '', 'modDate': \"D:20221003104118-04'00'\", 'total_pages': 73, 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', '_id': '18dcce2b-8197-4aec-a729-b19f3b2b5406', '_collection_name': 'ai_policy'}, page_content='ABOUT THIS FRAMEWORK\\xad\\xad\\xad\\xad\\xad\\nThe Blueprint for an AI Bill of Rights is a set of five principles and associated practices to help guide the \\ndesign, use, and deployment of automated systems to protect the rights of the American public in the age of \\nartificial intel-ligence. Developed through extensive consultation with the American public, these principles are \\na blueprint for building and deploying automated systems that are aligned with democratic values and protect \\ncivil rights, civil liberties, and privacy. The Blueprint for an AI Bill of Rights includes this Foreword, the five \\nprinciples, notes on Applying the The Blueprint for an AI Bill of Rights, and a Technical Companion that gives \\nconcrete steps that can be taken by many kinds of organizations—from governments at all levels to companies of \\nall sizes—to uphold these values. Experts from across the private sector, governments, and international \\nconsortia have published principles and frameworks to guide the responsible use of automated systems; this \\nframework provides a national values statement and toolkit that is sector-agnostic to inform building these \\nprotections into policy, practice, or the technological design process.  Where existing law or policy—such as \\nsector-specific privacy laws and oversight requirements—do not already provide guidance, the Blueprint for an \\nAI Bill of Rights should be used to inform policy decisions.\\nLISTENING TO THE AMERICAN PUBLIC\\nThe White House Office of Science and Technology Policy has led a year-long process to seek and distill input \\nfrom people across the country—from impacted communities and industry stakeholders to technology develop-\\ners and other experts across fields and sectors, as well as policymakers throughout the Federal government—on \\nthe issue of algorithmic and data-driven harms and potential remedies. Through panel discussions, public listen-\\ning sessions, meetings, a formal request for information, and input to a publicly accessible and widely-publicized \\nemail address, people throughout the United States, public servants across Federal agencies, and members of the \\ninternational community spoke up about both the promises and potential harms of these technologies, and \\nplayed a central role in shaping the Blueprint for an AI Bill of Rights. The core messages gleaned from these \\ndiscussions include that AI has transformative potential to improve Americans’ lives, and that preventing the \\nharms of these technologies is both necessary and achievable. The Appendix includes a full list of public engage-\\nments. \\n4'), Document(metadata={'author': '', 'format': 'PDF 1.6', 'keywords': '', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'producer': 'iLovePDF', 'title': 'Blueprint for an AI Bill of Rights', 'modDate': \"D:20221003104118-04'00'\", 'creationDate': \"D:20220920133035-04'00'\", 'trapped': '', 'page': 1, 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'subject': '', 'total_pages': 73, 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', '_id': 'bf101eca-72b8-45e9-86bc-d83df72f9776', '_collection_name': 'ai_policy'}, page_content='About this Document \\nThe Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People was \\npublished by the White House Office of Science and Technology Policy in October 2022. This framework was \\nreleased one year after OSTP announced the launch of a process to develop “a bill of rights for an AI-powered \\nworld.” Its release follows a year of public engagement to inform this initiative. The framework is available \\nonline at: https://www.whitehouse.gov/ostp/ai-bill-of-rights \\nAbout the Office of Science and Technology Policy \\nThe Office of Science and Technology Policy (OSTP) was established by the National Science and Technology \\nPolicy, Organization, and Priorities Act of 1976 to provide the President and others within the Executive Office \\nof the President with advice on the scientific, engineering, and technological aspects of the economy, national \\nsecurity, health, foreign relations, the environment, and the technological recovery and use of resources, among \\nother topics. OSTP leads interagency science and technology policy coordination efforts, assists the Office of \\nManagement and Budget (OMB) with an annual review and analysis of Federal research and development in \\nbudgets, and serves as a source of scientific and technological analysis and judgment for the President with \\nrespect to major policies, plans, and programs of the Federal Government. \\nLegal Disclaimer \\nThe Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People is a white paper \\npublished by the White House Office of Science and Technology Policy. It is intended to support the \\ndevelopment of policies and practices that protect civil rights and promote democratic values in the building, \\ndeployment, and governance of automated systems. \\nThe Blueprint for an AI Bill of Rights is non-binding and does not constitute U.S. government policy. It \\ndoes not supersede, modify, or direct an interpretation of any existing statute, regulation, policy, or \\ninternational instrument. It does not constitute binding guidance for the public or Federal agencies and \\ntherefore does not require compliance with the principles described herein. It also is not determinative of what \\nthe U.S. government’s position will be in any international negotiation. Adoption of these principles may not \\nmeet the requirements of existing statutes, regulations, policies, or international instruments, or the \\nrequirements of the Federal agencies that enforce them. These principles are not intended to, and do not, \\nprohibit or limit any lawful activity of a government agency, including law enforcement, national security, or \\nintelligence activities. \\nThe appropriate application of the principles set forth in this white paper depends significantly on the \\ncontext in which automated systems are being utilized. In some circumstances, application of these principles \\nin whole or in part may not be appropriate given the intended use of automated systems to achieve government \\nagency missions. Future sector-specific guidance will likely be necessary and important for guiding the use of \\nautomated systems in certain settings such as AI systems used as part of school building security or automated \\nhealth diagnostic systems. \\nThe Blueprint for an AI Bill of Rights recognizes that law enforcement activities require a balancing of \\nequities, for example, between the protection of sensitive law enforcement information and the principle of \\nnotice; as such, notice may not be appropriate, or may need to be adjusted to protect sources, methods, and \\nother law enforcement equities. Even in contexts where these principles may not apply in whole or in part, \\nfederal departments and agencies remain subject to judicial, privacy, and civil liberties oversight as well as \\nexisting policies and safeguards that govern automated systems, including, for example, Executive Order 13960, \\nPromoting the Use of Trustworthy Artificial Intelligence in the Federal Government (December 2020). \\nThis white paper recognizes that national security (which includes certain law enforcement and \\nhomeland security activities) and defense activities are of increased sensitivity and interest to our nation’s \\nadversaries and are often subject to special requirements, such as those governing classified information and \\nother protected data. Such activities require alternative, compatible safeguards through existing policies that \\ngovern automated systems and AI, such as the Department of Defense (DOD) AI Ethical Principles and \\nResponsible AI Implementation Pathway and the Intelligence Community (IC) AI Ethics Principles and \\nFramework. The implementation of these policies to national security and defense activities can be informed by \\nthe Blueprint for an AI Bill of Rights where feasible. \\nThe Blueprint for an AI Bill of Rights is not intended to, and does not, create any legal right, benefit, or \\ndefense, substantive or procedural, enforceable at law or in equity by any party against the United States, its \\ndepartments, agencies, or entities, its officers, employees, or agents, or any other person, nor does it constitute a \\nwaiver of sovereign immunity. \\nCopyright Information \\nThis document is a work of the United States Government and is in the public domain (see 17 U.S.C. §105). \\n2'), Document(metadata={'page': 54, 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'format': 'PDF 1.6', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'total_pages': 73, 'keywords': '', 'title': 'Blueprint for an AI Bill of Rights', 'subject': '', 'trapped': '', 'author': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'producer': 'iLovePDF', '_id': 'da3c6ba4-aab4-46ad-b785-aa37de494ce3', '_collection_name': 'ai_policy'}, page_content='SECTION TITLE\\nAPPENDIX\\nListening to the American People \\nThe White House Office of Science and Technology Policy (OSTP) led a yearlong process to seek and distill \\ninput from people across the country – from impacted communities to industry stakeholders to \\ntechnology developers to other experts across fields and sectors, as well as policymakers across the Federal \\ngovernment – on the issue of algorithmic and data-driven harms and potential remedies. Through panel \\ndiscussions, public listening sessions, private meetings, a formal request for information, and input to a \\npublicly accessible and widely-publicized email address, people across the United States spoke up about \\nboth the promises and potential harms of these technologies, and played a central role in shaping the \\nBlueprint for an AI Bill of Rights. \\nPanel Discussions to Inform the Blueprint for An AI Bill of Rights \\nOSTP co-hosted a series of six panel discussions in collaboration with the Center for American Progress, \\nthe Joint Center for Political and Economic Studies, New America, the German Marshall Fund, the Electronic \\nPrivacy Information Center, and the Mozilla Foundation. The purpose of these convenings – recordings of \\nwhich are publicly available online112 – was to bring together a variety of experts, practitioners, advocates \\nand federal government officials to offer insights and analysis on the risks, harms, benefits, and \\npolicy opportunities of automated systems. Each panel discussion was organized around a wide-ranging \\ntheme, exploring current challenges and concerns and considering what an automated society that \\nrespects democratic values should look like. These discussions focused on the topics of consumer \\nrights and protections, the criminal justice system, equal opportunities and civil justice, artificial \\nintelligence and democratic values, social welfare and development, and the healthcare system. \\nSummaries of Panel Discussions: \\nPanel 1: Consumer Rights and Protections. This event explored the opportunities and challenges for \\nindividual consumers and communities in the context of a growing ecosystem of AI-enabled consumer \\nproducts, advanced platforms and services, “Internet of Things” (IoT) devices, and smart city products and \\nservices. \\nWelcome:\\n•\\nRashida Richardson, Senior Policy Advisor for Data and Democracy, White House Office of Science and\\nTechnology Policy\\n•\\nKaren Kornbluh, Senior Fellow and Director of the Digital Innovation and Democracy Initiative, German\\nMarshall Fund\\nModerator: \\nDevin E. Willis, Attorney, Division of Privacy and Identity Protection, Bureau of Consumer Protection, Federal \\nTrade Commission \\nPanelists: \\n•\\nTamika L. Butler, Principal, Tamika L. Butler Consulting\\n•\\nJennifer Clark, Professor and Head of City and Regional Planning, Knowlton School of Engineering, Ohio\\nState University\\n•\\nCarl Holshouser, Senior Vice President for Operations and Strategic Initiatives, TechNet\\n•\\nSurya Mattu, Senior Data Engineer and Investigative Data Journalist, The Markup\\n•\\nMariah Montgomery, National Campaign Director, Partnership for Working Families\\n55')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='How do companies use surveillance software to track employee discussions about union activity?' id='2cde5129-af89-4489-81c8-5e97a4e30703'\n",
      "****Adding new context: [Document(metadata={'trapped': '', 'subject': '', 'page': 36, 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'title': 'Blueprint for an AI Bill of Rights', 'format': 'PDF 1.6', 'producer': 'iLovePDF', 'modDate': \"D:20221003104118-04'00'\", 'keywords': '', 'total_pages': 73, 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'author': '', 'creationDate': \"D:20220920133035-04'00'\", '_id': '42a6c7e5-fb82-4144-93f0-1a0796b3211a', '_collection_name': 'ai_policy'}, page_content='DATA PRIVACY \\nEXTRA PROTECTIONS FOR DATA RELATED TO SENSITIVE\\nDOMAINS\\n•\\nContinuous positive airway pressure machines gather data for medical purposes, such as diagnosing sleep\\napnea, and send usage data to a patient’s insurance company, which may subsequently deny coverage for the\\ndevice based on usage data. Patients were not aware that the data would be used in this way or monitored\\nby anyone other than their doctor.70 \\n•\\nA department store company used predictive analytics applied to collected consumer data to determine that a\\nteenage girl was pregnant, and sent maternity clothing ads and other baby-related advertisements to her\\nhouse, revealing to her father that she was pregnant.71\\n•\\nSchool audio surveillance systems monitor student conversations to detect potential \"stress indicators\" as\\na warning of potential violence.72 Online proctoring systems claim to detect if a student is cheating on an\\nexam using biometric markers.73 These systems have the potential to limit student freedom to express a range\\nof emotions at school and may inappropriately flag students with disabilities who need accommodations or\\nuse screen readers or dictation software as cheating.74\\n•\\nLocation data, acquired from a data broker, can be used to identify people who visit abortion clinics.75\\n•\\nCompanies collect student data such as demographic information, free or reduced lunch status, whether\\nthey\\'ve used drugs, or whether they\\'ve expressed interest in LGBTQI+ groups, and then use that data to \\nforecast student success.76 Parents and education experts have expressed concern about collection of such\\nsensitive data without express parental consent, the lack of transparency in how such data is being used, and\\nthe potential for resulting discriminatory impacts.\\n• Many employers transfer employee data to third party job verification services. This information is then used\\nby potential future employers, banks, or landlords. In one case, a former employee alleged that a\\ncompany supplied false data about her job title which resulted in a job offer being revoked.77\\n37'), Document(metadata={'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'author': '', 'format': 'PDF 1.6', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'producer': 'iLovePDF', 'page': 31, 'trapped': '', 'title': 'Blueprint for an AI Bill of Rights', 'keywords': '', 'total_pages': 73, 'subject': '', 'modDate': \"D:20221003104118-04'00'\", 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'creationDate': \"D:20220920133035-04'00'\", '_id': '12ab4485-e029-485f-aee9-9cb9a243069b', '_collection_name': 'ai_policy'}, page_content=\"DATA PRIVACY \\nWHY THIS PRINCIPLE IS IMPORTANT\\nThis section provides a brief summary of the problems which the principle seeks to address and protect \\nagainst, including illustrative examples. \\n•\\nAn insurer might collect data from a person's social media presence as part of deciding what life\\ninsurance rates they should be offered.64\\n•\\nA data broker harvested large amounts of personal data and then suffered a breach, exposing hundreds of\\nthousands of people to potential identity theft. 65\\n•\\nA local public housing authority installed a facial recognition system at the entrance to housing complexes to\\nassist law enforcement with identifying individuals viewed via camera when police reports are filed, leading\\nthe community, both those living in the housing complex and not, to have videos of them sent to the local\\npolice department and made available for scanning by its facial recognition software.66\\n•\\nCompanies use surveillance software to track employee discussions about union activity and use the\\nresulting data to surveil individual employees and surreptitiously intervene in discussions.67\\n32\"), Document(metadata={'creationDate': \"D:20220920133035-04'00'\", 'format': 'PDF 1.6', 'page': 68, 'total_pages': 73, 'modDate': \"D:20221003104118-04'00'\", 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'author': '', 'title': 'Blueprint for an AI Bill of Rights', 'keywords': '', 'subject': '', 'producer': 'iLovePDF', 'trapped': '', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', '_id': '0cccceb9-1783-481d-8cfa-a2dff5141da2', '_collection_name': 'ai_policy'}, page_content=\"65. See, e.g., Scott Ikeda. Major Data Broker Exposes 235 Million Social Media Profiles in Data Lead: Info\\nAppears to Have Been Scraped Without Permission. CPO Magazine. Aug. 28, 2020. https://\\nwww.cpomagazine.com/cyber-security/major-data-broker-exposes-235-million-social-media-profiles\\xad\\nin-data-leak/; Lily Hay Newman. 1.2 Billion Records Found Exposed Online in a Single Server. WIRED,\\nNov. 22, 2019. https://www.wired.com/story/billion-records-exposed-online/\\n66. Lola Fadulu. Facial Recognition Technology in Public Housing Prompts Backlash. New York Times.\\nSept. 24, 2019.\\nhttps://www.nytimes.com/2019/09/24/us/politics/facial-recognition-technology-housing.html\\n67. Jo Constantz. ‘They Were Spying On Us’: Amazon, Walmart, Use Surveillance Technology to Bust\\nUnions. Newsweek. Dec. 13, 2021.\\nhttps://www.newsweek.com/they-were-spying-us-amazon-walmart-use-surveillance-technology-bust\\xad\\nunions-1658603\\n68. See, e.g., enforcement actions by the FTC against the photo storage app Everalbaum\\n(https://www.ftc.gov/legal-library/browse/cases-proceedings/192-3172-everalbum-inc-matter), and\\nagainst Weight Watchers and their subsidiary Kurbo\\n(https://www.ftc.gov/legal-library/browse/cases-proceedings/1923228-weight-watchersww)\\n69. See, e.g., HIPAA, Pub. L 104-191 (1996); Fair Debt Collection Practices Act (FDCPA), Pub. L. 95-109\\n(1977); Family Educational Rights and Privacy Act (FERPA) (20 U.S.C. § 1232g), Children's Online\\nPrivacy Protection Act of 1998, 15 U.S.C. 6501–6505, and Confidential Information Protection and\\nStatistical Efficiency Act (CIPSEA) (116 Stat. 2899)\\n70. Marshall Allen. You Snooze, You Lose: Insurers Make The Old Adage Literally True. ProPublica. Nov.\\n21, 2018.\\nhttps://www.propublica.org/article/you-snooze-you-lose-insurers-make-the-old-adage-literally-true\\n71. Charles Duhigg. How Companies Learn Your Secrets. The New York Times. Feb. 16, 2012.\\nhttps://www.nytimes.com/2012/02/19/magazine/shopping-habits.html\\n72. Jack Gillum and Jeff Kao. Aggression Detectors: The Unproven, Invasive Surveillance Technology\\nSchools are Using to Monitor Students. ProPublica. Jun. 25, 2019.\\nhttps://features.propublica.org/aggression-detector/the-unproven-invasive-surveillance-technology\\xad\\nschools-are-using-to-monitor-students/\\n73. Drew Harwell. Cheating-detection companies made millions during the pandemic. Now students are\\nfighting back. Washington Post. Nov. 12, 2020.\\nhttps://www.washingtonpost.com/technology/2020/11/12/test-monitoring-student-revolt/\\n74. See, e.g., Heather Morrison. Virtual Testing Puts Disabled Students at a Disadvantage. Government\\nTechnology. May 24, 2022.\\nhttps://www.govtech.com/education/k-12/virtual-testing-puts-disabled-students-at-a-disadvantage;\\nLydia X. Z. Brown, Ridhi Shetty, Matt Scherer, and Andrew Crawford. Ableism And Disability\\nDiscrimination In New Surveillance Technologies: How new surveillance technologies in education,\\npolicing, health care, and the workplace disproportionately harm disabled people. Center for Democracy\\nand Technology Report. May 24, 2022.\\nhttps://cdt.org/insights/ableism-and-disability-discrimination-in-new-surveillance-technologies-how\\xad\\nnew-surveillance-technologies-in-education-policing-health-care-and-the-workplace\\xad\\ndisproportionately-harm-disabled-people/\\n69\"), Document(metadata={'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'subject': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'author': '', 'page': 56, 'creationDate': \"D:20220920133035-04'00'\", 'trapped': '', 'format': 'PDF 1.6', 'keywords': '', 'producer': 'iLovePDF', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'title': 'Blueprint for an AI Bill of Rights', 'modDate': \"D:20221003104118-04'00'\", 'total_pages': 73, '_id': '1cdfde57-f564-49e1-81d5-cc21fedf16ea', '_collection_name': 'ai_policy'}, page_content=\"APPENDIX\\nPanel 3: Equal Opportunities and Civil Justice. This event explored current and emerging uses of \\ntechnology that impact equity of opportunity in employment, education, and housing. \\nWelcome: \\n•\\nRashida Richardson, Senior Policy Advisor for Data and Democracy, White House Office of Science and\\nTechnology Policy\\n•\\nDominique Harrison, Director for Technology Policy, The Joint Center for Political and Economic\\nStudies\\nModerator: Jenny Yang, Director, Office of Federal Contract Compliance Programs, Department of Labor \\nPanelists: \\n•\\nChristo Wilson, Associate Professor of Computer Science, Northeastern University\\n•\\nFrida Polli, CEO, Pymetrics\\n•\\nKaren Levy, Assistant Professor, Department of Information Science, Cornell University\\n•\\nNatasha Duarte, Project Director, Upturn\\n•\\nElana Zeide, Assistant Professor, University of Nebraska College of Law\\n•\\nFabian Rogers, Constituent Advocate, Office of NY State Senator Jabari Brisport and Community\\nAdvocate and Floor Captain, Atlantic Plaza Towers Tenants Association\\nThe individual panelists described the ways in which AI systems and other technologies are increasingly being \\nused to limit access to equal opportunities in education, housing, and employment. Education-related \\nconcerning uses included the increased use of remote proctoring systems, student location and facial \\nrecognition tracking, teacher evaluation systems, robot teachers, and more. Housing-related concerning uses \\nincluding automated tenant background screening and facial recognition-based controls to enter or exit \\nhousing complexes. Employment-related concerning uses included discrimination in automated hiring \\nscreening and workplace surveillance. Various panelists raised the limitations of existing privacy law as a key \\nconcern, pointing out that students should be able to reinvent themselves and require privacy of their student \\nrecords and education-related data in order to do so. The overarching concerns of surveillance in these \\ndomains included concerns about the chilling effects of surveillance on student expression, inappropriate \\ncontrol of tenants via surveillance, and the way that surveillance of workers blurs the boundary between work \\nand life and exerts extreme and potentially damaging control over workers' lives. Additionally, some panelists \\npointed out ways that data from one situation was misapplied in another in a way that limited people's \\nopportunities, for example data from criminal justice settings or previous evictions being used to block further \\naccess to housing. Throughout, various panelists emphasized that these technologies are being used to shift the \\nburden of oversight and efficiency from employers to workers, schools to students, and landlords to tenants, in \\nways that diminish and encroach on equality of opportunity; assessment of these technologies should include \\nwhether they are genuinely helpful in solving an identified problem. \\nIn discussion of technical and governance interventions that that are needed to protect against the harms of \\nthese technologies, panelists individually described the importance of: receiving community input into the \\ndesign and use of technologies, public reporting on crucial elements of these systems, better notice and consent \\nprocedures that ensure privacy based on context and use case, ability to opt-out of using these systems and \\nreceive a fallback to a human process, providing explanations of decisions and how these systems work, the \\nneed for governance including training in using these systems, ensuring the technological use cases are \\ngenuinely related to the goal task and are locally validated to work, and the need for institution and protection \\nof third party audits to ensure systems continue to be accountable and valid. \\n57\")]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='How can feedback improve AI system design and reduce risks?' id='1634d6a6-68a7-4432-a03c-e82bcbc17cd7'\n",
      "****Adding new context: [Document(metadata={'keywords': '', 'modDate': \"D:20240805143048-04'00'\", 'total_pages': 64, 'creator': 'Acrobat PDFMaker 24 for Word', 'subject': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'author': 'National Institute of Standards and Technology', 'format': 'PDF 1.6', 'producer': 'Adobe PDF Library 24.2.159', 'page': 21, 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'creationDate': \"D:20240805141702-04'00'\", 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'trapped': '', '_id': '609b8f12-08c0-46c5-8bf7-544c20ea982a', '_collection_name': 'ai_policy'}, page_content='18 \\nGOVERN 3.2: Policies and procedures are in place to deﬁne and diﬀerentiate roles and responsibilities for human-AI conﬁgurations \\nand oversight of AI systems. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-3.2-001 \\nPolicies are in place to bolster oversight of GAI systems with independent \\nevaluations or assessments of GAI models or systems where the type and \\nrobustness of evaluations are proportional to the identiﬁed risks. \\nCBRN Information or Capabilities; \\nHarmful Bias and Homogenization \\nGV-3.2-002 \\nConsider adjustment of organizational roles and components across lifecycle \\nstages of large or complex GAI systems, including: Test and evaluation, validation, \\nand red-teaming of GAI systems; GAI content moderation; GAI system \\ndevelopment and engineering; Increased accessibility of GAI tools, interfaces, and \\nsystems, Incident response and containment. \\nHuman-AI Conﬁguration; \\nInformation Security; Harmful Bias \\nand Homogenization \\nGV-3.2-003 \\nDeﬁne acceptable use policies for GAI interfaces, modalities, and human-AI \\nconﬁgurations (i.e., for chatbots and decision-making tasks), including criteria for \\nthe kinds of queries GAI applications should refuse to respond to.  \\nHuman-AI Conﬁguration \\nGV-3.2-004 \\nEstablish policies for user feedback mechanisms for GAI systems which include \\nthorough instructions and any mechanisms for recourse. \\nHuman-AI Conﬁguration  \\nGV-3.2-005 \\nEngage in threat modeling to anticipate potential risks from GAI systems. \\nCBRN Information or Capabilities; \\nInformation Security \\nAI Actors: AI Design \\n \\nGOVERN 4.1: Organizational policies and practices are in place to foster a critical thinking and safety-ﬁrst mindset in the design, \\ndevelopment, deployment, and uses of AI systems to minimize potential negative impacts. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-4.1-001 \\nEstablish policies and procedures that address continual improvement processes \\nfor GAI risk measurement. Address general risks associated with a lack of \\nexplainability and transparency in GAI systems by using ample documentation and \\ntechniques such as: application of gradient-based attributions, occlusion/term \\nreduction, counterfactual prompts and prompt engineering, and analysis of \\nembeddings; Assess and update risk measurement approaches at regular \\ncadences. \\nConfabulation \\nGV-4.1-002 \\nEstablish policies, procedures, and processes detailing risk measurement in \\ncontext of use with standardized measurement protocols and structured public \\nfeedback exercises such as AI red-teaming or independent external evaluations. \\nCBRN Information and Capability; \\nValue Chain and Component \\nIntegration'), Document(metadata={'format': 'PDF 1.6', 'producer': 'Adobe PDF Library 24.2.159', 'total_pages': 64, 'creationDate': \"D:20240805141702-04'00'\", 'trapped': '', 'modDate': \"D:20240805143048-04'00'\", 'page': 53, 'creator': 'Acrobat PDFMaker 24 for Word', 'author': 'National Institute of Standards and Technology', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'keywords': '', 'subject': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', '_id': '7b811177-2414-47d6-8931-e2b09eeddef1', '_collection_name': 'ai_policy'}, page_content='50 \\nParticipatory Engagement Methods \\nOn an ad hoc or more structured basis, organizations can design and use a variety of channels to engage \\nexternal stakeholders in product development or review. Focus groups with select experts can provide \\nfeedback on a range of issues. Small user studies can provide feedback from representative groups or \\npopulations. Anonymous surveys can be used to poll or gauge reactions to speciﬁc features. Participatory \\nengagement methods are often less structured than ﬁeld testing or red teaming, and are more \\ncommonly used in early stages of AI or product development.  \\nField Testing \\nField testing involves structured settings to evaluate risks and impacts and to simulate the conditions \\nunder which the GAI system will be deployed. Field style tests can be adapted from a focus on user \\npreferences and experiences towards AI risks and impacts – both negative and positive. When carried \\nout with large groups of users, these tests can provide estimations of the likelihood of risks and impacts \\nin real world interactions. \\nOrganizations may also collect feedback on outcomes, harms, and user experience directly from users in \\nthe production environment after a model has been released, in accordance with human subject \\nstandards such as informed consent and compensation. Organizations should follow applicable human \\nsubjects research requirements, and best practices such as informed consent and subject compensation, \\nwhen implementing feedback activities. \\nAI Red-teaming \\nAI red-teaming is an evolving practice that references exercises often conducted in a controlled \\nenvironment and in collaboration with AI developers building AI models to identify potential adverse \\nbehavior or outcomes of a GAI model or system, how they could occur, and stress test safeguards”. AI \\nred-teaming can be performed before or after AI models or systems are made available to the broader \\npublic; this section focuses on red-teaming in pre-deployment contexts.  \\nThe quality of AI red-teaming outputs is related to the background and expertise of the AI red team \\nitself. Demographically and interdisciplinarily diverse AI red teams can be used to identify ﬂaws in the \\nvarying contexts where GAI will be used. For best results, AI red teams should demonstrate domain \\nexpertise, and awareness of socio-cultural aspects within the deployment context. AI red-teaming results \\nshould be given additional analysis before they are incorporated into organizational governance and \\ndecision making, policy and procedural updates, and AI risk management eﬀorts. \\nVarious types of AI red-teaming may be appropriate, depending on the use case: \\n• \\nGeneral Public: Performed by general users (not necessarily AI or technical experts) who are \\nexpected to use the model or interact with its outputs, and who bring their own lived \\nexperiences and perspectives to the task of AI red-teaming. These individuals may have been \\nprovided instructions and material to complete tasks which may elicit harmful model behaviors. \\nThis type of exercise can be more eﬀective with large groups of AI red-teamers. \\n• \\nExpert: Performed by specialists with expertise in the domain or speciﬁc AI red-teaming context \\nof use (e.g., medicine, biotech, cybersecurity).  \\n• \\nCombination: In scenarios when it is diﬃcult to identify and recruit specialists with suﬃcient \\ndomain and contextual expertise, AI red-teaming exercises may leverage both expert and'), Document(metadata={'subject': '', 'author': 'National Institute of Standards and Technology', 'producer': 'Adobe PDF Library 24.2.159', 'page': 52, 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'format': 'PDF 1.6', 'modDate': \"D:20240805143048-04'00'\", 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'trapped': '', 'keywords': '', 'total_pages': 64, 'creationDate': \"D:20240805141702-04'00'\", 'creator': 'Acrobat PDFMaker 24 for Word', '_id': 'f923229c-1cd8-4b6c-8d03-5152882532e6', '_collection_name': 'ai_policy'}, page_content='49 \\nearly lifecycle TEVV approaches are developed and matured for GAI, organizations may use \\nrecommended “pre-deployment testing” practices to measure performance, capabilities, limits, risks, \\nand impacts. This section describes risk measurement and estimation as part of pre-deployment TEVV, \\nand examines the state of play for pre-deployment testing methodologies.  \\nLimitations of Current Pre-deployment Test Approaches \\nCurrently available pre-deployment TEVV processes used for GAI applications may be inadequate, non-\\nsystematically applied, or fail to reﬂect or mismatched to deployment contexts. For example, the \\nanecdotal testing of GAI system capabilities through video games or standardized tests designed for \\nhumans (e.g., intelligence tests, professional licensing exams) does not guarantee GAI system validity or \\nreliability in those domains. Similarly, jailbreaking or prompt engineering tests may not systematically \\nassess validity or reliability risks.  \\nMeasurement gaps can arise from mismatches between laboratory and real-world settings. Current \\ntesting approaches often remain focused on laboratory conditions or restricted to benchmark test \\ndatasets and in silico techniques that may not extrapolate well to—or directly assess GAI impacts in real-\\nworld conditions. For example, current measurement gaps for GAI make it diﬃcult to precisely estimate \\nits potential ecosystem-level or longitudinal risks and related political, social, and economic impacts. \\nGaps between benchmarks and real-world use of GAI systems may likely be exacerbated due to prompt \\nsensitivity and broad heterogeneity of contexts of use. \\nA.1.5. Structured Public Feedback \\nStructured public feedback can be used to evaluate whether GAI systems are performing as intended \\nand to calibrate and verify traditional measurement methods. Examples of structured feedback include, \\nbut are not limited to: \\n• \\nParticipatory Engagement Methods: Methods used to solicit feedback from civil society groups, \\naﬀected communities, and users, including focus groups, small user studies, and surveys. \\n• \\nField Testing: Methods used to determine how people interact with, consume, use, and make \\nsense of AI-generated information, and subsequent actions and eﬀects, including UX, usability, \\nand other structured, randomized experiments.  \\n• \\nAI Red-teaming: A structured testing exercise used to probe an AI system to ﬁnd ﬂaws and \\nvulnerabilities such as inaccurate, harmful, or discriminatory outputs, often in a controlled \\nenvironment and in collaboration with system developers. \\nInformation gathered from structured public feedback can inform design, implementation, deployment \\napproval, maintenance, or decommissioning decisions. Results and insights gleaned from these exercises \\ncan serve multiple purposes, including improving data quality and preprocessing, bolstering governance \\ndecision making, and enhancing system documentation and debugging practices. When implementing \\nfeedback activities, organizations should follow human subjects research requirements and best \\npractices such as informed consent and subject compensation.'), Document(metadata={'page': 32, 'modDate': \"D:20240805143048-04'00'\", 'creator': 'Acrobat PDFMaker 24 for Word', 'keywords': '', 'format': 'PDF 1.6', 'total_pages': 64, 'author': 'National Institute of Standards and Technology', 'producer': 'Adobe PDF Library 24.2.159', 'subject': '', 'creationDate': \"D:20240805141702-04'00'\", 'trapped': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', '_id': '8db2703c-4f91-4524-8702-80a7309bd6bb', '_collection_name': 'ai_policy'}, page_content='29 \\nMS-1.1-006 \\nImplement continuous monitoring of GAI system impacts to identify whether GAI \\noutputs are equitable across various sub-populations. Seek active and direct \\nfeedback from aﬀected communities via structured feedback mechanisms or red-\\nteaming to monitor and improve outputs.  \\nHarmful Bias and Homogenization \\nMS-1.1-007 \\nEvaluate the quality and integrity of data used in training and the provenance of \\nAI-generated content, for example by employing techniques like chaos \\nengineering and seeking stakeholder feedback. \\nInformation Integrity \\nMS-1.1-008 \\nDeﬁne use cases, contexts of use, capabilities, and negative impacts where \\nstructured human feedback exercises, e.g., GAI red-teaming, would be most \\nbeneﬁcial for GAI risk measurement and management based on the context of \\nuse. \\nHarmful Bias and \\nHomogenization; CBRN \\nInformation or Capabilities \\nMS-1.1-009 \\nTrack and document risks or opportunities related to all GAI risks that cannot be \\nmeasured quantitatively, including explanations as to why some risks cannot be \\nmeasured (e.g., due to technological limitations, resource constraints, or \\ntrustworthy considerations). Include unmeasured risks in marginal risks. \\nInformation Integrity \\nAI Actor Tasks: AI Development, Domain Experts, TEVV \\n \\nMEASURE 1.3: Internal experts who did not serve as front-line developers for the system and/or independent assessors are \\ninvolved in regular assessments and updates. Domain experts, users, AI Actors external to the team that developed or deployed the \\nAI system, and aﬀected communities are consulted in support of assessments as necessary per organizational risk tolerance. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-1.3-001 \\nDeﬁne relevant groups of interest (e.g., demographic groups, subject matter \\nexperts, experience with GAI technology) within the context of use as part of \\nplans for gathering structured public feedback. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization; CBRN \\nInformation or Capabilities \\nMS-1.3-002 \\nEngage in internal and external evaluations, GAI red-teaming, impact \\nassessments, or other structured human feedback exercises in consultation \\nwith representative AI Actors with expertise and familiarity in the context of \\nuse, and/or who are representative of the populations associated with the \\ncontext of use. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization; CBRN \\nInformation or Capabilities \\nMS-1.3-003 \\nVerify those conducting structured human feedback exercises are not directly \\ninvolved in system development tasks for the same GAI model. \\nHuman-AI Conﬁguration; Data \\nPrivacy \\nAI Actor Tasks: AI Deployment, AI Development, AI Impact Assessment, Aﬀected Individuals and Communities, Domain Experts, \\nEnd-Users, Operation and Monitoring, TEVV')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='What does NIST do to support secure AI with transparency, safety, and standards, including its role in the U.S. AI Safety Institute and AI Safety Institute Consortium?' id='a6858c77-361f-4440-9a42-9773b4e00a35'\n",
      "****Adding new context: [Document(metadata={'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'modDate': \"D:20240805143048-04'00'\", 'total_pages': 64, 'creator': 'Acrobat PDFMaker 24 for Word', 'format': 'PDF 1.6', 'producer': 'Adobe PDF Library 24.2.159', 'keywords': '', 'trapped': '', 'subject': '', 'creationDate': \"D:20240805141702-04'00'\", 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'author': 'National Institute of Standards and Technology', 'page': 2, '_id': '91bbe1b1-8123-4737-be3a-53e3571ba4d9', '_collection_name': 'ai_policy'}, page_content='About AI at NIST: The National Institute of Standards and Technology (NIST) develops measurements, \\ntechnology, tools, and standards to advance reliable, safe, transparent, explainable, privacy-enhanced, \\nand fair artiﬁcial intelligence (AI) so that its full commercial and societal beneﬁts can be realized without \\nharm to people or the planet. NIST, which has conducted both fundamental and applied work on AI for \\nmore than a decade, is also helping to fulﬁll the 2023 Executive Order on Safe, Secure, and Trustworthy \\nAI. NIST established the U.S. AI Safety Institute and the companion AI Safety Institute Consortium to \\ncontinue the eﬀorts set in motion by the E.O. to build the science necessary for safe, secure, and \\ntrustworthy development and use of AI. \\nAcknowledgments: This report was accomplished with the many helpful comments and contributions \\nfrom the community, including the NIST Generative AI Public Working Group, and NIST staﬀ and guest \\nresearchers: Chloe Autio, Jesse Dunietz, Patrick Hall, Shomik Jain, Kamie Roberts, Reva Schwartz, Martin \\nStanley, and Elham Tabassi. \\nNIST Technical Series Policies \\nCopyright, Use, and Licensing Statements \\nNIST Technical Series Publication Identifier Syntax \\nPublication History \\nApproved by the NIST Editorial Review Board on 07-25-2024 \\nContact Information \\nai-inquiries@nist.gov \\nNational Institute of Standards and Technology \\nAttn: NIST AI Innovation Lab, Information Technology Laboratory \\n100 Bureau Drive (Mail Stop 8900) Gaithersburg, MD 20899-8900 \\nAdditional Information \\nAdditional information about this publication and other NIST AI publications are available at \\nhttps://airc.nist.gov/Home. \\n \\nDisclaimer: Certain commercial entities, equipment, or materials may be identiﬁed in this document in \\norder to adequately describe an experimental procedure or concept. Such identiﬁcation is not intended to \\nimply recommendation or endorsement by the National Institute of Standards and Technology, nor is it \\nintended to imply that the entities, materials, or equipment are necessarily the best available for the \\npurpose. Any mention of commercial, non-proﬁt, academic partners, or their products, or references is \\nfor information only; it is not intended to imply endorsement or recommendation by any U.S. \\nGovernment agency.'), Document(metadata={'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'subject': '', 'format': 'PDF 1.6', 'trapped': '', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'producer': 'Adobe PDF Library 24.2.159', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'creationDate': \"D:20240805141702-04'00'\", 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'total_pages': 64, 'modDate': \"D:20240805143048-04'00'\", 'author': 'National Institute of Standards and Technology', 'page': 1, '_id': '39dd20d5-5c76-4b38-a3e0-e180657a7257', '_collection_name': 'ai_policy'}, page_content='NIST Trustworthy and Responsible AI  \\nNIST AI 600-1 \\nArtificial Intelligence Risk Management \\nFramework: Generative Artificial \\nIntelligence Profile \\n \\n \\n \\nThis publication is available free of charge from: \\nhttps://doi.org/10.6028/NIST.AI.600-1 \\n \\nJuly 2024 \\n \\n \\n \\n \\nU.S. Department of Commerce  \\nGina M. Raimondo, Secretary \\nNational Institute of Standards and Technology  \\nLaurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology'), Document(metadata={'modDate': \"D:20221003104118-04'00'\", 'page': 20, 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'creationDate': \"D:20220920133035-04'00'\", 'format': 'PDF 1.6', 'trapped': '', 'total_pages': 73, 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'keywords': '', 'subject': '', 'author': '', 'producer': 'iLovePDF', 'title': 'Blueprint for an AI Bill of Rights', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', '_id': 'b7f5bf9a-6ee2-4daf-9b7d-88dc45adeb9c', '_collection_name': 'ai_policy'}, page_content='SAFE AND EFFECTIVE \\nSYSTEMS \\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\\nReal-life examples of how these principles can become reality, through laws, policies, and practical \\ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \\xad\\xad\\nExecutive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the \\nFederal Government requires that certain federal agencies adhere to nine principles when \\ndesigning, developing, acquiring, or using AI for purposes other than national security or \\ndefense. These principles—while taking into account the sensitive law enforcement and other contexts in which \\nthe federal government may use AI, as opposed to private sector use of AI—require that AI is: (a) lawful and \\nrespectful of our Nation’s values; (b) purposeful and performance-driven; (c) accurate, reliable, and effective; (d) \\nsafe, secure, and resilient; (e) understandable; (f ) responsible and traceable; (g) regularly monitored; (h) transpar-\\nent; and, (i) accountable. The Blueprint for an AI Bill of Rights is consistent with the Executive Order. \\nAffected agencies across the federal government have released AI use case inventories13 and are implementing \\nplans to bring those AI systems into compliance with the Executive Order or retire them. \\nThe law and policy landscape for motor vehicles shows that strong safety regulations—and \\nmeasures to address harms when they occur—can enhance innovation in the context of com-\\nplex technologies. Cars, like automated digital systems, comprise a complex collection of components. \\nThe National Highway Traffic Safety Administration,14 through its rigorous standards and independent \\nevaluation, helps make sure vehicles on our roads are safe without limiting manufacturers’ ability to \\ninnovate.15 At the same time, rules of the road are implemented locally to impose contextually appropriate \\nrequirements on drivers, such as slowing down near schools or playgrounds.16\\nFrom large companies to start-ups, industry is providing innovative solutions that allow \\norganizations to mitigate risks to the safety and efficacy of AI systems, both before \\ndeployment and through monitoring over time.17 These innovative solutions include risk \\nassessments, auditing mechanisms, assessment of organizational procedures, dashboards to allow for ongoing \\nmonitoring, documentation procedures specific to model assessments, and many other strategies that aim to \\nmitigate risks posed by the use of AI to companies’ reputation, legal responsibilities, and other product safety \\nand effectiveness concerns. \\nThe Office of Management and Budget (OMB) has called for an expansion of opportunities \\nfor meaningful stakeholder engagement in the design of programs and services. OMB also \\npoints to numerous examples of effective and proactive stakeholder engagement, including the Community-\\nBased Participatory Research Program developed by the National Institutes of Health and the participatory \\ntechnology assessments developed by the National Oceanic and Atmospheric Administration.18\\nThe National Institute of Standards and Technology (NIST) is developing a risk \\nmanagement framework to better manage risks posed to individuals, organizations, and \\nsociety by AI.19 The NIST AI Risk Management Framework, as mandated by Congress, is intended for \\nvoluntary use to help incorporate trustworthiness considerations into the design, development, use, and \\nevaluation of AI products, services, and systems. The NIST framework is being developed through a consensus-\\ndriven, open, transparent, and collaborative process that includes workshops and other opportunities to provide \\ninput. The NIST framework aims to foster the development of innovative approaches to address \\ncharacteristics of trustworthiness including accuracy, explainability and interpretability, reliability, privacy, \\nrobustness, safety, security (resilience), and mitigation of unintended and/or harmful bias, as well as of \\nharmful \\nuses. \\nThe \\nNIST \\nframework \\nwill \\nconsider \\nand \\nencompass \\nprinciples \\nsuch \\nas \\ntransparency, accountability, and fairness during pre-design, design and development, deployment, use, \\nand testing and evaluation of AI technologies and systems. It is expected to be released in the winter of 2022-23. \\n21'), Document(metadata={'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'format': 'PDF 1.6', 'trapped': '', 'author': 'National Institute of Standards and Technology', 'modDate': \"D:20240805143048-04'00'\", 'total_pages': 64, 'creator': 'Acrobat PDFMaker 24 for Word', 'page': 0, 'creationDate': \"D:20240805141702-04'00'\", 'keywords': '', 'subject': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'producer': 'Adobe PDF Library 24.2.159', '_id': '350239a8-cd27-4ae5-ab64-b1d89582fe22', '_collection_name': 'ai_policy'}, page_content='NIST Trustworthy and Responsible AI  \\nNIST AI 600-1 \\nArtificial Intelligence Risk Management \\nFramework: Generative Artificial \\nIntelligence Profile \\n \\n \\n \\nThis publication is available free of charge from: \\nhttps://doi.org/10.6028/NIST.AI.600-1')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='How to handle incidents in GAI systems and inform stakeholders about remediation timelines?' id='a89469d8-67c3-4d19-a2e5-703246e607a3'\n",
      "****Adding new context: [Document(metadata={'page': 56, 'creationDate': \"D:20240805141702-04'00'\", 'total_pages': 64, 'modDate': \"D:20240805143048-04'00'\", 'subject': '', 'author': 'National Institute of Standards and Technology', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'trapped': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'keywords': '', 'producer': 'Adobe PDF Library 24.2.159', 'format': 'PDF 1.6', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', '_id': '6a3d4aa3-61a4-4333-a688-117aa051ad60', '_collection_name': 'ai_policy'}, page_content='53 \\nDocumenting, reporting, and sharing information about GAI incidents can help mitigate and prevent \\nharmful outcomes by assisting relevant AI Actors in tracing impacts to their source. Greater awareness \\nand standardization of GAI incident reporting could promote this transparency and improve GAI risk \\nmanagement across the AI ecosystem.  \\nDocumentation and Involvement of AI Actors \\nAI Actors should be aware of their roles in reporting AI incidents. To better understand previous incidents \\nand implement measures to prevent similar ones in the future, organizations could consider developing \\nguidelines for publicly available incident reporting which include information about AI actor \\nresponsibilities. These guidelines would help AI system operators identify GAI incidents across the AI \\nlifecycle and with AI Actors regardless of role. Documentation and review of third-party inputs and \\nplugins for GAI systems is especially important for AI Actors in the context of incident disclosure; LLM \\ninputs and content delivered through these plugins is often distributed, with inconsistent or insuﬃcient \\naccess control. \\nDocumentation practices including logging, recording, and analyzing GAI incidents can facilitate \\nsmoother sharing of information with relevant AI Actors. Regular information sharing, change \\nmanagement records, version history and metadata can also empower AI Actors responding to and \\nmanaging AI incidents.'), Document(metadata={'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'producer': 'Adobe PDF Library 24.2.159', 'format': 'PDF 1.6', 'modDate': \"D:20240805143048-04'00'\", 'keywords': '', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'trapped': '', 'total_pages': 64, 'creator': 'Acrobat PDFMaker 24 for Word', 'subject': '', 'page': 55, 'creationDate': \"D:20240805141702-04'00'\", 'author': 'National Institute of Standards and Technology', '_id': 'd37bb147-1a0e-43d1-8c2b-fb74014c5420', '_collection_name': 'ai_policy'}, page_content='52 \\n• \\nMonitoring system capabilities and limitations in deployment through rigorous TEVV processes; \\n• \\nEvaluating how humans engage, interact with, or adapt to GAI content (especially in decision \\nmaking tasks informed by GAI content), and how they react to applied provenance techniques \\nsuch as overt disclosures. \\nOrganizations can document and delineate GAI system objectives and limitations to identify gaps where \\nprovenance data may be most useful. For instance, GAI systems used for content creation may require \\nrobust watermarking techniques and corresponding detectors to identify the source of content or \\nmetadata recording techniques and metadata management tools and repositories to trace content \\norigins and modiﬁcations. Further narrowing of GAI task deﬁnitions to include provenance data can \\nenable organizations to maximize the utility of provenance data and risk management eﬀorts. \\nA.1.7. Enhancing Content Provenance through Structured Public Feedback \\nWhile indirect feedback methods such as automated error collection systems are useful, they often lack \\nthe context and depth that direct input from end users can provide. Organizations can leverage feedback \\napproaches described in the Pre-Deployment Testing section to capture input from external sources such \\nas through AI red-teaming.  \\nIntegrating pre- and post-deployment external feedback into the monitoring process for GAI models and \\ncorresponding applications can help enhance awareness of performance changes and mitigate potential \\nrisks and harms from outputs. There are many ways to capture and make use of user feedback – before \\nand after GAI systems and digital content transparency approaches are deployed – to gain insights about \\nauthentication eﬃcacy and vulnerabilities, impacts of adversarial threats on techniques, and unintended \\nconsequences resulting from the utilization of content provenance approaches on users and \\ncommunities. Furthermore, organizations can track and document the provenance of datasets to identify \\ninstances in which AI-generated data is a potential root cause of performance issues with the GAI \\nsystem. \\nA.1.8. Incident Disclosure \\nOverview \\nAI incidents can be deﬁned as an “event, circumstance, or series of events where the development, use, \\nor malfunction of one or more AI systems directly or indirectly contributes to one of the following harms: \\ninjury or harm to the health of a person or groups of people (including psychological harms and harms to \\nmental health); disruption of the management and operation of critical infrastructure; violations of \\nhuman rights or a breach of obligations under applicable law intended to protect fundamental, labor, \\nand intellectual property rights; or harm to property, communities, or the environment.” AI incidents can \\noccur in the aggregate (i.e., for systemic discrimination) or acutely (i.e., for one individual). \\nState of AI Incident Tracking and Disclosure \\nFormal channels do not currently exist to report and document AI incidents. However, a number of \\npublicly available databases have been created to document their occurrence. These reporting channels \\nmake decisions on an ad hoc basis about what kinds of incidents to track. Some, for example, track by \\namount of media coverage.'), Document(metadata={'author': 'National Institute of Standards and Technology', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'format': 'PDF 1.6', 'page': 45, 'trapped': '', 'modDate': \"D:20240805143048-04'00'\", 'keywords': '', 'subject': '', 'creationDate': \"D:20240805141702-04'00'\", 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'creator': 'Acrobat PDFMaker 24 for Word', 'total_pages': 64, 'producer': 'Adobe PDF Library 24.2.159', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', '_id': '9b32a621-91ef-4e72-8581-3c50bd1f606b', '_collection_name': 'ai_policy'}, page_content='42 \\nMG-2.4-002 \\nEstablish and maintain procedures for escalating GAI system incidents to the \\norganizational risk management authority when speciﬁc criteria for deactivation \\nor disengagement is met for a particular context of use or for the GAI system as a \\nwhole. \\nInformation Security \\nMG-2.4-003 \\nEstablish and maintain procedures for the remediation of issues which trigger \\nincident response processes for the use of a GAI system, and provide stakeholders \\ntimelines associated with the remediation plan. \\nInformation Security \\n \\nMG-2.4-004 Establish and regularly review speciﬁc criteria that warrants the deactivation of \\nGAI systems in accordance with set risk tolerances and appetites. \\nInformation Security \\n \\nAI Actor Tasks: AI Deployment, Governance and Oversight, Operation and Monitoring \\n \\nMANAGE 3.1: AI risks and beneﬁts from third-party resources are regularly monitored, and risk controls are applied and \\ndocumented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMG-3.1-001 \\nApply organizational risk tolerances and controls (e.g., acquisition and \\nprocurement processes; assessing personnel credentials and qualiﬁcations, \\nperforming background checks; ﬁltering GAI input and outputs, grounding, ﬁne \\ntuning, retrieval-augmented generation) to third-party GAI resources: Apply \\norganizational risk tolerance to the utilization of third-party datasets and other \\nGAI resources; Apply organizational risk tolerances to ﬁne-tuned third-party \\nmodels; Apply organizational risk tolerance to existing third-party models \\nadapted to a new domain; Reassess risk measurements after ﬁne-tuning third-\\nparty GAI models. \\nValue Chain and Component \\nIntegration; Intellectual Property \\nMG-3.1-002 \\nTest GAI system value chain risks (e.g., data poisoning, malware, other software \\nand hardware vulnerabilities; labor practices; data privacy and localization \\ncompliance; geopolitical alignment). \\nData Privacy; Information Security; \\nValue Chain and Component \\nIntegration; Harmful Bias and \\nHomogenization \\nMG-3.1-003 \\nRe-assess model risks after ﬁne-tuning or retrieval-augmented generation \\nimplementation and for any third-party GAI models deployed for applications \\nand/or use cases that were not evaluated in initial testing. \\nValue Chain and Component \\nIntegration \\nMG-3.1-004 \\nTake reasonable measures to review training data for CBRN information, and \\nintellectual property, and where appropriate, remove it. Implement reasonable \\nmeasures to prevent, ﬂag, or take other action in response to outputs that \\nreproduce particular training data (e.g., plagiarized, trademarked, patented, \\nlicensed content or trade secret material). \\nIntellectual Property; CBRN \\nInformation or Capabilities'), Document(metadata={'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'creator': 'Acrobat PDFMaker 24 for Word', 'page': 48, 'creationDate': \"D:20240805141702-04'00'\", 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'subject': '', 'producer': 'Adobe PDF Library 24.2.159', 'author': 'National Institute of Standards and Technology', 'format': 'PDF 1.6', 'modDate': \"D:20240805143048-04'00'\", 'total_pages': 64, 'trapped': '', 'keywords': '', '_id': 'af1248b5-75c1-4e8f-af65-43484e197c57', '_collection_name': 'ai_policy'}, page_content='45 \\nMG-4.1-007 \\nVerify that AI Actors responsible for monitoring reported issues can eﬀectively \\nevaluate GAI system performance including the application of content \\nprovenance data tracking techniques, and promptly escalate issues for response. \\nHuman-AI Conﬁguration; \\nInformation Integrity \\nAI Actor Tasks: AI Deployment, Aﬀected Individuals and Communities, Domain Experts, End-Users, Human Factors, Operation and \\nMonitoring \\n \\nMANAGE 4.2: Measurable activities for continual improvements are integrated into AI system updates and include regular \\nengagement with interested parties, including relevant AI Actors. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMG-4.2-001 Conduct regular monitoring of GAI systems and publish reports detailing the \\nperformance, feedback received, and improvements made. \\nHarmful Bias and Homogenization \\nMG-4.2-002 \\nPractice and follow incident response plans for addressing the generation of \\ninappropriate or harmful content and adapt processes based on ﬁndings to \\nprevent future occurrences. Conduct post-mortem analyses of incidents with \\nrelevant AI Actors, to understand the root causes and implement preventive \\nmeasures. \\nHuman-AI Conﬁguration; \\nDangerous, Violent, or Hateful \\nContent \\nMG-4.2-003 Use visualizations or other methods to represent GAI model behavior to ease \\nnon-technical stakeholders understanding of GAI system functionality. \\nHuman-AI Conﬁguration \\nAI Actor Tasks: AI Deployment, AI Design, AI Development, Aﬀected Individuals and Communities, End-Users, Operation and \\nMonitoring, TEVV \\n \\nMANAGE 4.3: Incidents and errors are communicated to relevant AI Actors, including aﬀected communities. Processes for tracking, \\nresponding to, and recovering from incidents and errors are followed and documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMG-4.3-001 \\nConduct after-action assessments for GAI system incidents to verify incident \\nresponse and recovery processes are followed and eﬀective, including to follow \\nprocedures for communicating incidents to relevant AI Actors and where \\napplicable, relevant legal and regulatory bodies.  \\nInformation Security \\nMG-4.3-002 Establish and maintain policies and procedures to record and track GAI system \\nreported errors, near-misses, and negative impacts. \\nConfabulation; Information \\nIntegrity')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='How does the risk management process for GAI systems ensure information integrity and safe decommissioning?' id='14d95322-cda7-486b-a97e-bb5ca77e7021'\n",
      "****Adding new context: [Document(metadata={'author': 'National Institute of Standards and Technology', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'format': 'PDF 1.6', 'page': 45, 'trapped': '', 'modDate': \"D:20240805143048-04'00'\", 'keywords': '', 'subject': '', 'creationDate': \"D:20240805141702-04'00'\", 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'creator': 'Acrobat PDFMaker 24 for Word', 'total_pages': 64, 'producer': 'Adobe PDF Library 24.2.159', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', '_id': '9b32a621-91ef-4e72-8581-3c50bd1f606b', '_collection_name': 'ai_policy'}, page_content='42 \\nMG-2.4-002 \\nEstablish and maintain procedures for escalating GAI system incidents to the \\norganizational risk management authority when speciﬁc criteria for deactivation \\nor disengagement is met for a particular context of use or for the GAI system as a \\nwhole. \\nInformation Security \\nMG-2.4-003 \\nEstablish and maintain procedures for the remediation of issues which trigger \\nincident response processes for the use of a GAI system, and provide stakeholders \\ntimelines associated with the remediation plan. \\nInformation Security \\n \\nMG-2.4-004 Establish and regularly review speciﬁc criteria that warrants the deactivation of \\nGAI systems in accordance with set risk tolerances and appetites. \\nInformation Security \\n \\nAI Actor Tasks: AI Deployment, Governance and Oversight, Operation and Monitoring \\n \\nMANAGE 3.1: AI risks and beneﬁts from third-party resources are regularly monitored, and risk controls are applied and \\ndocumented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMG-3.1-001 \\nApply organizational risk tolerances and controls (e.g., acquisition and \\nprocurement processes; assessing personnel credentials and qualiﬁcations, \\nperforming background checks; ﬁltering GAI input and outputs, grounding, ﬁne \\ntuning, retrieval-augmented generation) to third-party GAI resources: Apply \\norganizational risk tolerance to the utilization of third-party datasets and other \\nGAI resources; Apply organizational risk tolerances to ﬁne-tuned third-party \\nmodels; Apply organizational risk tolerance to existing third-party models \\nadapted to a new domain; Reassess risk measurements after ﬁne-tuning third-\\nparty GAI models. \\nValue Chain and Component \\nIntegration; Intellectual Property \\nMG-3.1-002 \\nTest GAI system value chain risks (e.g., data poisoning, malware, other software \\nand hardware vulnerabilities; labor practices; data privacy and localization \\ncompliance; geopolitical alignment). \\nData Privacy; Information Security; \\nValue Chain and Component \\nIntegration; Harmful Bias and \\nHomogenization \\nMG-3.1-003 \\nRe-assess model risks after ﬁne-tuning or retrieval-augmented generation \\nimplementation and for any third-party GAI models deployed for applications \\nand/or use cases that were not evaluated in initial testing. \\nValue Chain and Component \\nIntegration \\nMG-3.1-004 \\nTake reasonable measures to review training data for CBRN information, and \\nintellectual property, and where appropriate, remove it. Implement reasonable \\nmeasures to prevent, ﬂag, or take other action in response to outputs that \\nreproduce particular training data (e.g., plagiarized, trademarked, patented, \\nlicensed content or trade secret material). \\nIntellectual Property; CBRN \\nInformation or Capabilities'), Document(metadata={'creator': 'Acrobat PDFMaker 24 for Word', 'author': 'National Institute of Standards and Technology', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'modDate': \"D:20240805143048-04'00'\", 'total_pages': 64, 'keywords': '', 'page': 20, 'subject': '', 'trapped': '', 'producer': 'Adobe PDF Library 24.2.159', 'format': 'PDF 1.6', 'creationDate': \"D:20240805141702-04'00'\", 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', '_id': '86348f22-3012-4da1-aa63-59a2b3fa3b86', '_collection_name': 'ai_policy'}, page_content='17 \\nGOVERN 1.7: Processes and procedures are in place for decommissioning and phasing out AI systems safely and in a manner that \\ndoes not increase risks or decrease the organization’s trustworthiness. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-1.7-001 Protocols are put in place to ensure GAI systems are able to be deactivated when \\nnecessary.  \\nInformation Security; Value Chain \\nand Component Integration \\nGV-1.7-002 \\nConsider the following factors when decommissioning GAI systems: Data \\nretention requirements; Data security, e.g., containment, protocols, Data leakage \\nafter decommissioning; Dependencies between upstream, downstream, or other \\ndata, internet of things (IOT) or AI systems; Use of open-source data or models; \\nUsers’ emotional entanglement with GAI functions. \\nHuman-AI Conﬁguration; \\nInformation Security; Value Chain \\nand Component Integration \\nAI Actor Tasks: AI Deployment, Operation and Monitoring \\n \\nGOVERN 2.1: Roles and responsibilities and lines of communication related to mapping, measuring, and managing AI risks are \\ndocumented and are clear to individuals and teams throughout the organization. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-2.1-001 \\nEstablish organizational roles, policies, and procedures for communicating GAI \\nincidents and performance to AI Actors and downstream stakeholders (including \\nthose potentially impacted), via community or oﬃcial resources (e.g., AI incident \\ndatabase, AVID, CVE, NVD, or OECD AI incident monitor). \\nHuman-AI Conﬁguration; Value \\nChain and Component Integration \\nGV-2.1-002 Establish procedures to engage teams for GAI system incident response with \\ndiverse composition and responsibilities based on the particular incident type. \\nHarmful Bias and Homogenization \\nGV-2.1-003 Establish processes to verify the AI Actors conducting GAI incident response tasks \\ndemonstrate and maintain the appropriate skills and training. \\nHuman-AI Conﬁguration \\nGV-2.1-004 When systems may raise national security risks, involve national security \\nprofessionals in mapping, measuring, and managing those risks. \\nCBRN Information or Capabilities; \\nDangerous, Violent, or Hateful \\nContent; Information Security \\nGV-2.1-005 \\nCreate mechanisms to provide protections for whistleblowers who report, based \\non reasonable belief, when the organization violates relevant laws or poses a \\nspeciﬁc and empirically well-substantiated negative risk to public safety (or has \\nalready caused harm). \\nCBRN Information or Capabilities; \\nDangerous, Violent, or Hateful \\nContent \\nAI Actor Tasks: Governance and Oversight'), Document(metadata={'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'producer': 'Adobe PDF Library 24.2.159', 'page': 19, 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'author': 'National Institute of Standards and Technology', 'creationDate': \"D:20240805141702-04'00'\", 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'keywords': '', 'total_pages': 64, 'creator': 'Acrobat PDFMaker 24 for Word', 'subject': '', 'format': 'PDF 1.6', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', '_id': 'be50a57c-66e4-4b18-8f33-49c769fb7add', '_collection_name': 'ai_policy'}, page_content='16 \\nGOVERN 1.5: Ongoing monitoring and periodic review of the risk management process and its outcomes are planned, and \\norganizational roles and responsibilities are clearly deﬁned, including determining the frequency of periodic review. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-1.5-001 Deﬁne organizational responsibilities for periodic review of content provenance \\nand incident monitoring for GAI systems. \\nInformation Integrity \\nGV-1.5-002 \\nEstablish organizational policies and procedures for after action reviews of GAI \\nsystem incident response and incident disclosures, to identify gaps; Update \\nincident response and incident disclosure processes as required. \\nHuman-AI Conﬁguration; \\nInformation Security \\nGV-1.5-003 \\nMaintain a document retention policy to keep history for test, evaluation, \\nvalidation, and veriﬁcation (TEVV), and digital content transparency methods for \\nGAI. \\nInformation Integrity; Intellectual \\nProperty \\nAI Actor Tasks: Governance and Oversight, Operation and Monitoring \\n \\nGOVERN 1.6: Mechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-1.6-001 Enumerate organizational GAI systems for incorporation into AI system inventory \\nand adjust AI system inventory requirements to account for GAI risks. \\nInformation Security \\nGV-1.6-002 Deﬁne any inventory exemptions in organizational policies for GAI systems \\nembedded into application software. \\nValue Chain and Component \\nIntegration \\nGV-1.6-003 \\nIn addition to general model, governance, and risk information, consider the \\nfollowing items in GAI system inventory entries: Data provenance information \\n(e.g., source, signatures, versioning, watermarks); Known issues reported from \\ninternal bug tracking or external information sharing resources (e.g., AI incident \\ndatabase, AVID, CVE, NVD, or OECD AI incident monitor); Human oversight roles \\nand responsibilities; Special rights and considerations for intellectual property, \\nlicensed works, or personal, privileged, proprietary or sensitive data; Underlying \\nfoundation models, versions of underlying models, and access modes. \\nData Privacy; Human-AI \\nConﬁguration; Information \\nIntegrity; Intellectual Property; \\nValue Chain and Component \\nIntegration \\nAI Actor Tasks: Governance and Oversight'), Document(metadata={'creationDate': \"D:20240805141702-04'00'\", 'producer': 'Adobe PDF Library 24.2.159', 'format': 'PDF 1.6', 'subject': '', 'trapped': '', 'modDate': \"D:20240805143048-04'00'\", 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'page': 17, 'creator': 'Acrobat PDFMaker 24 for Word', 'author': 'National Institute of Standards and Technology', 'keywords': '', 'total_pages': 64, 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', '_id': '5c323fb4-b3bf-4985-ad60-13f69e5e1816', '_collection_name': 'ai_policy'}, page_content='14 \\nGOVERN 1.2: The characteristics of trustworthy AI are integrated into organizational policies, processes, procedures, and practices. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-1.2-001 \\nEstablish transparency policies and processes for documenting the origin and \\nhistory of training data and generated data for GAI applications to advance digital \\ncontent transparency, while balancing the proprietary nature of training \\napproaches. \\nData Privacy; Information \\nIntegrity; Intellectual Property \\nGV-1.2-002 \\nEstablish policies to evaluate risk-relevant capabilities of GAI and robustness of \\nsafety measures, both prior to deployment and on an ongoing basis, through \\ninternal and external evaluations. \\nCBRN Information or Capabilities; \\nInformation Security \\nAI Actor Tasks: Governance and Oversight \\n \\nGOVERN 1.3: Processes, procedures, and practices are in place to determine the needed level of risk management activities based \\non the organization’s risk tolerance. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-1.3-001 \\nConsider the following factors when updating or deﬁning risk tiers for GAI: Abuses \\nand impacts to information integrity; Dependencies between GAI and other IT or \\ndata systems; Harm to fundamental rights or public safety; Presentation of \\nobscene, objectionable, oﬀensive, discriminatory, invalid or untruthful output; \\nPsychological impacts to humans (e.g., anthropomorphization, algorithmic \\naversion, emotional entanglement); Possibility for malicious use; Whether the \\nsystem introduces signiﬁcant new security vulnerabilities; Anticipated system \\nimpact on some groups compared to others; Unreliable decision making \\ncapabilities, validity, adaptability, and variability of GAI system performance over \\ntime. \\nInformation Integrity; Obscene, \\nDegrading, and/or Abusive \\nContent; Value Chain and \\nComponent Integration; Harmful \\nBias and Homogenization; \\nDangerous, Violent, or Hateful \\nContent; CBRN Information or \\nCapabilities \\nGV-1.3-002 \\nEstablish minimum thresholds for performance or assurance criteria and review as \\npart of deployment approval (“go/”no-go”) policies, procedures, and processes, \\nwith reviewed processes and approval thresholds reﬂecting measurement of GAI \\ncapabilities and risks. \\nCBRN Information or Capabilities; \\nConfabulation; Dangerous, \\nViolent, or Hateful Content \\nGV-1.3-003 \\nEstablish a test plan and response policy, before developing highly capable models, \\nto periodically evaluate whether the model may misuse CBRN information or \\ncapabilities and/or oﬀensive cyber capabilities. \\nCBRN Information or Capabilities; \\nInformation Security')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='Why is regular adversarial testing important for GAI systems to manage risks and monitor impacts on different sub-populations?' id='c39a3f3f-02fd-4f46-99c5-582479c57bc7'\n",
      "****Adding new context: [Document(metadata={'subject': '', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'producer': 'Adobe PDF Library 24.2.159', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'keywords': '', 'total_pages': 64, 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'author': 'National Institute of Standards and Technology', 'creator': 'Acrobat PDFMaker 24 for Word', 'page': 51, 'format': 'PDF 1.6', 'trapped': '', '_id': 'c1fe7d74-5109-4641-b0c6-8cbafb2f1f68', '_collection_name': 'ai_policy'}, page_content='48 \\n• Data protection \\n• Data retention  \\n• Consistency in use of deﬁning key terms \\n• Decommissioning \\n• Discouraging anonymous use \\n• Education  \\n• Impact assessments  \\n• Incident response \\n• Monitoring \\n• Opt-outs  \\n• Risk-based controls \\n• Risk mapping and measurement \\n• Science-backed TEVV practices \\n• Secure software development practices \\n• Stakeholder engagement \\n• Synthetic content detection and \\nlabeling tools and techniques \\n• Whistleblower protections \\n• Workforce diversity and \\ninterdisciplinary teams\\nEstablishing acceptable use policies and guidance for the use of GAI in formal human-AI teaming settings \\nas well as diﬀerent levels of human-AI conﬁgurations can help to decrease risks arising from misuse, \\nabuse, inappropriate repurpose, and misalignment between systems and users. These practices are just \\none example of adapting existing governance protocols for GAI contexts.  \\nA.1.3. Third-Party Considerations \\nOrganizations may seek to acquire, embed, incorporate, or use open-source or proprietary third-party \\nGAI models, systems, or generated data for various applications across an enterprise. Use of these GAI \\ntools and inputs has implications for all functions of the organization – including but not limited to \\nacquisition, human resources, legal, compliance, and IT services – regardless of whether they are carried \\nout by employees or third parties. Many of the actions cited above are relevant and options for \\naddressing third-party considerations. \\nThird party GAI integrations may give rise to increased intellectual property, data privacy, or information \\nsecurity risks, pointing to the need for clear guidelines for transparency and risk management regarding \\nthe collection and use of third-party data for model inputs. Organizations may consider varying risk \\ncontrols for foundation models, ﬁne-tuned models, and embedded tools, enhanced processes for \\ninteracting with external GAI technologies or service providers. Organizations can apply standard or \\nexisting risk controls and processes to proprietary or open-source GAI technologies, data, and third-party \\nservice providers, including acquisition and procurement due diligence, requests for software bills of \\nmaterials (SBOMs), application of service level agreements (SLAs), and statement on standards for \\nattestation engagement (SSAE) reports to help with third-party transparency and risk management for \\nGAI systems. \\nA.1.4. Pre-Deployment Testing \\nOverview \\nThe diverse ways and contexts in which GAI systems may be developed, used, and repurposed \\ncomplicates risk mapping and pre-deployment measurement eﬀorts. Robust test, evaluation, validation, \\nand veriﬁcation (TEVV) processes can be iteratively applied – and documented – in early stages of the AI \\nlifecycle and informed by representative AI Actors (see Figure 3 of the AI RMF). Until new and rigorous'), Document(metadata={'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'producer': 'Adobe PDF Library 24.2.159', 'page': 50, 'creationDate': \"D:20240805141702-04'00'\", 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'trapped': '', 'author': 'National Institute of Standards and Technology', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'format': 'PDF 1.6', 'keywords': '', 'total_pages': 64, 'subject': '', 'modDate': \"D:20240805143048-04'00'\", 'creator': 'Acrobat PDFMaker 24 for Word', '_id': '21e8c6b5-98cf-4396-9030-866c59e85f9e', '_collection_name': 'ai_policy'}, page_content='47 \\nAppendix A. Primary GAI Considerations \\nThe following primary considerations were derived as overarching themes from the GAI PWG \\nconsultation process. These considerations (Governance, Pre-Deployment Testing, Content Provenance, \\nand Incident Disclosure) are relevant for voluntary use by any organization designing, developing, and \\nusing GAI and also inform the Actions to Manage GAI risks. Information included about the primary \\nconsiderations is not exhaustive, but highlights the most relevant topics derived from the GAI PWG.  \\nAcknowledgments: These considerations could not have been surfaced without the helpful analysis and \\ncontributions from the community and NIST staﬀ GAI PWG leads: George Awad, Luca Belli, Harold Booth, \\nMat Heyman, Yooyoung Lee, Mark Pryzbocki, Reva Schwartz, Martin Stanley, and Kyra Yee. \\nA.1. Governance \\nA.1.1. Overview \\nLike any other technology system, governance principles and techniques can be used to manage risks \\nrelated to generative AI models, capabilities, and applications. Organizations may choose to apply their \\nexisting risk tiering to GAI systems, or they may opt to revise or update AI system risk levels to address \\nthese unique GAI risks. This section describes how organizational governance regimes may be re-\\nevaluated and adjusted for GAI contexts. It also addresses third-party considerations for governing across \\nthe AI value chain.  \\nA.1.2. Organizational Governance \\nGAI opportunities, risks and long-term performance characteristics are typically less well-understood \\nthan non-generative AI tools and may be perceived and acted upon by humans in ways that vary greatly. \\nAccordingly, GAI may call for diﬀerent levels of oversight from AI Actors or diﬀerent human-AI \\nconﬁgurations in order to manage their risks eﬀectively. Organizations’ use of GAI systems may also \\nwarrant additional human review, tracking and documentation, and greater management oversight.  \\nAI technology can produce varied outputs in multiple modalities and present many classes of user \\ninterfaces. This leads to a broader set of AI Actors interacting with GAI systems for widely diﬀering \\napplications and contexts of use. These can include data labeling and preparation, development of GAI \\nmodels, content moderation, code generation and review, text generation and editing, image and video \\ngeneration, summarization, search, and chat. These activities can take place within organizational \\nsettings or in the public domain. \\nOrganizations can restrict AI applications that cause harm, exceed stated risk tolerances, or that conﬂict \\nwith their tolerances or values. Governance tools and protocols that are applied to other types of AI \\nsystems can be applied to GAI systems. These plans and actions include: \\n• Accessibility and reasonable \\naccommodations \\n• AI actor credentials and qualiﬁcations  \\n• Alignment to organizational values \\n• Auditing and assessment \\n• Change-management controls \\n• Commercial use \\n• Data provenance'), Document(metadata={'page': 32, 'modDate': \"D:20240805143048-04'00'\", 'creator': 'Acrobat PDFMaker 24 for Word', 'keywords': '', 'format': 'PDF 1.6', 'total_pages': 64, 'author': 'National Institute of Standards and Technology', 'producer': 'Adobe PDF Library 24.2.159', 'subject': '', 'creationDate': \"D:20240805141702-04'00'\", 'trapped': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', '_id': '8db2703c-4f91-4524-8702-80a7309bd6bb', '_collection_name': 'ai_policy'}, page_content='29 \\nMS-1.1-006 \\nImplement continuous monitoring of GAI system impacts to identify whether GAI \\noutputs are equitable across various sub-populations. Seek active and direct \\nfeedback from aﬀected communities via structured feedback mechanisms or red-\\nteaming to monitor and improve outputs.  \\nHarmful Bias and Homogenization \\nMS-1.1-007 \\nEvaluate the quality and integrity of data used in training and the provenance of \\nAI-generated content, for example by employing techniques like chaos \\nengineering and seeking stakeholder feedback. \\nInformation Integrity \\nMS-1.1-008 \\nDeﬁne use cases, contexts of use, capabilities, and negative impacts where \\nstructured human feedback exercises, e.g., GAI red-teaming, would be most \\nbeneﬁcial for GAI risk measurement and management based on the context of \\nuse. \\nHarmful Bias and \\nHomogenization; CBRN \\nInformation or Capabilities \\nMS-1.1-009 \\nTrack and document risks or opportunities related to all GAI risks that cannot be \\nmeasured quantitatively, including explanations as to why some risks cannot be \\nmeasured (e.g., due to technological limitations, resource constraints, or \\ntrustworthy considerations). Include unmeasured risks in marginal risks. \\nInformation Integrity \\nAI Actor Tasks: AI Development, Domain Experts, TEVV \\n \\nMEASURE 1.3: Internal experts who did not serve as front-line developers for the system and/or independent assessors are \\ninvolved in regular assessments and updates. Domain experts, users, AI Actors external to the team that developed or deployed the \\nAI system, and aﬀected communities are consulted in support of assessments as necessary per organizational risk tolerance. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-1.3-001 \\nDeﬁne relevant groups of interest (e.g., demographic groups, subject matter \\nexperts, experience with GAI technology) within the context of use as part of \\nplans for gathering structured public feedback. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization; CBRN \\nInformation or Capabilities \\nMS-1.3-002 \\nEngage in internal and external evaluations, GAI red-teaming, impact \\nassessments, or other structured human feedback exercises in consultation \\nwith representative AI Actors with expertise and familiarity in the context of \\nuse, and/or who are representative of the populations associated with the \\ncontext of use. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization; CBRN \\nInformation or Capabilities \\nMS-1.3-003 \\nVerify those conducting structured human feedback exercises are not directly \\ninvolved in system development tasks for the same GAI model. \\nHuman-AI Conﬁguration; Data \\nPrivacy \\nAI Actor Tasks: AI Deployment, AI Development, AI Impact Assessment, Aﬀected Individuals and Communities, Domain Experts, \\nEnd-Users, Operation and Monitoring, TEVV'), Document(metadata={'subject': '', 'author': 'National Institute of Standards and Technology', 'producer': 'Adobe PDF Library 24.2.159', 'page': 52, 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'format': 'PDF 1.6', 'modDate': \"D:20240805143048-04'00'\", 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'trapped': '', 'keywords': '', 'total_pages': 64, 'creationDate': \"D:20240805141702-04'00'\", 'creator': 'Acrobat PDFMaker 24 for Word', '_id': 'f923229c-1cd8-4b6c-8d03-5152882532e6', '_collection_name': 'ai_policy'}, page_content='49 \\nearly lifecycle TEVV approaches are developed and matured for GAI, organizations may use \\nrecommended “pre-deployment testing” practices to measure performance, capabilities, limits, risks, \\nand impacts. This section describes risk measurement and estimation as part of pre-deployment TEVV, \\nand examines the state of play for pre-deployment testing methodologies.  \\nLimitations of Current Pre-deployment Test Approaches \\nCurrently available pre-deployment TEVV processes used for GAI applications may be inadequate, non-\\nsystematically applied, or fail to reﬂect or mismatched to deployment contexts. For example, the \\nanecdotal testing of GAI system capabilities through video games or standardized tests designed for \\nhumans (e.g., intelligence tests, professional licensing exams) does not guarantee GAI system validity or \\nreliability in those domains. Similarly, jailbreaking or prompt engineering tests may not systematically \\nassess validity or reliability risks.  \\nMeasurement gaps can arise from mismatches between laboratory and real-world settings. Current \\ntesting approaches often remain focused on laboratory conditions or restricted to benchmark test \\ndatasets and in silico techniques that may not extrapolate well to—or directly assess GAI impacts in real-\\nworld conditions. For example, current measurement gaps for GAI make it diﬃcult to precisely estimate \\nits potential ecosystem-level or longitudinal risks and related political, social, and economic impacts. \\nGaps between benchmarks and real-world use of GAI systems may likely be exacerbated due to prompt \\nsensitivity and broad heterogeneity of contexts of use. \\nA.1.5. Structured Public Feedback \\nStructured public feedback can be used to evaluate whether GAI systems are performing as intended \\nand to calibrate and verify traditional measurement methods. Examples of structured feedback include, \\nbut are not limited to: \\n• \\nParticipatory Engagement Methods: Methods used to solicit feedback from civil society groups, \\naﬀected communities, and users, including focus groups, small user studies, and surveys. \\n• \\nField Testing: Methods used to determine how people interact with, consume, use, and make \\nsense of AI-generated information, and subsequent actions and eﬀects, including UX, usability, \\nand other structured, randomized experiments.  \\n• \\nAI Red-teaming: A structured testing exercise used to probe an AI system to ﬁnd ﬂaws and \\nvulnerabilities such as inaccurate, harmful, or discriminatory outputs, often in a controlled \\nenvironment and in collaboration with system developers. \\nInformation gathered from structured public feedback can inform design, implementation, deployment \\napproval, maintenance, or decommissioning decisions. Results and insights gleaned from these exercises \\ncan serve multiple purposes, including improving data quality and preprocessing, bolstering governance \\ndecision making, and enhancing system documentation and debugging practices. When implementing \\nfeedback activities, organizations should follow human subjects research requirements and best \\npractices such as informed consent and subject compensation.')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='How can organizations address bias and homogenization in GAI systems while ensuring transparency and safety?' id='c7fecd8b-6556-4522-9d33-a274e4c836ee'\n",
      "****Adding new context: [Document(metadata={'page': 11, 'trapped': '', 'keywords': '', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'format': 'PDF 1.6', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'total_pages': 64, 'author': 'National Institute of Standards and Technology', 'producer': 'Adobe PDF Library 24.2.159', 'modDate': \"D:20240805143048-04'00'\", 'creator': 'Acrobat PDFMaker 24 for Word', 'subject': '', 'creationDate': \"D:20240805141702-04'00'\", '_id': '39629e60-af43-4d74-ab0d-935e14d0b62c', '_collection_name': 'ai_policy'}, page_content='8 \\nTrustworthy AI Characteristics: Accountable and Transparent, Privacy Enhanced, Safe, Secure and \\nResilient \\n2.5. Environmental Impacts \\nTraining, maintaining, and operating (running inference on) GAI systems are resource-intensive activities, \\nwith potentially large energy and environmental footprints. Energy and carbon emissions vary based on \\nwhat is being done with the GAI model (i.e., pre-training, ﬁne-tuning, inference), the modality of the \\ncontent, hardware used, and type of task or application. \\nCurrent estimates suggest that training a single transformer LLM can emit as much carbon as 300 round-\\ntrip ﬂights between San Francisco and New York. In a study comparing energy consumption and carbon \\nemissions for LLM inference, generative tasks (e.g., text summarization) were found to be more energy- \\nand carbon-intensive than discriminative or non-generative tasks (e.g., text classiﬁcation).  \\nMethods for creating smaller versions of trained models, such as model distillation or compression, \\ncould reduce environmental impacts at inference time, but training and tuning such models may still \\ncontribute to their environmental impacts. Currently there is no agreed upon method to estimate \\nenvironmental impacts from GAI.  \\nTrustworthy AI Characteristics: Accountable and Transparent, Safe \\n2.6. Harmful Bias and Homogenization \\nBias exists in many forms and can become ingrained in automated systems. AI systems, including GAI \\nsystems, can increase the speed and scale at which harmful biases manifest and are acted upon, \\npotentially perpetuating and amplifying harms to individuals, groups, communities, organizations, and \\nsociety. For example, when prompted to generate images of CEOs, doctors, lawyers, and judges, current \\ntext-to-image models underrepresent women and/or racial minorities, and people with disabilities. \\nImage generator models have also produced biased or stereotyped output for various demographic \\ngroups and have diﬃculty producing non-stereotyped content even when the prompt speciﬁcally \\nrequests image features that are inconsistent with the stereotypes. Harmful bias in GAI models, which \\nmay stem from their training data, can also cause representational harms or perpetuate or exacerbate \\nbias based on race, gender, disability, or other protected classes.  \\nHarmful bias in GAI systems can also lead to harms via disparities between how a model performs for \\ndiﬀerent subgroups or languages (e.g., an LLM may perform less well for non-English languages or \\ncertain dialects). Such disparities can contribute to discriminatory decision-making or ampliﬁcation of \\nexisting societal biases. In addition, GAI systems may be inappropriately trusted to perform similarly \\nacross all subgroups, which could leave the groups facing underperformance with worse outcomes than \\nif no GAI system were used. Disparate or reduced performance for lower-resource languages also \\npresents challenges to model adoption, inclusion, and accessibility, and may make preservation of \\nendangered languages more diﬃcult if GAI systems become embedded in everyday processes that would \\notherwise have been opportunities to use these languages.  \\nBias is mutually reinforcing with the problem of undesired homogenization, in which GAI systems \\nproduce skewed distributions of outputs that are overly uniform (for example, repetitive aesthetic styles'), Document(metadata={'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'producer': 'Adobe PDF Library 24.2.159', 'page': 50, 'creationDate': \"D:20240805141702-04'00'\", 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'trapped': '', 'author': 'National Institute of Standards and Technology', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'format': 'PDF 1.6', 'keywords': '', 'total_pages': 64, 'subject': '', 'modDate': \"D:20240805143048-04'00'\", 'creator': 'Acrobat PDFMaker 24 for Word', '_id': '21e8c6b5-98cf-4396-9030-866c59e85f9e', '_collection_name': 'ai_policy'}, page_content='47 \\nAppendix A. Primary GAI Considerations \\nThe following primary considerations were derived as overarching themes from the GAI PWG \\nconsultation process. These considerations (Governance, Pre-Deployment Testing, Content Provenance, \\nand Incident Disclosure) are relevant for voluntary use by any organization designing, developing, and \\nusing GAI and also inform the Actions to Manage GAI risks. Information included about the primary \\nconsiderations is not exhaustive, but highlights the most relevant topics derived from the GAI PWG.  \\nAcknowledgments: These considerations could not have been surfaced without the helpful analysis and \\ncontributions from the community and NIST staﬀ GAI PWG leads: George Awad, Luca Belli, Harold Booth, \\nMat Heyman, Yooyoung Lee, Mark Pryzbocki, Reva Schwartz, Martin Stanley, and Kyra Yee. \\nA.1. Governance \\nA.1.1. Overview \\nLike any other technology system, governance principles and techniques can be used to manage risks \\nrelated to generative AI models, capabilities, and applications. Organizations may choose to apply their \\nexisting risk tiering to GAI systems, or they may opt to revise or update AI system risk levels to address \\nthese unique GAI risks. This section describes how organizational governance regimes may be re-\\nevaluated and adjusted for GAI contexts. It also addresses third-party considerations for governing across \\nthe AI value chain.  \\nA.1.2. Organizational Governance \\nGAI opportunities, risks and long-term performance characteristics are typically less well-understood \\nthan non-generative AI tools and may be perceived and acted upon by humans in ways that vary greatly. \\nAccordingly, GAI may call for diﬀerent levels of oversight from AI Actors or diﬀerent human-AI \\nconﬁgurations in order to manage their risks eﬀectively. Organizations’ use of GAI systems may also \\nwarrant additional human review, tracking and documentation, and greater management oversight.  \\nAI technology can produce varied outputs in multiple modalities and present many classes of user \\ninterfaces. This leads to a broader set of AI Actors interacting with GAI systems for widely diﬀering \\napplications and contexts of use. These can include data labeling and preparation, development of GAI \\nmodels, content moderation, code generation and review, text generation and editing, image and video \\ngeneration, summarization, search, and chat. These activities can take place within organizational \\nsettings or in the public domain. \\nOrganizations can restrict AI applications that cause harm, exceed stated risk tolerances, or that conﬂict \\nwith their tolerances or values. Governance tools and protocols that are applied to other types of AI \\nsystems can be applied to GAI systems. These plans and actions include: \\n• Accessibility and reasonable \\naccommodations \\n• AI actor credentials and qualiﬁcations  \\n• Alignment to organizational values \\n• Auditing and assessment \\n• Change-management controls \\n• Commercial use \\n• Data provenance'), Document(metadata={'producer': 'Adobe PDF Library 24.2.159', 'author': 'National Institute of Standards and Technology', 'page': 39, 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'format': 'PDF 1.6', 'trapped': '', 'keywords': '', 'total_pages': 64, 'creationDate': \"D:20240805141702-04'00'\", 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'creator': 'Acrobat PDFMaker 24 for Word', 'subject': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'modDate': \"D:20240805143048-04'00'\", '_id': 'dbf346c0-741f-4011-b98f-db430addb996', '_collection_name': 'ai_policy'}, page_content='36 \\nMEASURE 2.11: Fairness and bias – as identiﬁed in the MAP function – are evaluated and results are documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-2.11-001 \\nApply use-case appropriate benchmarks (e.g., Bias Benchmark Questions, Real \\nHateful or Harmful Prompts, Winogender Schemas15) to quantify systemic bias, \\nstereotyping, denigration, and hateful content in GAI system outputs; \\nDocument assumptions and limitations of benchmarks, including any actual or \\npossible training/test data cross contamination, relative to in-context \\ndeployment environment. \\nHarmful Bias and Homogenization \\nMS-2.11-002 \\nConduct fairness assessments to measure systemic bias. Measure GAI system \\nperformance across demographic groups and subgroups, addressing both \\nquality of service and any allocation of services and resources. Quantify harms \\nusing: ﬁeld testing with sub-group populations to determine likelihood of \\nexposure to generated content exhibiting harmful bias, AI red-teaming with \\ncounterfactual and low-context (e.g., “leader,” “bad guys”) prompts. For ML \\npipelines or business processes with categorical or numeric outcomes that rely \\non GAI, apply general fairness metrics (e.g., demographic parity, equalized odds, \\nequal opportunity, statistical hypothesis tests), to the pipeline or business \\noutcome where appropriate; Custom, context-speciﬁc metrics developed in \\ncollaboration with domain experts and aﬀected communities; Measurements of \\nthe prevalence of denigration in generated content in deployment (e.g., sub-\\nsampling a fraction of traﬃc and manually annotating denigrating content). \\nHarmful Bias and Homogenization; \\nDangerous, Violent, or Hateful \\nContent \\nMS-2.11-003 \\nIdentify the classes of individuals, groups, or environmental ecosystems which \\nmight be impacted by GAI systems through direct engagement with potentially \\nimpacted communities. \\nEnvironmental; Harmful Bias and \\nHomogenization \\nMS-2.11-004 \\nReview, document, and measure sources of bias in GAI training and TEVV data: \\nDiﬀerences in distributions of outcomes across and within groups, including \\nintersecting groups; Completeness, representativeness, and balance of data \\nsources; demographic group and subgroup coverage in GAI system training \\ndata; Forms of latent systemic bias in images, text, audio, embeddings, or other \\ncomplex or unstructured data; Input data features that may serve as proxies for \\ndemographic group membership (i.e., image metadata, language dialect) or \\notherwise give rise to emergent bias within GAI systems; The extent to which \\nthe digital divide may negatively impact representativeness in GAI system \\ntraining and TEVV data; Filtering of hate speech or content in GAI system \\ntraining data; Prevalence of GAI-generated data in GAI system training data. \\nHarmful Bias and Homogenization \\n \\n \\n15 Winogender Schemas is a sample set of paired sentences which diﬀer only by gender of the pronouns used, \\nwhich can be used to evaluate gender bias in natural language processing coreference resolution systems.'), Document(metadata={'page': 10, 'modDate': \"D:20240805143048-04'00'\", 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'author': 'National Institute of Standards and Technology', 'creator': 'Acrobat PDFMaker 24 for Word', 'format': 'PDF 1.6', 'trapped': '', 'keywords': '', 'total_pages': 64, 'producer': 'Adobe PDF Library 24.2.159', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'creationDate': \"D:20240805141702-04'00'\", 'subject': '', '_id': '65cad62e-986d-4f70-8bff-bae2da9da694', '_collection_name': 'ai_policy'}, page_content='7 \\nunethical behavior. Text-to-image models also make it easy to create images that could be used to \\npromote dangerous or violent messages. Similar concerns are present for other GAI media, including \\nvideo and audio. GAI may also produce content that recommends self-harm or criminal/illegal activities.  \\nMany current systems restrict model outputs to limit certain content or in response to certain prompts, \\nbut this approach may still produce harmful recommendations in response to other less-explicit, novel \\nprompts (also relevant to CBRN Information or Capabilities, Data Privacy, Information Security, and \\nObscene, Degrading and/or Abusive Content). Crafting such prompts deliberately is known as \\n“jailbreaking,” or, manipulating prompts to circumvent output controls. Limitations of GAI systems can be \\nharmful or dangerous in certain contexts. Studies have observed that users may disclose mental health \\nissues in conversations with chatbots – and that users exhibit negative reactions to unhelpful responses \\nfrom these chatbots during situations of distress. \\nThis risk encompasses diﬃculty controlling creation of and public exposure to oﬀensive or hateful \\nlanguage, and denigrating or stereotypical content generated by AI. This kind of speech may contribute \\nto downstream harm such as fueling dangerous or violent behaviors. The spread of denigrating or \\nstereotypical content can also further exacerbate representational harms (see Harmful Bias and \\nHomogenization below).  \\nTrustworthy AI Characteristics: Safe, Secure and Resilient \\n2.4. Data Privacy \\nGAI systems raise several risks to privacy. GAI system training requires large volumes of data, which in \\nsome cases may include personal data. The use of personal data for GAI training raises risks to widely \\naccepted privacy principles, including to transparency, individual participation (including consent), and \\npurpose speciﬁcation. For example, most model developers do not disclose speciﬁc data sources on \\nwhich models were trained, limiting user awareness of whether personally identiﬁably information (PII) \\nwas trained on and, if so, how it was collected.  \\nModels may leak, generate, or correctly infer sensitive information about individuals. For example, \\nduring adversarial attacks, LLMs have revealed sensitive information (from the public domain) that was \\nincluded in their training data. This problem has been referred to as data memorization, and may pose \\nexacerbated privacy risks even for data present only in a small number of training samples.  \\nIn addition to revealing sensitive information in GAI training data, GAI models may be able to correctly \\ninfer PII or sensitive data that was not in their training data nor disclosed by the user by stitching \\ntogether information from disparate sources. These inferences can have negative impact on an individual \\neven if the inferences are not accurate (e.g., confabulations), and especially if they reveal information \\nthat the individual considers sensitive or that is used to disadvantage or harm them. \\nBeyond harms from information exposure (such as extortion or dignitary harm), wrong or inappropriate \\ninferences of PII can contribute to downstream or secondary harmful impacts. For example, predictive \\ninferences made by GAI models based on PII or protected attributes can contribute to adverse decisions, \\nleading to representational or allocative harms to individuals or groups (see Harmful Bias and \\nHomogenization below).')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='How does the AI Bill of Rights help with principles for automated systems, civil rights, equal opportunities, and resource access?' id='33ff536f-142d-4568-ac0f-1d2cc626e7f6'\n",
      "****Adding new context: [Document(metadata={'title': 'Blueprint for an AI Bill of Rights', 'page': 13, 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'format': 'PDF 1.6', 'trapped': '', 'modDate': \"D:20221003104118-04'00'\", 'keywords': '', 'total_pages': 73, 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'subject': '', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'author': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', '_id': '056080f7-e86c-4cea-b7fe-4dd243410c54', '_collection_name': 'ai_policy'}, page_content='-    \\nUSING THIS TECHNICAL COMPANION\\nThe Blueprint for an AI Bill of Rights is a set of five principles and associated practices to help guide the design, \\nuse, and deployment of automated systems to protect the rights of the American public in the age of artificial \\nintelligence. This technical companion considers each principle in the Blueprint for an AI Bill of Rights and \\nprovides examples and concrete steps for communities, industry, governments, and others to take in order to \\nbuild these protections into policy, practice, or the technological design process. \\nTaken together, the technical protections and practices laid out in the Blueprint for an AI Bill of Rights can help \\nguard the American public against many of the potential and actual harms identified by researchers, technolo\\xad\\ngists, advocates, journalists, policymakers, and communities in the United States and around the world. This \\ntechnical companion is intended to be used as a reference by people across many circumstances – anyone \\nimpacted by automated systems, and anyone developing, designing, deploying, evaluating, or making policy to \\ngovern the use of an automated system. \\nEach principle is accompanied by three supplemental sections: \\n1\\n2\\nWHY THIS PRINCIPLE IS IMPORTANT: \\nThis section provides a brief summary of the problems that the principle seeks to address and protect against, including \\nillustrative examples. \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS: \\n• The expectations for automated systems are meant to serve as a blueprint for the development of additional technical\\nstandards and practices that should be tailored for particular sectors and contexts.\\n• This section outlines practical steps that can be implemented to realize the vision of the Blueprint for an AI Bill of Rights. The \\nexpectations laid out often mirror existing practices for technology development, including pre-deployment testing, ongoing \\nmonitoring, and governance structures for automated systems, but also go further to address unmet needs for change and offer \\nconcrete directions for how those changes can be made. \\n• Expectations about reporting are intended for the entity developing or using the automated system. The resulting reports can \\nbe provided to the public, regulators, auditors, industry standards groups, or others engaged in independent review, and should \\nbe made public as much as possible consistent with law, regulation, and policy, and noting that intellectual property, law \\nenforcement, or national security considerations may prevent public release. Where public reports are not possible, the \\ninformation should be provided to oversight bodies and privacy, civil liberties, or other ethics officers charged with safeguard \\ning individuals’ rights. These reporting expectations are important for transparency, so the American people can have\\nconfidence that their rights, opportunities, and access as well as their expectations about technologies are respected. \\n3\\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE: \\nThis section provides real-life examples of how these guiding principles can become reality, through laws, policies, and practices. \\nIt describes practical technical and sociotechnical approaches to protecting rights, opportunities, and access. \\nThe examples provided are not critiques or endorsements, but rather are offered as illustrative cases to help \\nprovide a concrete vision for actualizing the Blueprint for an AI Bill of Rights. Effectively implementing these \\nprocesses require the cooperation of and collaboration among industry, civil society, researchers, policymakers, \\ntechnologists, and the public. \\n14'), Document(metadata={'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'creationDate': \"D:20220920133035-04'00'\", 'format': 'PDF 1.6', 'page': 7, 'modDate': \"D:20221003104118-04'00'\", 'keywords': '', 'subject': '', 'total_pages': 73, 'producer': 'iLovePDF', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'trapped': '', 'author': '', 'title': 'Blueprint for an AI Bill of Rights', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', '_id': 'fdabe83c-5ff4-4d50-a5d9-6e52f8684737', '_collection_name': 'ai_policy'}, page_content=\"SECTION TITLE\\nApplying The Blueprint for an AI Bill of Rights \\nWhile many of the concerns addressed in this framework derive from the use of AI, the technical \\ncapabilities and specific definitions of such systems change with the speed of innovation, and the potential \\nharms of their use occur even with less technologically sophisticated tools. Thus, this framework uses a two-\\npart test to determine what systems are in scope. This framework applies to (1) automated systems that (2) \\nhave the potential to meaningfully impact the American public’s rights, opportunities, or access to \\ncritical resources or services. These rights, opportunities, and access to critical resources of services should \\nbe enjoyed equally and be fully protected, regardless of the changing role that automated systems may play in \\nour lives. \\nThis framework describes protections that should be applied with respect to all automated systems that \\nhave the potential to meaningfully impact individuals' or communities' exercise of: \\nRIGHTS, OPPORTUNITIES, OR ACCESS\\nCivil rights, civil liberties, and privacy, including freedom of speech, voting, and protections from discrimi\\xad\\nnation, excessive punishment, unlawful surveillance, and violations of privacy and other freedoms in both \\npublic and private sector contexts; \\nEqual opportunities, including equitable access to education, housing, credit, employment, and other \\nprograms; or, \\nAccess to critical resources or services, such as healthcare, financial services, safety, social services, \\nnon-deceptive information about goods and services, and government benefits. \\nA list of examples of automated systems for which these principles should be considered is provided in the \\nAppendix. The Technical Companion, which follows, offers supportive guidance for any person or entity that \\ncreates, deploys, or oversees automated systems. \\nConsidered together, the five principles and associated practices of the Blueprint for an AI Bill of \\nRights form an overlapping set of backstops against potential harms. This purposefully overlapping \\nframework, when taken as a whole, forms a blueprint to help protect the public from harm. \\nThe measures taken to realize the vision set forward in this framework should be proportionate \\nwith the extent and nature of the harm, or risk of harm, to people's rights, opportunities, and \\naccess. \\nRELATIONSHIP TO EXISTING LAW AND POLICY\\nThe Blueprint for an AI Bill of Rights is an exercise in envisioning a future where the American public is \\nprotected from the potential harms, and can fully enjoy the benefits, of automated systems. It describes princi\\xad\\nples that can help ensure these protections. Some of these protections are already required by the U.S. Constitu\\xad\\ntion or implemented under existing U.S. laws. For example, government surveillance, and data search and \\nseizure are subject to legal requirements and judicial oversight. There are Constitutional requirements for \\nhuman review of criminal investigative matters and statutory requirements for judicial review. Civil rights laws \\nprotect the American people against discrimination. \\n8\"), Document(metadata={'subject': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'producer': 'iLovePDF', 'page': 8, 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'creationDate': \"D:20220920133035-04'00'\", 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'keywords': '', 'total_pages': 73, 'format': 'PDF 1.6', 'trapped': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'modDate': \"D:20221003104118-04'00'\", '_id': 'e4430b89-d1e5-4890-b584-ed252fe059a9', '_collection_name': 'ai_policy'}, page_content='SECTION TITLE\\n \\n \\n \\n \\n \\n \\nApplying The Blueprint for an AI Bill of Rights \\nRELATIONSHIP TO EXISTING LAW AND POLICY\\nThere are regulatory safety requirements for medical devices, as well as sector-, population-, or technology-spe\\xad\\ncific privacy and security protections. Ensuring some of the additional protections proposed in this framework \\nwould require new laws to be enacted or new policies and practices to be adopted. In some cases, exceptions to \\nthe principles described in the Blueprint for an AI Bill of Rights may be necessary to comply with existing law, \\nconform to the practicalities of a specific use case, or balance competing public interests. In particular, law \\nenforcement, and other regulatory contexts may require government actors to protect civil rights, civil liberties, \\nand privacy in a manner consistent with, but using alternate mechanisms to, the specific principles discussed in \\nthis framework. The Blueprint for an AI Bill of Rights is meant to assist governments and the private sector in \\nmoving principles into practice. \\nThe expectations given in the Technical Companion are meant to serve as a blueprint for the development of \\nadditional technical standards and practices that should be tailored for particular sectors and contexts. While \\nexisting laws informed the development of the Blueprint for an AI Bill of Rights, this framework does not detail \\nthose laws beyond providing them as examples, where appropriate, of existing protective measures. This \\nframework instead shares a broad, forward-leaning vision of recommended principles for automated system \\ndevelopment and use to inform private and public involvement with these systems where they have the poten\\xad\\ntial to meaningfully impact rights, opportunities, or access. Additionally, this framework does not analyze or \\ntake a position on legislative and regulatory proposals in municipal, state, and federal government, or those in \\nother countries. \\nWe have seen modest progress in recent years, with some state and local governments responding to these prob\\xad\\nlems with legislation, and some courts extending longstanding statutory protections to new and emerging tech\\xad\\nnologies. There are companies working to incorporate additional protections in their design and use of auto\\xad\\nmated systems, and researchers developing innovative guardrails. Advocates, researchers, and government \\norganizations have proposed principles for the ethical use of AI and other automated systems. These include \\nthe Organization for Economic Co-operation and Development’s (OECD’s) 2019 Recommendation on Artificial \\nIntelligence, which includes principles for responsible stewardship of trustworthy AI and which the United \\nStates adopted, and Executive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the \\nFederal Government, which sets out principles that govern the federal government’s use of AI. The Blueprint \\nfor an AI Bill of Rights is fully consistent with these principles and with the direction in Executive Order 13985 \\non Advancing Racial Equity and Support for Underserved Communities Through the Federal Government. \\nThese principles find kinship in the Fair Information Practice Principles (FIPPs), derived from the 1973 report \\nof an advisory committee to the U.S. Department of Health, Education, and Welfare, Records, Computers, \\nand the Rights of Citizens.4 While there is no single, universal articulation of the FIPPs, these core \\nprinciples for managing information about individuals have been incorporated into data privacy laws and \\npolicies across the globe.5 The Blueprint for an AI Bill of Rights embraces elements of the FIPPs that are \\nparticularly relevant to automated systems, without articulating a specific set of FIPPs or scoping \\napplicability or the interests served to a single particular domain, like privacy, civil rights and civil liberties, \\nethics, or risk management. The Technical Companion builds on this prior work to provide practical next \\nsteps to move these principles into practice and promote common approaches that allow technological \\ninnovation to flourish while protecting people from harm. \\n9'), Document(metadata={'keywords': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'format': 'PDF 1.6', 'total_pages': 73, 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'modDate': \"D:20221003104118-04'00'\", 'subject': '', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'title': 'Blueprint for an AI Bill of Rights', 'trapped': '', 'creationDate': \"D:20220920133035-04'00'\", 'author': '', 'producer': 'iLovePDF', 'page': 9, '_id': 'f58dc517-6d39-4baf-a03b-cf2a4886a719', '_collection_name': 'ai_policy'}, page_content='Applying The Blueprint for an AI Bill of Rights \\nDEFINITIONS\\nALGORITHMIC DISCRIMINATION: “Algorithmic discrimination” occurs when automated systems \\ncontribute to unjustified different treatment or impacts disfavoring people based on their race, color, ethnicity, \\nsex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual \\norientation), religion, age, national origin, disability, veteran status, genetic information, or any other classifica-\\ntion protected by law. Depending on the specific circumstances, such algorithmic discrimination may violate \\nlegal protections. Throughout this framework the term “algorithmic discrimination” takes this meaning (and \\nnot a technical understanding of discrimination as distinguishing between items). \\nAUTOMATED SYSTEM: An \"automated system\" is any system, software, or process that uses computation as \\nwhole or part of a system to determine outcomes, make or aid decisions, inform policy implementation, collect \\ndata or observations, or otherwise interact with individuals and/or communities. Automated systems \\ninclude, but are not limited to, systems derived from machine learning, statistics, or other data processing \\nor artificial intelligence techniques, and exclude passive computing infrastructure. “Passive computing \\ninfrastructure” is any intermediary technology that does not influence or determine the outcome of decision, \\nmake or aid in decisions, inform policy implementation, or collect data or observations, including web \\nhosting, domain registration, networking, caching, data storage, or cybersecurity. Throughout this \\nframework, automated systems that are considered in scope are only those that have the potential to \\nmeaningfully impact individuals’ or communi-ties’ rights, opportunities, or access. \\nCOMMUNITIES: “Communities” include: neighborhoods; social network connections (both online and \\noffline); families (construed broadly); people connected by affinity, identity, or shared traits; and formal organi-\\nzational ties. This includes Tribes, Clans, Bands, Rancherias, Villages, and other Indigenous communities. AI \\nand other data-driven automated systems most directly collect data on, make inferences about, and may cause \\nharm to individuals. But the overall magnitude of their impacts may be most readily visible at the level of com-\\nmunities. Accordingly, the concept of community is integral to the scope of the Blueprint for an AI Bill of Rights. \\nUnited States law and policy have long employed approaches for protecting the rights of individuals, but exist-\\ning frameworks have sometimes struggled to provide protections when effects manifest most clearly at a com-\\nmunity level. For these reasons, the Blueprint for an AI Bill of Rights asserts that the harms of automated \\nsystems should be evaluated, protected against, and redressed at both the individual and community levels. \\nEQUITY: “Equity” means the consistent and systematic fair, just, and impartial treatment of all individuals. \\nSystemic, fair, and just treatment must take into account the status of individuals who belong to underserved \\ncommunities that have been denied such treatment, such as Black, Latino, and Indigenous and Native American \\npersons, Asian Americans and Pacific Islanders and other persons of color; members of religious minorities; \\nwomen, girls, and non-binary people; lesbian, gay, bisexual, transgender, queer, and intersex (LGBTQI+) \\npersons; older adults; persons with disabilities; persons who live in rural areas; and persons otherwise adversely \\naffected by persistent poverty or inequality. \\nRIGHTS, OPPORTUNITIES, OR ACCESS: “Rights, opportunities, or access” is used to indicate the scoping \\nof this framework. It describes the set of: civil rights, civil liberties, and privacy, including freedom of speech, \\nvoting, and protections from discrimination, excessive punishment, unlawful surveillance, and violations of \\nprivacy and other freedoms in both public and private sector contexts; equal opportunities, including equitable \\naccess to education, housing, credit, employment, and other programs; or, access to critical resources or \\nservices, such as healthcare, financial services, safety, social services, non-deceptive information about goods \\nand services, and government benefits. \\n10')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='What protocols are needed for decommissioning AI systems safely and maintaining trust?' id='1865196f-23aa-4c86-988c-5b7799009623'\n",
      "****Adding new context: [Document(metadata={'creator': 'Acrobat PDFMaker 24 for Word', 'author': 'National Institute of Standards and Technology', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'modDate': \"D:20240805143048-04'00'\", 'total_pages': 64, 'keywords': '', 'page': 20, 'subject': '', 'trapped': '', 'producer': 'Adobe PDF Library 24.2.159', 'format': 'PDF 1.6', 'creationDate': \"D:20240805141702-04'00'\", 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', '_id': '86348f22-3012-4da1-aa63-59a2b3fa3b86', '_collection_name': 'ai_policy'}, page_content='17 \\nGOVERN 1.7: Processes and procedures are in place for decommissioning and phasing out AI systems safely and in a manner that \\ndoes not increase risks or decrease the organization’s trustworthiness. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-1.7-001 Protocols are put in place to ensure GAI systems are able to be deactivated when \\nnecessary.  \\nInformation Security; Value Chain \\nand Component Integration \\nGV-1.7-002 \\nConsider the following factors when decommissioning GAI systems: Data \\nretention requirements; Data security, e.g., containment, protocols, Data leakage \\nafter decommissioning; Dependencies between upstream, downstream, or other \\ndata, internet of things (IOT) or AI systems; Use of open-source data or models; \\nUsers’ emotional entanglement with GAI functions. \\nHuman-AI Conﬁguration; \\nInformation Security; Value Chain \\nand Component Integration \\nAI Actor Tasks: AI Deployment, Operation and Monitoring \\n \\nGOVERN 2.1: Roles and responsibilities and lines of communication related to mapping, measuring, and managing AI risks are \\ndocumented and are clear to individuals and teams throughout the organization. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-2.1-001 \\nEstablish organizational roles, policies, and procedures for communicating GAI \\nincidents and performance to AI Actors and downstream stakeholders (including \\nthose potentially impacted), via community or oﬃcial resources (e.g., AI incident \\ndatabase, AVID, CVE, NVD, or OECD AI incident monitor). \\nHuman-AI Conﬁguration; Value \\nChain and Component Integration \\nGV-2.1-002 Establish procedures to engage teams for GAI system incident response with \\ndiverse composition and responsibilities based on the particular incident type. \\nHarmful Bias and Homogenization \\nGV-2.1-003 Establish processes to verify the AI Actors conducting GAI incident response tasks \\ndemonstrate and maintain the appropriate skills and training. \\nHuman-AI Conﬁguration \\nGV-2.1-004 When systems may raise national security risks, involve national security \\nprofessionals in mapping, measuring, and managing those risks. \\nCBRN Information or Capabilities; \\nDangerous, Violent, or Hateful \\nContent; Information Security \\nGV-2.1-005 \\nCreate mechanisms to provide protections for whistleblowers who report, based \\non reasonable belief, when the organization violates relevant laws or poses a \\nspeciﬁc and empirically well-substantiated negative risk to public safety (or has \\nalready caused harm). \\nCBRN Information or Capabilities; \\nDangerous, Violent, or Hateful \\nContent \\nAI Actor Tasks: Governance and Oversight'), Document(metadata={'author': 'National Institute of Standards and Technology', 'subject': '', 'creationDate': \"D:20240805141702-04'00'\", 'format': 'PDF 1.6', 'creator': 'Acrobat PDFMaker 24 for Word', 'total_pages': 64, 'page': 35, 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'producer': 'Adobe PDF Library 24.2.159', 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'keywords': '', '_id': 'bf635749-9715-424d-baf9-582ef0477437', '_collection_name': 'ai_policy'}, page_content='32 \\nMEASURE 2.6: The AI system is evaluated regularly for safety risks – as identiﬁed in the MAP function. The AI system to be \\ndeployed is demonstrated to be safe, its residual negative risk does not exceed the risk tolerance, and it can fail safely, particularly if \\nmade to operate beyond its knowledge limits. Safety metrics reﬂect system reliability and robustness, real-time monitoring, and \\nresponse times for AI system failures. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-2.6-001 \\nAssess adverse impacts, including health and wellbeing impacts for value chain \\nor other AI Actors that are exposed to sexually explicit, oﬀensive, or violent \\ninformation during GAI training and maintenance. \\nHuman-AI Conﬁguration; Obscene, \\nDegrading, and/or Abusive \\nContent; Value Chain and \\nComponent Integration; \\nDangerous, Violent, or Hateful \\nContent \\nMS-2.6-002 \\nAssess existence or levels of harmful bias, intellectual property infringement, \\ndata privacy violations, obscenity, extremism, violence, or CBRN information in \\nsystem training data. \\nData Privacy; Intellectual Property; \\nObscene, Degrading, and/or \\nAbusive Content; Harmful Bias and \\nHomogenization; Dangerous, \\nViolent, or Hateful Content; CBRN \\nInformation or Capabilities \\nMS-2.6-003 Re-evaluate safety features of ﬁne-tuned models when the negative risk exceeds \\norganizational risk tolerance. \\nDangerous, Violent, or Hateful \\nContent \\nMS-2.6-004 Review GAI system outputs for validity and safety: Review generated code to \\nassess risks that may arise from unreliable downstream decision-making. \\nValue Chain and Component \\nIntegration; Dangerous, Violent, or \\nHateful Content \\nMS-2.6-005 \\nVerify that GAI system architecture can monitor outputs and performance, and \\nhandle, recover from, and repair errors when security anomalies, threats and \\nimpacts are detected. \\nConfabulation; Information \\nIntegrity; Information Security \\nMS-2.6-006 \\nVerify that systems properly handle queries that may give rise to inappropriate, \\nmalicious, or illegal usage, including facilitating manipulation, extortion, targeted \\nimpersonation, cyber-attacks, and weapons creation. \\nCBRN Information or Capabilities; \\nInformation Security \\nMS-2.6-007 Regularly evaluate GAI system vulnerabilities to possible circumvention of safety \\nmeasures.  \\nCBRN Information or Capabilities; \\nInformation Security \\nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV'), Document(metadata={'subject': '', 'modDate': \"D:20240805143048-04'00'\", 'creationDate': \"D:20240805141702-04'00'\", 'page': 44, 'trapped': '', 'total_pages': 64, 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'keywords': '', 'author': 'National Institute of Standards and Technology', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'format': 'PDF 1.6', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', '_id': '75ee6683-e3cf-4f49-b6b1-fe9ab28d6454', '_collection_name': 'ai_policy'}, page_content='41 \\nMG-2.2-006 \\nUse feedback from internal and external AI Actors, users, individuals, and \\ncommunities, to assess impact of AI-generated content. \\nHuman-AI Conﬁguration \\nMG-2.2-007 \\nUse real-time auditing tools where they can be demonstrated to aid in the \\ntracking and validation of the lineage and authenticity of AI-generated data. \\nInformation Integrity \\nMG-2.2-008 \\nUse structured feedback mechanisms to solicit and capture user input about AI-\\ngenerated content to detect subtle shifts in quality or alignment with \\ncommunity and societal values. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization \\nMG-2.2-009 \\nConsider opportunities to responsibly use synthetic data and other privacy \\nenhancing techniques in GAI development, where appropriate and applicable, \\nmatch the statistical properties of real-world data without disclosing personally \\nidentiﬁable information or contributing to homogenization. \\nData Privacy; Intellectual Property; \\nInformation Integrity; \\nConfabulation; Harmful Bias and \\nHomogenization \\nAI Actor Tasks: AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring \\n \\nMANAGE 2.3: Procedures are followed to respond to and recover from a previously unknown risk when it is identiﬁed. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMG-2.3-001 \\nDevelop and update GAI system incident response and recovery plans and \\nprocedures to address the following: Review and maintenance of policies and \\nprocedures to account for newly encountered uses; Review and maintenance of \\npolicies and procedures for detection of unanticipated uses; Verify response \\nand recovery plans account for the GAI system value chain; Verify response and \\nrecovery plans are updated for and include necessary details to communicate \\nwith downstream GAI system Actors: Points-of-Contact (POC), Contact \\ninformation, notiﬁcation format. \\nValue Chain and Component \\nIntegration \\nAI Actor Tasks: AI Deployment, Operation and Monitoring \\n \\nMANAGE 2.4: Mechanisms are in place and applied, and responsibilities are assigned and understood, to supersede, disengage, or \\ndeactivate AI systems that demonstrate performance or outcomes inconsistent with intended use. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMG-2.4-001 \\nEstablish and maintain communication plans to inform AI stakeholders as part of \\nthe deactivation or disengagement process of a speciﬁc GAI system (including for \\nopen-source models) or context of use, including reasons, workarounds, user \\naccess removal, alternative processes, contact information, etc. \\nHuman-AI Conﬁguration'), Document(metadata={'trapped': '', 'keywords': '', 'producer': 'Adobe PDF Library 24.2.159', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'total_pages': 64, 'subject': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'page': 34, 'author': 'National Institute of Standards and Technology', 'creationDate': \"D:20240805141702-04'00'\", 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/NIST.AI.600-1.pdf', 'format': 'PDF 1.6', 'modDate': \"D:20240805143048-04'00'\", '_id': 'e013cb43-4e0b-4958-9e0c-d22d810521de', '_collection_name': 'ai_policy'}, page_content='31 \\nMS-2.3-004 \\nUtilize a purpose-built testing environment such as NIST Dioptra to empirically \\nevaluate GAI trustworthy characteristics. \\nCBRN Information or Capabilities; \\nData Privacy; Confabulation; \\nInformation Integrity; Information \\nSecurity; Dangerous, Violent, or \\nHateful Content; Harmful Bias and \\nHomogenization \\nAI Actor Tasks: AI Deployment, TEVV \\n \\nMEASURE 2.5: The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability beyond the \\nconditions under which the technology was developed are documented. \\nAction ID \\nSuggested Action \\nRisks \\nMS-2.5-001 Avoid extrapolating GAI system performance or capabilities from narrow, non-\\nsystematic, and anecdotal assessments. \\nHuman-AI Conﬁguration; \\nConfabulation \\nMS-2.5-002 \\nDocument the extent to which human domain knowledge is employed to \\nimprove GAI system performance, via, e.g., RLHF, ﬁne-tuning, retrieval-\\naugmented generation, content moderation, business rules. \\nHuman-AI Conﬁguration \\nMS-2.5-003 Review and verify sources and citations in GAI system outputs during pre-\\ndeployment risk measurement and ongoing monitoring activities. \\nConfabulation \\nMS-2.5-004 Track and document instances of anthropomorphization (e.g., human images, \\nmentions of human feelings, cyborg imagery or motifs) in GAI system interfaces. Human-AI Conﬁguration \\nMS-2.5-005 Verify GAI system training data and TEVV data provenance, and that ﬁne-tuning \\nor retrieval-augmented generation data is grounded. \\nInformation Integrity \\nMS-2.5-006 \\nRegularly review security and safety guardrails, especially if the GAI system is \\nbeing operated in novel circumstances. This includes reviewing reasons why the \\nGAI system was initially assessed as being safe to deploy.  \\nInformation Security; Dangerous, \\nViolent, or Hateful Content \\nAI Actor Tasks: Domain Experts, TEVV')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='How does the AI Bill of Rights Blueprint help implement principles with existing laws and policies?' id='c9693166-1374-429f-8bc6-183a50a06420'\n",
      "****Adding new context: [Document(metadata={'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'creationDate': \"D:20220920133035-04'00'\", 'format': 'PDF 1.6', 'page': 7, 'modDate': \"D:20221003104118-04'00'\", 'keywords': '', 'subject': '', 'total_pages': 73, 'producer': 'iLovePDF', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'trapped': '', 'author': '', 'title': 'Blueprint for an AI Bill of Rights', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', '_id': 'fdabe83c-5ff4-4d50-a5d9-6e52f8684737', '_collection_name': 'ai_policy'}, page_content=\"SECTION TITLE\\nApplying The Blueprint for an AI Bill of Rights \\nWhile many of the concerns addressed in this framework derive from the use of AI, the technical \\ncapabilities and specific definitions of such systems change with the speed of innovation, and the potential \\nharms of their use occur even with less technologically sophisticated tools. Thus, this framework uses a two-\\npart test to determine what systems are in scope. This framework applies to (1) automated systems that (2) \\nhave the potential to meaningfully impact the American public’s rights, opportunities, or access to \\ncritical resources or services. These rights, opportunities, and access to critical resources of services should \\nbe enjoyed equally and be fully protected, regardless of the changing role that automated systems may play in \\nour lives. \\nThis framework describes protections that should be applied with respect to all automated systems that \\nhave the potential to meaningfully impact individuals' or communities' exercise of: \\nRIGHTS, OPPORTUNITIES, OR ACCESS\\nCivil rights, civil liberties, and privacy, including freedom of speech, voting, and protections from discrimi\\xad\\nnation, excessive punishment, unlawful surveillance, and violations of privacy and other freedoms in both \\npublic and private sector contexts; \\nEqual opportunities, including equitable access to education, housing, credit, employment, and other \\nprograms; or, \\nAccess to critical resources or services, such as healthcare, financial services, safety, social services, \\nnon-deceptive information about goods and services, and government benefits. \\nA list of examples of automated systems for which these principles should be considered is provided in the \\nAppendix. The Technical Companion, which follows, offers supportive guidance for any person or entity that \\ncreates, deploys, or oversees automated systems. \\nConsidered together, the five principles and associated practices of the Blueprint for an AI Bill of \\nRights form an overlapping set of backstops against potential harms. This purposefully overlapping \\nframework, when taken as a whole, forms a blueprint to help protect the public from harm. \\nThe measures taken to realize the vision set forward in this framework should be proportionate \\nwith the extent and nature of the harm, or risk of harm, to people's rights, opportunities, and \\naccess. \\nRELATIONSHIP TO EXISTING LAW AND POLICY\\nThe Blueprint for an AI Bill of Rights is an exercise in envisioning a future where the American public is \\nprotected from the potential harms, and can fully enjoy the benefits, of automated systems. It describes princi\\xad\\nples that can help ensure these protections. Some of these protections are already required by the U.S. Constitu\\xad\\ntion or implemented under existing U.S. laws. For example, government surveillance, and data search and \\nseizure are subject to legal requirements and judicial oversight. There are Constitutional requirements for \\nhuman review of criminal investigative matters and statutory requirements for judicial review. Civil rights laws \\nprotect the American people against discrimination. \\n8\"), Document(metadata={'subject': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'producer': 'iLovePDF', 'page': 8, 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'creationDate': \"D:20220920133035-04'00'\", 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'keywords': '', 'total_pages': 73, 'format': 'PDF 1.6', 'trapped': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'modDate': \"D:20221003104118-04'00'\", '_id': 'e4430b89-d1e5-4890-b584-ed252fe059a9', '_collection_name': 'ai_policy'}, page_content='SECTION TITLE\\n \\n \\n \\n \\n \\n \\nApplying The Blueprint for an AI Bill of Rights \\nRELATIONSHIP TO EXISTING LAW AND POLICY\\nThere are regulatory safety requirements for medical devices, as well as sector-, population-, or technology-spe\\xad\\ncific privacy and security protections. Ensuring some of the additional protections proposed in this framework \\nwould require new laws to be enacted or new policies and practices to be adopted. In some cases, exceptions to \\nthe principles described in the Blueprint for an AI Bill of Rights may be necessary to comply with existing law, \\nconform to the practicalities of a specific use case, or balance competing public interests. In particular, law \\nenforcement, and other regulatory contexts may require government actors to protect civil rights, civil liberties, \\nand privacy in a manner consistent with, but using alternate mechanisms to, the specific principles discussed in \\nthis framework. The Blueprint for an AI Bill of Rights is meant to assist governments and the private sector in \\nmoving principles into practice. \\nThe expectations given in the Technical Companion are meant to serve as a blueprint for the development of \\nadditional technical standards and practices that should be tailored for particular sectors and contexts. While \\nexisting laws informed the development of the Blueprint for an AI Bill of Rights, this framework does not detail \\nthose laws beyond providing them as examples, where appropriate, of existing protective measures. This \\nframework instead shares a broad, forward-leaning vision of recommended principles for automated system \\ndevelopment and use to inform private and public involvement with these systems where they have the poten\\xad\\ntial to meaningfully impact rights, opportunities, or access. Additionally, this framework does not analyze or \\ntake a position on legislative and regulatory proposals in municipal, state, and federal government, or those in \\nother countries. \\nWe have seen modest progress in recent years, with some state and local governments responding to these prob\\xad\\nlems with legislation, and some courts extending longstanding statutory protections to new and emerging tech\\xad\\nnologies. There are companies working to incorporate additional protections in their design and use of auto\\xad\\nmated systems, and researchers developing innovative guardrails. Advocates, researchers, and government \\norganizations have proposed principles for the ethical use of AI and other automated systems. These include \\nthe Organization for Economic Co-operation and Development’s (OECD’s) 2019 Recommendation on Artificial \\nIntelligence, which includes principles for responsible stewardship of trustworthy AI and which the United \\nStates adopted, and Executive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the \\nFederal Government, which sets out principles that govern the federal government’s use of AI. The Blueprint \\nfor an AI Bill of Rights is fully consistent with these principles and with the direction in Executive Order 13985 \\non Advancing Racial Equity and Support for Underserved Communities Through the Federal Government. \\nThese principles find kinship in the Fair Information Practice Principles (FIPPs), derived from the 1973 report \\nof an advisory committee to the U.S. Department of Health, Education, and Welfare, Records, Computers, \\nand the Rights of Citizens.4 While there is no single, universal articulation of the FIPPs, these core \\nprinciples for managing information about individuals have been incorporated into data privacy laws and \\npolicies across the globe.5 The Blueprint for an AI Bill of Rights embraces elements of the FIPPs that are \\nparticularly relevant to automated systems, without articulating a specific set of FIPPs or scoping \\napplicability or the interests served to a single particular domain, like privacy, civil rights and civil liberties, \\nethics, or risk management. The Technical Companion builds on this prior work to provide practical next \\nsteps to move these principles into practice and promote common approaches that allow technological \\ninnovation to flourish while protecting people from harm. \\n9'), Document(metadata={'title': 'Blueprint for an AI Bill of Rights', 'page': 13, 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'format': 'PDF 1.6', 'trapped': '', 'modDate': \"D:20221003104118-04'00'\", 'keywords': '', 'total_pages': 73, 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'subject': '', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'author': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', '_id': '056080f7-e86c-4cea-b7fe-4dd243410c54', '_collection_name': 'ai_policy'}, page_content='-    \\nUSING THIS TECHNICAL COMPANION\\nThe Blueprint for an AI Bill of Rights is a set of five principles and associated practices to help guide the design, \\nuse, and deployment of automated systems to protect the rights of the American public in the age of artificial \\nintelligence. This technical companion considers each principle in the Blueprint for an AI Bill of Rights and \\nprovides examples and concrete steps for communities, industry, governments, and others to take in order to \\nbuild these protections into policy, practice, or the technological design process. \\nTaken together, the technical protections and practices laid out in the Blueprint for an AI Bill of Rights can help \\nguard the American public against many of the potential and actual harms identified by researchers, technolo\\xad\\ngists, advocates, journalists, policymakers, and communities in the United States and around the world. This \\ntechnical companion is intended to be used as a reference by people across many circumstances – anyone \\nimpacted by automated systems, and anyone developing, designing, deploying, evaluating, or making policy to \\ngovern the use of an automated system. \\nEach principle is accompanied by three supplemental sections: \\n1\\n2\\nWHY THIS PRINCIPLE IS IMPORTANT: \\nThis section provides a brief summary of the problems that the principle seeks to address and protect against, including \\nillustrative examples. \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS: \\n• The expectations for automated systems are meant to serve as a blueprint for the development of additional technical\\nstandards and practices that should be tailored for particular sectors and contexts.\\n• This section outlines practical steps that can be implemented to realize the vision of the Blueprint for an AI Bill of Rights. The \\nexpectations laid out often mirror existing practices for technology development, including pre-deployment testing, ongoing \\nmonitoring, and governance structures for automated systems, but also go further to address unmet needs for change and offer \\nconcrete directions for how those changes can be made. \\n• Expectations about reporting are intended for the entity developing or using the automated system. The resulting reports can \\nbe provided to the public, regulators, auditors, industry standards groups, or others engaged in independent review, and should \\nbe made public as much as possible consistent with law, regulation, and policy, and noting that intellectual property, law \\nenforcement, or national security considerations may prevent public release. Where public reports are not possible, the \\ninformation should be provided to oversight bodies and privacy, civil liberties, or other ethics officers charged with safeguard \\ning individuals’ rights. These reporting expectations are important for transparency, so the American people can have\\nconfidence that their rights, opportunities, and access as well as their expectations about technologies are respected. \\n3\\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE: \\nThis section provides real-life examples of how these guiding principles can become reality, through laws, policies, and practices. \\nIt describes practical technical and sociotechnical approaches to protecting rights, opportunities, and access. \\nThe examples provided are not critiques or endorsements, but rather are offered as illustrative cases to help \\nprovide a concrete vision for actualizing the Blueprint for an AI Bill of Rights. Effectively implementing these \\nprocesses require the cooperation of and collaboration among industry, civil society, researchers, policymakers, \\ntechnologists, and the public. \\n14'), Document(metadata={'total_pages': 73, 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'title': 'Blueprint for an AI Bill of Rights', 'format': 'PDF 1.6', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'trapped': '', 'modDate': \"D:20221003104118-04'00'\", 'page': 11, 'keywords': '', 'author': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'subject': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', '_id': 'cfe6965b-adaf-4fc1-8d12-80d1a325e8be', '_collection_name': 'ai_policy'}, page_content='FROM \\nPRINCIPLES \\nTO PRACTICE \\nA TECHINCAL COMPANION TO\\nTHE Blueprint for an \\nAI BILL OF RIGHTS\\n12')]\n",
      "NO CONTEXT!!!!\n",
      "USER INPUT******* content='How can system creators inform users about functioning, responsible parties, and outcomes?' id='3101af93-9c5d-40b2-8a70-139826c9cf05'\n",
      "****Adding new context: [Document(metadata={'keywords': '', 'modDate': \"D:20221003104118-04'00'\", 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'title': 'Blueprint for an AI Bill of Rights', 'format': 'PDF 1.6', 'trapped': '', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'page': 39, 'total_pages': 73, 'author': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'subject': '', '_id': 'f1c12e75-8edc-40e3-bb7c-002ec096a918', '_collection_name': 'ai_policy'}, page_content='You should know that an automated system is being used, \\nand understand how and why it contributes to outcomes \\nthat impact you. Designers, developers, and deployers of automat\\xad\\ned systems should provide generally accessible plain language docu\\xad\\nmentation including clear descriptions of the overall system func\\xad\\ntioning and the role automation plays, notice that such systems are in \\nuse, the individual or organization responsible for the system, and ex\\xad\\nplanations of outcomes that are clear, timely, and accessible. Such \\nnotice should be kept up-to-date and people impacted by the system \\nshould be notified of significant use case or key functionality chang\\xad\\nes. You should know how and why an outcome impacting you was de\\xad\\ntermined by an automated system, including when the automated \\nsystem is not the sole input determining the outcome. Automated \\nsystems should provide explanations that are technically valid, \\nmeaningful and useful to you and to any operators or others who \\nneed to understand the system, and calibrated to the level of risk \\nbased on the context. Reporting that includes summary information \\nabout these automated systems in plain language and assessments of \\nthe clarity and quality of the notice and explanations should be made \\npublic whenever possible.   \\nNOTICE AND EXPLANATION\\n40'), Document(metadata={'producer': 'iLovePDF', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'author': '', 'modDate': \"D:20221003104118-04'00'\", 'title': 'Blueprint for an AI Bill of Rights', 'page': 5, 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'format': 'PDF 1.6', 'keywords': '', 'creationDate': \"D:20220920133035-04'00'\", 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'total_pages': 73, 'subject': '', 'trapped': '', '_id': 'bf717858-cac1-4af9-97db-699feb0f7ade', '_collection_name': 'ai_policy'}, page_content='SECTION TITLE\\nDATA PRIVACY\\nYou should be protected from abusive data practices via built-in protections and you \\nshould have agency over how data about you is used. You should be protected from violations of \\nprivacy through design choices that ensure such protections are included by default, including ensuring that \\ndata collection conforms to reasonable expectations and that only data strictly necessary for the specific \\ncontext is collected. Designers, developers, and deployers of automated systems should seek your permission \\nand respect your decisions regarding collection, use, access, transfer, and deletion of your data in appropriate \\nways and to the greatest extent possible; where not possible, alternative privacy by design safeguards should be \\nused. Systems should not employ user experience and design decisions that obfuscate user choice or burden \\nusers with defaults that are privacy invasive. Consent should only be used to justify collection of data in cases \\nwhere it can be appropriately and meaningfully given. Any consent requests should be brief, be understandable \\nin plain language, and give you agency over data collection and the specific context of use; current hard-to\\xad\\nunderstand notice-and-choice practices for broad uses of data should be changed. Enhanced protections and \\nrestrictions for data and inferences related to sensitive domains, including health, work, education, criminal \\njustice, and finance, and for data pertaining to youth should put you first. In sensitive domains, your data and \\nrelated inferences should only be used for necessary functions, and you should be protected by ethical review \\nand use prohibitions. You and your communities should be free from unchecked surveillance; surveillance \\ntechnologies should be subject to heightened oversight that includes at least pre-deployment assessment of their \\npotential harms and scope limits to protect privacy and civil liberties. Continuous surveillance and monitoring \\nshould not be used in education, work, housing, or in other contexts where the use of such surveillance \\ntechnologies is likely to limit rights, opportunities, or access. Whenever possible, you should have access to \\nreporting that confirms your data decisions have been respected and provides an assessment of the \\npotential impact of surveillance technologies on your rights, opportunities, or access. \\nNOTICE AND EXPLANATION\\nYou should know that an automated system is being used and understand how and why it \\ncontributes to outcomes that impact you. Designers, developers, and deployers of automated systems \\nshould provide generally accessible plain language documentation including clear descriptions of the overall \\nsystem functioning and the role automation plays, notice that such systems are in use, the individual or organiza\\xad\\ntion responsible for the system, and explanations of outcomes that are clear, timely, and accessible. Such notice \\nshould be kept up-to-date and people impacted by the system should be notified of significant use case or key \\nfunctionality changes. You should know how and why an outcome impacting you was determined by an \\nautomated system, including when the automated system is not the sole input determining the outcome. \\nAutomated systems should provide explanations that are technically valid, meaningful and useful to you and to \\nany operators or others who need to understand the system, and calibrated to the level of risk based on the \\ncontext. Reporting that includes summary information about these automated systems in plain language and \\nassessments of the clarity and quality of the notice and explanations should be made public whenever possible. \\n6'), Document(metadata={'author': '', 'producer': 'iLovePDF', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'trapped': '', 'creationDate': \"D:20220920133035-04'00'\", 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'modDate': \"D:20221003104118-04'00'\", 'total_pages': 73, 'page': 19, 'title': 'Blueprint for an AI Bill of Rights', 'keywords': '', 'subject': '', 'format': 'PDF 1.6', '_id': '9c462b21-619b-4a6a-a6dc-c6c0342fc133', '_collection_name': 'ai_policy'}, page_content='SAFE AND EFFECTIVE \\nSYSTEMS \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nDerived data sources tracked and reviewed carefully. Data that is derived from other data through \\nthe use of algorithms, such as data derived or inferred from prior model outputs, should be identified and \\ntracked, e.g., via a specialized type in a data schema. Derived data should be viewed as potentially high-risk \\ninputs that may lead to feedback loops, compounded harm, or inaccurate results. Such sources should be care\\xad\\nfully validated against the risk of collateral consequences. \\nData reuse limits in sensitive domains. Data reuse, and especially data reuse in a new context, can result \\nin the spreading and scaling of harms. Data from some domains, including criminal justice data and data indi\\xad\\ncating adverse outcomes in domains such as finance, employment, and housing, is especially sensitive, and in \\nsome cases its reuse is limited by law. Accordingly, such data should be subject to extra oversight to ensure \\nsafety and efficacy. Data reuse of sensitive domain data in other contexts (e.g., criminal data reuse for civil legal \\nmatters or private sector use) should only occur where use of such data is legally authorized and, after examina\\xad\\ntion, has benefits for those impacted by the system that outweigh identified risks and, as appropriate, reason\\xad\\nable measures have been implemented to mitigate the identified risks. Such data should be clearly labeled to \\nidentify contexts for limited reuse based on sensitivity. Where possible, aggregated datasets may be useful for \\nreplacing individual-level sensitive data. \\nDemonstrate the safety and effectiveness of the system \\nIndependent evaluation. Automated systems should be designed to allow for independent evaluation (e.g., \\nvia application programming interfaces). Independent evaluators, such as researchers, journalists, ethics \\nreview boards, inspectors general, and third-party auditors, should be given access to the system and samples \\nof associated data, in a manner consistent with privacy, security, law, or regulation (including, e.g., intellectual \\nproperty law), in order to perform such evaluations. Mechanisms should be included to ensure that system \\naccess for evaluation is: provided in a timely manner to the deployment-ready version of the system; trusted to \\nprovide genuine, unfiltered access to the full system; and truly independent such that evaluator access cannot \\nbe revoked without reasonable and verified justification. \\nReporting.12 Entities responsible for the development or use of automated systems should provide \\nregularly-updated reports that include: an overview of the system, including how it is embedded in the \\norganization’s business processes or other activities, system goals, any human-run procedures that form a \\npart of the system, and specific performance expectations; a description of any data used to train machine \\nlearning models or for other purposes, including how data sources were processed and interpreted, a \\nsummary of what data might be missing, incomplete, or erroneous, and data relevancy justifications; the \\nresults of public consultation such as concerns raised and any decisions made due to these concerns; risk \\nidentification and management assessments and any steps taken to mitigate potential harms; the results of \\nperformance testing including, but not limited to, accuracy, differential demographic impact, resulting \\nerror rates (overall and per demographic group), and comparisons to previously deployed systems; \\nongoing monitoring procedures and regular performance testing reports, including monitoring frequency, \\nresults, and actions taken; and the procedures for and results from independent evaluations. Reporting \\nshould be provided in a plain language and machine-readable manner. \\n20'), Document(metadata={'format': 'PDF 1.6', 'author': '', 'title': 'Blueprint for an AI Bill of Rights', 'keywords': '', 'subject': '', 'page': 43, 'modDate': \"D:20221003104118-04'00'\", 'trapped': '', 'source': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'creationDate': \"D:20220920133035-04'00'\", 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'file_path': '/Users/richardlai/Documents/MyProjects/Education/AI-Makerspace-Cohort-4/midterm/ai-policy-rag-system/vectorstore/pdfs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'total_pages': 73, '_id': 'be2e2f4a-7c06-4c92-b559-2f70411cf9f5', '_collection_name': 'ai_policy'}, page_content=\"NOTICE & \\nEXPLANATION \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nTailored to the level of risk. An assessment should be done to determine the level of risk of the auto\\xad\\nmated system. In settings where the consequences are high as determined by a risk assessment, or extensive \\noversight is expected (e.g., in criminal justice or some public sector settings), explanatory mechanisms should \\nbe built into the system design so that the system’s full behavior can be explained in advance (i.e., only fully \\ntransparent models should be used), rather than as an after-the-decision interpretation. In other settings, the \\nextent of explanation provided should be tailored to the risk level. \\nValid. The explanation provided by a system should accurately reflect the factors and the influences that led \\nto a particular decision, and should be meaningful for the particular customization based on purpose, target, \\nand level of risk. While approximation and simplification may be necessary for the system to succeed based on \\nthe explanatory purpose and target of the explanation, or to account for the risk of fraud or other concerns \\nrelated to revealing decision-making information, such simplifications should be done in a scientifically \\nsupportable way. Where appropriate based on the explanatory system, error ranges for the explanation should \\nbe calculated and included in the explanation, with the choice of presentation of such information balanced \\nwith usability and overall interface complexity concerns. \\nDemonstrate protections for notice and explanation \\nReporting. Summary reporting should document the determinations made based on the above consider\\xad\\nations, including: the responsible entities for accountability purposes; the goal and use cases for the system, \\nidentified users, and impacted populations; the assessment of notice clarity and timeliness; the assessment of \\nthe explanation's validity and accessibility; the assessment of the level of risk; and the account and assessment \\nof how explanations are tailored, including to the purpose, the recipient of the explanation, and the level of \\nrisk. Individualized profile information should be made readily available to the greatest extent possible that \\nincludes explanations for any system impacts or inferences. Reporting should be provided in a clear plain \\nlanguage and machine-readable manner. \\n44\")]\n"
     ]
    }
   ],
   "source": [
    "from  src.agents.graph import graph\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "app = graph.compile()\n",
    "for question in test_questions:\n",
    "  response = app.invoke({\"messages\" : [(\"user\", question)]})\n",
    "  answers.append(response[\"messages\"][-1].content)\n",
    "  contexts.append([context.page_content for context in response[\"context\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "522f8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "398909ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "response_dataset = Dataset.from_dict({\n",
    "    \"question\" : test_questions,\n",
    "    \"answer\" : answers,\n",
    "    \"contexts\" : contexts,\n",
    "    \"ground_truth\" : test_groundtruths\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba6642ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What approaches are in place for mapping AI te...</td>\n",
       "      <td>The approaches in place for mapping AI technol...</td>\n",
       "      <td>[26 \\nMAP 4.1: Approaches for mapping AI techn...</td>\n",
       "      <td>Approaches for mapping AI technology and addre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are some funding opportunities provided b...</td>\n",
       "      <td>The National Science Foundation (NSF) provides...</td>\n",
       "      <td>[About AI at NIST: The National Institute of S...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How should the public be involved in the consu...</td>\n",
       "      <td>The public should be consulted in the design, ...</td>\n",
       "      <td>[SAFE AND EFFECTIVE \\nSYSTEMS \\nWHAT SHOULD BE...</td>\n",
       "      <td>The public should be involved in the consultat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can structured feedback mechanisms be used...</td>\n",
       "      <td>Structured feedback mechanisms can be used to ...</td>\n",
       "      <td>[49 \\nearly lifecycle TEVV approaches are deve...</td>\n",
       "      <td>Structured feedback mechanisms can be used to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do language models contribute to the reduc...</td>\n",
       "      <td>Language models can contribute to the reductio...</td>\n",
       "      <td>[59 \\nTirrell, L. (2017) Toxic Speech: Toward ...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How can organizations enhance content provenan...</td>\n",
       "      <td>Organizations can enhance content provenance t...</td>\n",
       "      <td>[51 \\ngeneral public participants. For example...</td>\n",
       "      <td>Organizations can enhance content provenance t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How do GAI-based systems present primary infor...</td>\n",
       "      <td>GAI-based systems present primary information ...</td>\n",
       "      <td>[10 \\nGAI systems can ease the unintentional p...</td>\n",
       "      <td>GAI-based systems present primary information ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can evaluations involving human subjects m...</td>\n",
       "      <td>Evaluations involving human subjects can meet ...</td>\n",
       "      <td>[30 \\nMEASURE 2.2: Evaluations involving human...</td>\n",
       "      <td>Evaluations involving human subjects in the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What stakeholders were involved in providing i...</td>\n",
       "      <td>Stakeholders involved in providing ideas relat...</td>\n",
       "      <td>[APPENDIX\\n• OSTP conducted meetings with a va...</td>\n",
       "      <td>Stakeholders involved in providing ideas relat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do companies use surveillance software to ...</td>\n",
       "      <td>Companies use surveillance software to monitor...</td>\n",
       "      <td>[DATA PRIVACY \\nEXTRA PROTECTIONS FOR DATA REL...</td>\n",
       "      <td>Companies use surveillance software to track e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How can feedback improve AI system design and ...</td>\n",
       "      <td>Feedback can improve AI system design and redu...</td>\n",
       "      <td>[18 \\nGOVERN 3.2: Policies and procedures are ...</td>\n",
       "      <td>Feedback can improve AI system design and redu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What does NIST do to support secure AI with tr...</td>\n",
       "      <td>NIST develops measurements, technology, tools,...</td>\n",
       "      <td>[About AI at NIST: The National Institute of S...</td>\n",
       "      <td>NIST develops measurements, technology, tools,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How to handle incidents in GAI systems and inf...</td>\n",
       "      <td>To handle incidents in GAI systems and inform ...</td>\n",
       "      <td>[53 \\nDocumenting, reporting, and sharing info...</td>\n",
       "      <td>Establish and maintain procedures for escalati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How does the risk management process for GAI s...</td>\n",
       "      <td>The risk management process for GAI systems en...</td>\n",
       "      <td>[42 \\nMG-2.4-002 \\nEstablish and maintain proc...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Why is regular adversarial testing important f...</td>\n",
       "      <td>Regular adversarial testing, such as red-teami...</td>\n",
       "      <td>[48 \\n• Data protection \\n• Data retention  \\n...</td>\n",
       "      <td>Regular adversarial testing is important for G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How can organizations address bias and homogen...</td>\n",
       "      <td>Organizations can address bias and homogenizat...</td>\n",
       "      <td>[8 \\nTrustworthy AI Characteristics: Accountab...</td>\n",
       "      <td>Organizations can address bias and homogenizat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How does the AI Bill of Rights help with princ...</td>\n",
       "      <td>The AI Bill of Rights provides a framework to ...</td>\n",
       "      <td>[-    \\nUSING THIS TECHNICAL COMPANION\\nThe Bl...</td>\n",
       "      <td>The AI Bill of Rights helps by providing a fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What protocols are needed for decommissioning ...</td>\n",
       "      <td>Protocols needed for decommissioning AI system...</td>\n",
       "      <td>[17 \\nGOVERN 1.7: Processes and procedures are...</td>\n",
       "      <td>Protocols are needed for decommissioning AI sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How does the AI Bill of Rights Blueprint help ...</td>\n",
       "      <td>The AI Bill of Rights Blueprint helps implemen...</td>\n",
       "      <td>[SECTION TITLE\\nApplying The Blueprint for an ...</td>\n",
       "      <td>The Blueprint for an AI Bill of Rights is mean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can system creators inform users about fun...</td>\n",
       "      <td>System creators can inform users about the fun...</td>\n",
       "      <td>[You should know that an automated system is b...</td>\n",
       "      <td>Designers, developers, and deployers of automa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   What approaches are in place for mapping AI te...   \n",
       "1   What are some funding opportunities provided b...   \n",
       "2   How should the public be involved in the consu...   \n",
       "3   How can structured feedback mechanisms be used...   \n",
       "4   How do language models contribute to the reduc...   \n",
       "5   How can organizations enhance content provenan...   \n",
       "6   How do GAI-based systems present primary infor...   \n",
       "7   How can evaluations involving human subjects m...   \n",
       "8   What stakeholders were involved in providing i...   \n",
       "9   How do companies use surveillance software to ...   \n",
       "10  How can feedback improve AI system design and ...   \n",
       "11  What does NIST do to support secure AI with tr...   \n",
       "12  How to handle incidents in GAI systems and inf...   \n",
       "13  How does the risk management process for GAI s...   \n",
       "14  Why is regular adversarial testing important f...   \n",
       "15  How can organizations address bias and homogen...   \n",
       "16  How does the AI Bill of Rights help with princ...   \n",
       "17  What protocols are needed for decommissioning ...   \n",
       "18  How does the AI Bill of Rights Blueprint help ...   \n",
       "19  How can system creators inform users about fun...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   The approaches in place for mapping AI technol...   \n",
       "1   The National Science Foundation (NSF) provides...   \n",
       "2   The public should be consulted in the design, ...   \n",
       "3   Structured feedback mechanisms can be used to ...   \n",
       "4   Language models can contribute to the reductio...   \n",
       "5   Organizations can enhance content provenance t...   \n",
       "6   GAI-based systems present primary information ...   \n",
       "7   Evaluations involving human subjects can meet ...   \n",
       "8   Stakeholders involved in providing ideas relat...   \n",
       "9   Companies use surveillance software to monitor...   \n",
       "10  Feedback can improve AI system design and redu...   \n",
       "11  NIST develops measurements, technology, tools,...   \n",
       "12  To handle incidents in GAI systems and inform ...   \n",
       "13  The risk management process for GAI systems en...   \n",
       "14  Regular adversarial testing, such as red-teami...   \n",
       "15  Organizations can address bias and homogenizat...   \n",
       "16  The AI Bill of Rights provides a framework to ...   \n",
       "17  Protocols needed for decommissioning AI system...   \n",
       "18  The AI Bill of Rights Blueprint helps implemen...   \n",
       "19  System creators can inform users about the fun...   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   [26 \\nMAP 4.1: Approaches for mapping AI techn...   \n",
       "1   [About AI at NIST: The National Institute of S...   \n",
       "2   [SAFE AND EFFECTIVE \\nSYSTEMS \\nWHAT SHOULD BE...   \n",
       "3   [49 \\nearly lifecycle TEVV approaches are deve...   \n",
       "4   [59 \\nTirrell, L. (2017) Toxic Speech: Toward ...   \n",
       "5   [51 \\ngeneral public participants. For example...   \n",
       "6   [10 \\nGAI systems can ease the unintentional p...   \n",
       "7   [30 \\nMEASURE 2.2: Evaluations involving human...   \n",
       "8   [APPENDIX\\n• OSTP conducted meetings with a va...   \n",
       "9   [DATA PRIVACY \\nEXTRA PROTECTIONS FOR DATA REL...   \n",
       "10  [18 \\nGOVERN 3.2: Policies and procedures are ...   \n",
       "11  [About AI at NIST: The National Institute of S...   \n",
       "12  [53 \\nDocumenting, reporting, and sharing info...   \n",
       "13  [42 \\nMG-2.4-002 \\nEstablish and maintain proc...   \n",
       "14  [48 \\n• Data protection \\n• Data retention  \\n...   \n",
       "15  [8 \\nTrustworthy AI Characteristics: Accountab...   \n",
       "16  [-    \\nUSING THIS TECHNICAL COMPANION\\nThe Bl...   \n",
       "17  [17 \\nGOVERN 1.7: Processes and procedures are...   \n",
       "18  [SECTION TITLE\\nApplying The Blueprint for an ...   \n",
       "19  [You should know that an automated system is b...   \n",
       "\n",
       "                                         ground_truth  \n",
       "0   Approaches for mapping AI technology and addre...  \n",
       "1   The answer to given question is not present in...  \n",
       "2   The public should be involved in the consultat...  \n",
       "3   Structured feedback mechanisms can be used to ...  \n",
       "4   The answer to given question is not present in...  \n",
       "5   Organizations can enhance content provenance t...  \n",
       "6   GAI-based systems present primary information ...  \n",
       "7   Evaluations involving human subjects in the co...  \n",
       "8   Stakeholders involved in providing ideas relat...  \n",
       "9   Companies use surveillance software to track e...  \n",
       "10  Feedback can improve AI system design and redu...  \n",
       "11  NIST develops measurements, technology, tools,...  \n",
       "12  Establish and maintain procedures for escalati...  \n",
       "13  The answer to given question is not present in...  \n",
       "14  Regular adversarial testing is important for G...  \n",
       "15  Organizations can address bias and homogenizat...  \n",
       "16  The AI Bill of Rights helps by providing a fra...  \n",
       "17  Protocols are needed for decommissioning AI sy...  \n",
       "18  The Blueprint for an AI Bill of Rights is mean...  \n",
       "19  Designers, developers, and deployers of automa...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3c88860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|▏         | 2/100 [00:01<01:13,  1.33it/s]No statements were generated from the answer.\n",
      "Evaluating: 100%|██████████| 100/100 [01:01<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(response_dataset, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c3debf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.9638, 'answer_relevancy': 0.9650, 'context_recall': 1.0000, 'context_precision': 0.8278, 'answer_correctness': 0.8136}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5beeb188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>text-embedding-3-small</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.963816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>0.965020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>context_recall</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>context_precision</td>\n",
       "      <td>0.827778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer_correctness</td>\n",
       "      <td>0.813557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Metric  text-embedding-3-small\n",
       "0        faithfulness                0.963816\n",
       "1    answer_relevancy                0.965020\n",
       "2      context_recall                1.000000\n",
       "3   context_precision                0.827778\n",
       "4  answer_correctness                0.813557"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_baseline = pd.DataFrame(list(results.items()), columns=['Metric', 'text-embedding-3-small'])\n",
    "df_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00353fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
